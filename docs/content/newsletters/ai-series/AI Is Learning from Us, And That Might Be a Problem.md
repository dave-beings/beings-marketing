<a id="_ovufof1eatnj"></a>AI Is Learning from Us, And That Might Be a Problem

  
Standfirst: AI tools are learning from our habits, decisions, and compromises, not just our best work\. What we teach them now will shape the future of research\.

## <a id="_jcp66umkbqqg"></a>This week, I’m talking about what AI is picking up from us and how small choices now shape the kind of research it will produce later\.

AI is getting better at research\. Not because it understands people, but because it has been trained on us \- our habits, shortcuts and compromises\. And that’s where the risk begins\. It learns how we write, segment, code, build hypotheses, summarise, and interpret\.

That gives it power\. But it also creates risk\.

When AI reflects our behaviour back to us, it does not distinguish between best practice and habit\. It does not know whether we took our time or rushed it\. It cannot tell if a framework was chosen with care or pulled from the last project folder\. It just learns what we do, assumes it works, and starts repeating it\. As it does, those patterns scale\.

The output often feels helpful\. But it is worth asking what exactly we are teaching it\.

This is not a call to slow down\. AI is here, and for most research agencies, it is no longer optional\. The opportunity is real\. So is the need to stay sharp\. Building faster systems without thinking about what they are learning is like designing a high\-speed car without checking the brakes\. Eventually, something gives\.

A better approach involves building clarity in early\. That means watching for shortcuts, spotting bias before it embeds, and treating each moment of automation as a decision point\. The sooner that happens, the more valuable the tool becomes\.

It matters where we go next and what we let the system learn along the way\.

### <a id="_o3qtul3b48c9"></a>__AI Feels Like It Gets You\. That Can Be Misleading__

One of the cleverest things about AI tools right now is how quickly they start to sound like your team\. Ask for a summary, and it mirrors your voice\. Load in a few audience segments, and it reproduces the structure you usually apply\. Over time, it begins to anticipate what you want before you have asked for it\.

That can feel like a breakthrough\. The familiarity is reassuring\. The analysis looks like yours\. The insight sounds like your standard tone\. It creates a sense that things are working\. And, on the surface, they are\.

However, familiarity builds trust, and that trust can make us skip over what would usually be considered essential checks\. Once an output appears polished and plausible, the temptation is to skim it\. Instead of asking whether it reflects the data, we respond to whether it looks and sounds right\. At that point, the tool stops supporting the thinking and begins to replace it\.

There is a cognitive mechanism behind this\. Psychologists call it [processing fluency](https://www.convertize.com/glossary/processing-fluency/#:~:text=Processing%20Fluency%20is%20a%20cognitive,find%20simple%20information%20more%20believable.), the tendency to believe something more readily if it is easy to understand, pleasant to read, or visually familiar\. Studies have shown that people rate clearly written statements as more likely to be true than harder\-to\-read versions of the exact same statement\. In research, this translates into a preference for clean charts, smooth writing, and familiar logic flows\. None of that is inherently bad\. But it can quietly nudge us into overconfidence, especially when the outputs are coming from a machine trained to reflect our style\.

There is also a related phenomenon known as the [illusory truth effect](https://thedecisionlab.com/biases/illusory-truth-effect), where repeated exposure to a certain phrase or structure makes it feel more accurate over time, regardless of whether it has been tested\. In the context of AI\-assisted research, this creates a subtle risk\. If a particular theme keeps appearing in drafts, not because the data demands it, but because the model has learned to favour it, we may start accepting it as true through repetition rather than evidence\.

This is not a fault of the technology\. It is a quirk of how humans make decisions\. We tend to trust what feels fluent\. When that fluency has been trained on us, the effect is even stronger\.

For research teams, especially smaller ones with fewer layers of review, this makes it all the more important to check outputs with fresh eyes\. Not just for factual errors, but for tone, emphasis, and framing\. If a section of an AI\-generated report reads exactly how you would have written it, ask whether that is because the logic holds or because the tool has simply learned what tends to pass through unchallenged\.

The most convincing outputs are not always the most correct\. They are just the most comfortable to accept\.

## <a id="_iprwgzc33acr"></a>If You Let Client Pressure Set the Boundaries, Shortcut Thinking Becomes Generic Thinking

AI does not just learn from what we do\. It learns from what we prioritise\. The choices we make under pressure\. The steps we skip when the timeline tightens\. The corners we round off when the client starts getting twitchy\.

That is where things get more complicated\. Because while it is easy to spot bias in datasets or frameworks, it is harder to catch when it is creeping in through workflow\. Especially when the work still looks good\.

Over the past year, there has been a quiet shift in how client expectations are managed\. Faster timelines, broader scopes, less appetite for “we’ll come back to that”\. In the past, agencies would raise a flag\. The project would expand\. The timeline would stretch\. Or the brief would get cut down to fit\.

Now? The AI will handle it\. Or at least, that is the assumption\.

The temptation is obvious\. Why push back when the tools can help you absorb it? Why challenge a deadline when the summaries can be generated in minutes? Why worry about overreach when it all still fits into a tidy deck?

But every time a decision gets shaped by external pressure rather than research judgement, the system learns from it\. The tools begin to normalise work that was only ever meant to be a compromise\. And once those compromises start getting repeated, they stop looking like trade\-offs\. They become templates\.

This is where AI becomes risky in a different way\. Not because it fails, but because it starts to succeed at things we never intended it to do\.

If every brief that should have been qual gets rewritten as a chatbot because it is faster\. If every client request gets folded in instead of being re\-scoped\. If every ten\-day turnaround becomes five without anyone adjusting the expectations\. Then what the AI learns is that this is how research is done now\.

That would be fine if it always worked\. But the pressure that AI absorbs does not disappear\. It just stops being visible\. The risks that used to show up as workload or compromise now get baked into the process itself\. The outputs look smoother\. The slides come faster\. But the thinking underneath starts to thin out\.

What makes this harder to spot is that the work still feels usable\. The formatting is fine\. The tone is familiar\. It all looks like insight\. But over time, everything starts to blur together\. The outputs begin to mirror one another\. The ideas sound good, but increasingly predictable\. And if that predictability goes unchallenged, it becomes a feature of the system\.

This is not a call to reject automation\. But if all AI does is help us say the same thing, slightly faster, we are not making progress\. We are just moving in circles\.

This is where smaller agencies can lead\. You are closer to the moment when expectations get shaped\. You have more control over how you push back\. Just because the tool can deliver does not mean the ask makes sense\. The value of AI is not that it can make everything easier\. It is that it creates space to make more conscious decisions about what should be delivered in the first place\.

### <a id="_ip3h93no11hf"></a>__If We Train It With Care, It Can Push Us Forward__

AI reflects what we feed it\. That is the risk, yes\. But it is also the advantage\.

The most useful thing about these tools, particularly for small and medium\-sized agencies, is that they learn quickly\. You do not need a dedicated engineer or a custom model to shape the output\. You just need to be deliberate\. The tool takes cues from what you approve, what you reject, what you change, and what you reuse\. Over time, that becomes its model of how you work\.

That is where the opportunity sits\.

If you want the tech to reflect your values, your standards, and your attention to context and clarity, it needs to be shown those things\. Not once, but consistently\. If a summary feels too polished, edit it\. If a theme feels generic, reframe it\. If the structure of an AI\-generated deck feels passable but dull, rewrite it\. These are the choices that shape what the tool learns to prioritise\.

This does not need to be formal\. You do not need to stop your workflow to train a system\. You just need to use the tools in a way that makes it clear what good looks like and what does not\.

This is also where AI can become more than a time\-saver\. It can reveal habits that need rethinking\. It can show you how often a structure repeats\. It can prompt you to check whether a phrase still works, or whether it has simply become familiar through overuse\. It can help you notice what has stopped being useful, even if it still sounds right\.

If the work you produce is thoughtful, your tools will learn to reflect that\. If the work is rushed, they will learn that too\.

The technology is not trying to define what good research looks like\. It is simply copying what it sees\. That is why it matters to show it works; that is worth copying\.

You do not need to train it perfectly\. You just need to train it on purpose\.

__What are your tools learning from you right now, and what should they be learning instead?__

If you're ready to turn this into action, we’ve put together a short companion piece: __Preventing AI Bias: How to Make Research More Reliable & Ethical\. __It includes practical steps for building more thoughtful workflows, training your AI tools with intention, and protecting the quality of your work without adding unnecessary weight\.

Because reflection is good, but improvement is better\!

