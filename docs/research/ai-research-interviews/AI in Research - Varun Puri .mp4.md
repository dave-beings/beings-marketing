  
**Speaker 0 (30:45 \- 52:25):** Hi, Hashi. Can you hear me? 

**Speaker 1 (2:20:55 \- 2:31:55):** Hi, repeat. 

**Speaker 0 (2:49:05 \- 5:32:45):** Hi. Uh, do you also want to ask any, any question or you just want to see, uh, what are we talking? 

**Speaker 1 (5:47:45 \- 7:09:25):** No, I think I'm just going to follow your, uh, read. Hi, Varun. 

**Speaker 0 (7:16:55 \- 7:51:25):** Hello. Hi, VAR. Welcome. 

**Speaker 2 (8:08:55 \- 9:54:05):** Thank you. Thank you. How you gu? How you \<inaudible\>? Sorry, is Ida as well on the call. Thank, thank you for having me. 

**Speaker 0 (10:01:55 \- 10:46:05):** Yeah, we are. Well, uh, thanks a lot. Thank you very much. 

**Speaker 1 (10:54:35 \- 11:04:45):** Good, thank you. 

**Speaker 0 (11:19:45 \- 15:20:45):** And yeah, uh, I'm Gure, I'm, uh, product manager in research and insights at Beings, and Hershe is our marketing lead. And uh, ADA is not a person. It's our, uh, note taker. Oh, right, \<laugh\>. 

**Speaker 2 (15:48:35 \- 16:30:45):** Yeah. Well, I've known how to be polite to AI as well, so 

**Speaker 0 (16:43:55 \- 20:23:25):** Yeah. \<laugh\>, we appreciate that \<laugh\>. So she'll be happy. So yeah, she will be recording our, you know, uh, meeting. Are you okay with that? It's note taking purpose and this is, uh, strictly for our internal use. 

**Speaker 2 (20:30:25 \- 20:36:05):** Of course. 

**Speaker 0 (20:52:15 \- 27:41:25):** Thank you very much. Uh, okay. Yeah. So yeah, a little bit overview of our companies that we are developing, uh, AI driven products for market research. Uh, and we are going for product launch in a couple of months. So we are just trying to understand how we can make our products a little bit better. Mm-Hmm. \<affirmative\> some of critical concerns that people like you can have, uh, in the, in, in the field. 

**Speaker 2 (27:49:35 \- 27:55:25):** Right. 

**Speaker 0 (28:01:45 \- 31:06:05):** So, uh, yeah, uh, over to you. Please explain us a little bit more about your job and, uh, what are your day-to-Day responsibilities? And then, uh, I can ask a bit more tailored questions afterward. 

**Speaker 2 (31:18:25 \- 46:24:55):** Not a problem. So I am a research director at Cantor, uh, based outta London. And, um, I am mostly a quantitative researcher. I believe, uh, I'm trying to remember. Uh, the person who got in touch with me, uh, mentioned that this is mostly about qualitative solutions, but he said, I might be able to give you some useful input. Um, and I have been working at market Research for about, uh, 13 years now. Um, and it's mostly FMCG, um, range of products from yogurt, dairy, to alcohol to, um, um, to even, um, brands like L'Oreal, so personal care, uh, things like that. Um, but yeah, I think that summarizes what I do for a living. 

**Speaker 0 (46:53:45 \- 52:44:45):** Oh, very good. And very interesting. Thanks. And, uh, so over the last 13 years of period, uh, have you seen, uh, any, any other, uh, technologies that were like as disruptive as the AI is becoming recently and how the, uh, marketer sector is changing, uh, with these new emerging technologies? 

**Speaker 2 (53:18:35 \- 71:14:05):** No, not as disruptive. I, I mean, Canta has always been using artificial intelligence for predictive modeling, um, for, um, with embedded within the systems for, uh, digital analytics and things like that, but nothing quite as disruptive as generative ai. I think that is, uh, completely changing the landscape because, um, I think people make the mistake of looking at generative AI is just another machine, but it's so much more than that. Um, you know, especially since it's built on the neural networks that are very similar to how the human brain functions. So the entire idea of being able to generate responses, meaningful responses, deductive reasoning, if, and then reasoning, uh, very sort of almost mimicking, almost copying, uh, the way, uh, you know, human cognitive function is in itself a disruptive idea. So without a doubt, um, this is an inflection point. 

**Speaker 0 (71:24:35 \- 75:54:05):** Yeah. Right. Yeah, thanks. That's very true indeed. And, uh, when you say that you have been already using it for quite some time, can you tell me a little bit more, uh, that, are you developing in-House ai or you are very actively integrating whatever best is available in the market? 

**Speaker 2 (76:10:35 \- 89:33:05):** Yeah, so kta, I think because it's among the top five, I think they've got deeper pockets as far as sort of partnering with the big AI firms. Um, so integration does take place. Uh, more recently, integration. Before this, we were, you know, partnering with different sort of earlier forms of AI technology, like facial recognition, facial pattern recognition, sentiment analysis from that perspective. Uh, but now obviously with the big ones out there with open AI and Microsoft, uh, and, and things like that, uh, of course there are deeper partnerships now, uh, which is leading to the integration of that technology in a lot of the solutions that we have to order. Mm-Hmm. We have, we, we, we have to offer our clients. 

**Speaker 0 (89:54:25 \- 95:19:15):** Yeah. Thanks. Yeah, the, you raised very interesting, your point about sentiment analysis. So I'm wondering if you can tell me a little bit more that, uh, the sentiment analysis that you are talking about. Is it like text-based sentiment analysis or facial recognition and understanding the facial impression and then trying to do some sentiment there? 

**Speaker 2 (95:30:55 \- 113:18:45):** So we've always had, uh, and this was way before generative AI became a thing, but we've always had facial coding technology. So to be able to gauge people's reactions when they're looking at an ad mm-Hmm, \<affirmative\>, and to see what excites them, what makes them angry, what makes them sad, what makes them happy, um, you know, in different parts of the ad is something that we've always, uh, had. So that's not new. I think the way it's advancing now is the combination of that technology along with the interpretation of that and the implications. That's where generative AI comes in, because before that, we had, we looked at the information and we interpreted ourselves. Now we still do that, but now, obviously with Gen ai, I think, um, it'll just enhance it further, you know, with it, being able to also read the data and, uh, give us our own conclu give, give us its conclusions, you know, especially about detailed things that might be missed by the, by the human. 

**Speaker 2 (113:28:25 \- 134:44:45):** Um, so that's what it is. Now as far as sentiment analysis concerned, like I said, like, you know, as of now, um, from a qual perspective, I'm not quite sure what is happening at Cantor, um, because I'm not too close with the qual team, but I can just guess what's happening, right? As far as sentiment analysis concerned, um, to be able to, uh, not only read people's emotions implicitly without them having to say anything, uh, but to also be able to look at text-based analysis where you've got, uh, a range of data, let's say, from millions of conversations online on a specific topic. And to be able to read the emotion into that, the, the, the themes that are emerging and the kind of, uh, positions that are being taken. And that's something AI can do at scale and very rapidly. So it's, I, you know, I don't think I'm saying anything new here, but then if Canta doesn't take this route, then uh, we wouldn't be in the top five. Yeah. So I have a feeling that it is going to happen, and it is happening right now. There's probably somebody behind the scenes who's trying to figure out how to, how to put it all together. 

**Speaker 0 (135:09:35 \- 139:10:05):** Right. Thanks. And, uh, under your research responsibility umbrella, let's say that, are you a hundred percent focused on quantitative research or you also collaborate with Qual team, or you help and do you work on common project and 

**Speaker 2 (139:25:45 \- 166:03:05):** Yes, that's right. So we do collaborate with the Qual team. Um, what we do is we, you know, uh, as, as a, as a director, what I do is I make sure that in certain projects that require depth, I, uh, bring in a qualitative element, and I'm very much involved in shaping the discussion guide, uh, to ultimately when the quality team comes and gives me their report on, um, the findings, integrating those and, and guiding the responses to those reports, but also integrating that into the QU data. So that is something, uh, I, I do Now on the subject of sentiment analysis, I also wanna, um, uh, talk about, um, entity recognition, right? Um, to be able to, uh, recognize groups of, let's say people or certain organizations when it comes to, um, uh, and I think there's a lot of that that's being done with our digital analytics as well. Uh, we do have a platform at the background that sort of brings it all together. Uh, when it looks at conversations, let's say we wanna understand what is the future of alcohol in, uh, in a country, and we want to sort of, uh, understand the, the, the, the longitudinal trends and the predictive through a predictive lens, then what we do is we, we capture that information, and then obviously there's an AI model in the background that is able to slot things into certain spaces for further analysis. Anyway, um, that was on sentiment analysis. Yeah. 

**Speaker 0 (166:22:55 \- 172:05:55):** Very good. Yeah, thanks a lot. And, uh, if we have to talk about the integration of, uh, different AI technologies in your world, uh, can you gimme some examples that, uh, uh, what are the specific AI tools that you have picked up from the market in the last couple of years, and what kind of impact they, they had on your research, uh, work? 

**Speaker 2 (172:32:15 \- 191:35:55):** So we do have a, uh, we don't pick up AI tools from the market. So what we, we actually use, um, a partnership, uh, to, to obviously gain access to the technology. We've got our own team that is working with the partner in order to, uh, you know, create solutions, just like what you're trying to do, like product solutions, right? So we've always had a certain way of concept testing. We've always had a certain way of, um, ad testing. We've, we've, we've got something called Kaya now, and I'll go through that in details. But for concept testing, the advantage that we have is that we've been doing concept testing for donkeys years. We've got a, a database of millions and millions of tests that have been done over time. Yeah. Um, and we've got massive norms in databases. We've been able to, um, uh, to collect, uh, in a way the sentiment, uh, and the, the responses to each of these concepts when they were evaluated at the time. 

**Speaker 2 (191:54:55 \- 210:40:45):** And that forms this almost like this reservoir, this repertoire of, uh, knowledge that goes into, let's say a concept, uh, the concept AI that we have. So now, without having to actually, um, go and collect a new sample of people using all that knowledge, we can actually predict what a concept, our concept will fare in the market. Similarly, with ad testing, right? We've got, uh, our product is called Link ai, and with it uses the same repository of sort of all the ads that Canta has tested over time. So we, it's, it's very much grounded in human responses, and it's using all those human responses to sort of predict how an ad will perform in the market without even, uh, uh, you know, recruiting or respondents \<inaudible\>. And then we've got something called Kaya. Kaya is, again, a bespoke tool, uh, for each client that essentially takes all the information and the consumer information they have, uh, and that we've done for them. 

**Speaker 2 (210:48:25 \- 224:26:05):** And we've got some clients who've been with us for, you know, decades, and there's a lot of information there. Uh, and it's almost like, uh, their own, uh, chat bot tool where you can, you can type in whatever question you want about whatever brand within your portfolio, and it gives you the latest information, or it'll give you trended information, it'll give you charts, it'll give you analysis, interpretations, implications. Mm-Hmm. \<affirmative\>, we've got that too. Um, and for more information, you can just have a look at the kta website. I'm sure it's all, all over there as well. Um, what else do we have? Um, there was something else that was, you know, a recently launched product I'm trying to remember. Uh, but anyway, you get the idea, the, the, these products that have been created for this purpose. Yes. Yeah. 

**Speaker 0 (224:26:05 \- 238:26:45):** Right. No, thanks, Laurie. That's very interesting. And, uh, just a quick follow up with your predictive nature of your research, when you're trying to understand how the ad could have, uh, uh, how, how the ad was received in the past, uh, and how, uh, the new ad might behave, uh, in, in the future. And you're using your past experience, but the past experience, uh, is based on a different kind of human interactions that had a very different experiences, but the new ad is going to be launched in a new environment where the human insight or the human understanding could be very different. So how do you ensure the reliability of your predictive modeling that, uh, I, I'm trying to understand the success rate that, how good it is it at predicting the future and predicting the, uh, future vibe? 

**Speaker 2 (238:42:35 \- 247:14:05):** Yeah, and that's a good question. Now, what we, we, we wanna make a very clear to our clients that, um, when you're looking at something like a Link AI tool that is based on predictive modeling on human behavior, now let's, whether it's past human behavior or as future behavior, human behavior is human behavior. If you look at it over a period of time, there's certain things that make people happy. There's certain things that make people sad. Mm-Hmm. There's certain peoples that make people, uh, make, make, uh, that elicit certain emotions that drive a certain level of likability. 

**Speaker 2 (247:48:05 \- 265:35:55):** Yes, ads are unique, but there is a huge commonality factor with, with a lot of common emotions that are being elicited by ads. Having said that, we don't tell our clients that this is your go-to, we don't tell clients that, listen, you know, if you are going to test the massive campaign, and this campaign is really important for you, we don't tell them that. Do link AI instead Mm-Hmm. \<affirmative\>, we actually tell them we rec, we recommend, actually, since this is a new campaign, you need a a new sample. We, we need to make sure we get it right. If there are any outliers or unique aspects of this, we need to be able to call it out for those nuances. And, and so if there are certain use cases for this, yeah. Now with what we do tend to do with Link is sometimes, like, you know, the companies that we work at, whether it's the Unilevers or the Di Joes or the Colgates or whatever it might be, the non, sometimes they have many versions of an ad they've got, they might have five or six creatives to test, and they can't test. 

**Speaker 2 (265:47:45 \- 280:31:05):** It's not a, it's not easy to afford them. And so the first screening stage is when they shortlist the best ad to be tested so that they can guarantee that, you know, there's this chance of success. Mm-Hmm. \<affirmative\>. Um, and that's where they use it. It's not for the big strategic decisions, because that is, we wanna maintain that. Listen, at the end of the day, this is going to be the best bet that you have to give you a prediction. However, there is a margin of error. And that margin of error could be based on something very unique or a certain celebrity you might have used in this ad that is only popular now versus back in the day. And so if you want to capture those nuances and you're gonna be investing, uh, you know, over a millions on the ad, then you might wanna spend a few more thousands just to test it with a, with a set of respondents. 

**Speaker 2 (280:34:45 \- 293:23:15):** If not, and if you just wanna shortlist an ad, um, from a group of five or six ads, or even three or four ads, then maybe consider link AI and you'll get a good idea of what to list. Because even if it's 80% Right, that's good enough to know which ad is better than the other ad. Yeah. Right? And, and that's where all the commonalities come in, right? Because human behavior at the end of the day is, is very, it, it might, they might make it out to be very complex, but there's a very fixed spectrum of emotions that we've, like, we're talking about thousands and thousands of ads that have been analyzed over time. And you see very common patterns, em emerging no matter how different the ad might be, there's certain things that trigger an emotion. We capture that. So the universal truths, and then there differences. 

**Speaker 0 (293:39:45 \- 298:35:25):** Very good. Yeah, very well explained. Thank you. Um, so if now we zoom out a little bit and try to look at the bigger, uh, quantitative research area, uh, what are the potential limitations of, uh, AI insight that one can be concerned about? And do you really want AI to be more helpful? 

**Speaker 2 (299:21:15 \- 314:13:35):** The first thing that comes to mind is data quality. Now, I don't mean like with the technology that we have right now, especially with open ai, uh, potentially, uh, GP PT five, um, which is gonna be four or five times more powerful than GPT-4. I'm not concerned about the accuracy of its ability to do logical reasoning. Mm-hmm. \<affirmative\>. But what I'm concerned about is the data that's the, the data that's put into it. So if you're gonna have a model and you put rubbish in, you're gonna get rubbish out. You're gonna, you're gonna prompt in a terrible way. You're not gonna get anything meaningful out of it. And I think, I think that is the biggest concern, is not so much whether the technology is good or bad. The technology is great, is whether humans can actually, uh, uh, match up to it in terms of what they want from it and what information they put in. 

**Speaker 2 (314:17:55 \- 327:53:35):** You can put in, uh, you can create a bespoke model using, uh, uh, you know, GPT, um, and you can feed all the wrong information in its learning model, in which case it's gonna just give you rubbish at the end. It's not gonna give you the truth. It's gonna give you nonsense. So rubbish and rubbish out situation. So that is a concern. And obviously there need to be strict controls on that. Um, I think, um, complexity, so when GPT first became popular, my personal story is I picked it up the moment it was launched, and I started playing around when, and I was, and this was the earlier primitive models, right? And that itself was quite impressive what it was being able to do. Now, what shocked me was I was talking about this to my, my colleagues, and this is the case everywhere, right? 

**Speaker 2 (327:53:35 \- 340:38:15):** This is not only, it's not unique to, uh, some of, you know, my colleagues, it's, it's, it's, it's, it's, it's kind of, it's quite common for a lot of people to have dismissed it. So one and a half years ago, I remember start doing it, and I was like, listen, this is gonna change the game. It, you cannot treat it like a machine. It's basically copying the way you think \<laugh\>. And if it improves in technology, uh, if it improves in, in processing, uh, if it improves in its ability to make those logical linkages, it's gonna change the field. And back then they were like, oh yeah, this is like a science fiction thing today. We see what's happening. You know, there's outfits like you that have, you know, obviously taken this opportunity to say, we need to make something with it. And just like you, there's so many people out there canta on it now. 

**Speaker 2 (340:45:55 \- 356:52:05):** Um, and, and, and so I think the problem is the complexity. So I, the complexity of access, the complexity of how to deal with this thing, uh, now to me, who is, I'm inclined towards it. I am, uh, as far as I'm concerned, I'm an early adopter. And so these things excite me. Uh, and so I found it easy to sort of, to, to work my way around it, to, to, to understand what prompting is, to understand what the capability is, to understand actually the capabilities are endless. Uh, it's, it, it's, it's only as good as, as your imagination. And if you can really imagine, you can literally do a lot of things with this thing. Um, and, uh, I think the complexity, so any product that comes into the market, people are too used to having, uh, information put in front of them, uh, like almost like a silver on a silver spoon. 

**Speaker 2 (356:59:25 \- 369:58:15):** I think that's the, that's the problem. It's like a lot of people are gonna expect products to be created that literally do the thinking for you, rather than you actually shaping the thinking using it. Um, that is going to be a barrier to usage. So I would recommend that, you know, like, and Microsoft started doing it now, they started creating dumb products, I'd call them, that basically you don't have to do anything. It does it in the background for you anyway, and it just gives you stuff. And I think that's what is, you got any product that's created will have to be simplified to that extent, where it, it reduces the level of human interaction and increases the level of auto responses to, uh, to what, what, you know, what framework you want to serve. Uh, if that makes sense. 

**Speaker 0 (370:16:45 \- 391:42:05):** Yeah. No, yeah, you, you very, you raised very interesting, uh, point. So that's a very, uh, you know, very brilliant perspective. Uh, so, but, uh, there, there, there are like a lot of people who will think that, uh, machine needs a supervisor as well. Uh, I totally agree that, uh, if I'm asking dumb question, it'll gimme dumb answers. There is no question about the authenticity of that statement. Uh, but sometime, uh, if I ask if, if, if I'm coming from let's say, healthcare search background or, or different background, I might ask similar question, then how can I ensure that machine is giving me, uh, a bit more contextual answer? And if it's not understanding context of my questions or the background, uh, it can still gimme very generic answer. So maybe from machine's perspective, that's like very reasonable answer. Uh, human will look at that, uh, you can straight rule out that, oh, no, that's not useful for me. It's a very helpful for a FinTech person. But yeah, as a healthcare surgeon, this is not giving me any information. So where do you see the role of human in this whole overall chain to build trust in the, in a i outputs? 

**Speaker 2 (391:56:45 \- 406:13:25):** I think it's about, it's about, see, and I think it's, it's happening to a certain extent now anyway, but I think what AI needs to, what the, any AI needs to be designed, see, because the problem is humans are funny from that perspective. You know, we'll ask a question and then we'll imagine that the machine knows exactly what we're thinking. Now, if we were speaking to another human being, and we, again, like for instance, in the research field, when the client is giving me a brief on a, on a project, if they don't give me a good brief, I will not create a good research design and they will not get a good report. Right? It's as simple as that. If they give me a detailed brief, if they tell me who the stakeholders are, if they tell me what their marketing objectives are, if they tell me what their agenda is and how they want to shape, um, the outcome of their brand and their product, where they see it going, then I don't know what questions to ask. 

**Speaker 2 (406:21:45 \- 420:19:15):** Now, it's very simple. Now, when, when a human is dealing with a machine, we're not used to talking to machines. And so we don't give them that detailed information. Fair enough. It's fine. You don't wanna, you don't wanna treat this thing like, you know, you are briefing it, don't have to do it, in which case AI is going to have to meet the human halfway, right? And so you create follow up prompts. So you just ask a question, it gives you an answer, but then it also gives you sub-questions as, did you mean this? Would you like to know more about this follow up questions to elaborate on what the human might have wanted as well? And I think that is the one way to get the human, just to have to click on the, the list of potential directions that they can take the conversation in to elaborate on that. 

**Speaker 2 (420:26:15 \- 434:42:25):** But I think, you know, I, I, I, and that's the problem because a lot of times when I've tried to train some of my team on, um, prompting, um, it's, they ask questions as if the AI is able to read their mind and exactly what they want. And I'm like, \<laugh\> gonna be able to do that. You're not connected to it for much, you know, it's not being able to read your mind. So it, and that's the problem really, I think. Um, but that's what AI needs to do. And a lot of companies now are doing it. Like, you know, if you look at chat GP four oh, or you look at, um, um, I think Claude does it too. And I think, um, um, what's it called, the Microsoft one copilot co copilot does it too, but it's, they've got a limited set of sort of follow up. 

**Speaker 2 (434:44:55 \- 445:44:05):** Yeah. Questions. Would you like to know more about this? How, how about this? What is the next predictive thing? Mm-Hmm. \<affirmative\>, I think what I started doing initially was when I put in a prompt and before it had the follow-up prompting questions, I used to actually say, after the prompt, I'd say, listen, after you give me the answer, I'd like to also ask, uh, you know, I'd like to also list down the po potential directions that this, this topic can go in, um, and list them down for me too. So it'll gimme the answer, and then it would sort of tell me the different directions it can take. Very good. Um, and so I knew, okay, this is what I need to explore. This is how I need to go further, further That needs to be an automated process. 

**Speaker 0 (446:00:35 \- 459:06:35):** Sure. Yeah. I totally agree. Yeah. Uh, very interesting. Thanks. And, uh, if, if, if we talk like the, if I talk about the AI applications in quant and qual research, like you are mostly in quant, but yeah, you are also dealing with qual. So where do you think, uh, you can see more useful applications of AI technology? I mean, in both sides you are having some kind of your project planning than your recording data. You are collect after collecting your, analyzing it in some way, you're interpreting, interpret, doing the interpretation of that data, uh, the whole year could be very similar, but the type of data is different. Hmm. So how do you think AI could be more helpful in, uh, in, in, in both of these areas and which one is more suitable? 

**Speaker 2 (459:16:55 \- 478:00:25):** So if I had to just think of qual for a second, I mean, it's already there, right? We've got transcription services, automated transcription services on the spot transcription, um, which saves a lot of time. You've got analysis with, uh, you know, uh, natural language processing models like GPTs that, uh, can, uh, essentially do a lot of text mining and predictive analytics. And from a, from a, for a large, uh, text, um, or what do you call them, um, transcriptions, pick out exactly what you, you want to talk about in the key questions. You've got summarization as well, tremendous time saving, how it can take, um, a certain topic that you want to explore, summarize the content, uh, at different places in the transcript so that you have one cohesive view and one, one combined view of everything. And of course, with presentation as well, to be able to create, uh, to able to generate PowerPoints, um, a you know, in that same process or to, or to generate PowerPoints, that puts all the insights down. 

**Speaker 2 (478:00:25 \- 493:07:35):** So all you have to do at the end is just have a quick look, see if it makes sense, make a few modifications here or there. Um, and that's it, you know? And so, you know, if you look at the, that project life cycle, this is what I would think of doing transcription analysis, summarization presentation. How does it sort of integrate into the entire process to take something that would normally take about, um, two or three weeks, um, just one week now, because the discussion guide can be created, uh, using generative ai, maybe just need some iterations at the end to, to make it perfect, um, you know, limited human interaction, uh, and time saving. The only time that it would take would be to do the fee. Like, you know, let's say if you were having a focus group interview or you're going to have a an DI, those are gonna take the amount of time that they do, but everything else, top and tail has shrunk. Yeah. 

**Speaker 0 (493:16:35 \- 496:02:05):** Tha thank you very much. Do you mind telling the name of the tools that you're using for the various steps of these, these projects? Or, or even if you're not directly using it, but somebody else in your team? So, 

**Speaker 2 (496:07:05 \- 516:30:45):** So we are tied up with, with, uh, with Microsoft. Mm-Hmm. \<affirmative\>. So, so what we use is we do use copilot. Uh, but that's only because, listen, the other issue is the challenges, your data privacy issues, right? Because we're dealing with such confidential data from clients. And so now because we've got that partnership with Microsoft, we know that, uh, I mean we, it's a guarantee that the, the data is held privately. It's not open source, it's not going out anyway. It's within the, within the, the, the KTA cloud. Um, and so, you know, and even then we take, we, we take precautions, right? We, we mask the brand names, for instance. Yeah. We don't, we don't put it out there yet. And like, I think it's because we're still, a lot of people are still because it's highly sensitive. Uh, and so it just, even though we, you know, there's an agreement and it's all private and it's all secretive, uh, and it's all, uh, you know, privacy is insured, um, uh, you know, we still tend to mask the brand names, um, so that there is no indication at all that this belongs to a certain client. 

**Speaker 0 (516:47:15 \- 524:00:45):** Okay. Yeah. Very good. Um, when you mentioned about, like, uh, you, uh, about the qualitative research about a transcription summary presentation, data analysis, and, uh, at the end, you want to get like automated, uh, PowerPoint presentation as well. Do you already have that tool that can be, uh, where you can feed your data and, and it can spit out some PowerPoint presentation depending on the different kind of audience, or that was a wish list 

**Speaker 2 (524:15:05 \- 531:50:45):** For Qual? I'm not sure. So I can't, I can't speak for Qual. I'm pretty sure they're working on it. Um, but for quant, like for instance, for the concept stuff and the link stuff, and we've got, we've got automated stuff. So, um, we've got something called Cantor Marketplace, uh, which has a whole range of solutions, and it creates a dashboard, and that dashboard has all the data beautifully, uh, laid out, and then you just download it and it creates a presentation for you. 

**Speaker 0 (532:18:25 \- 536:34:05):** Uh, the dashboard, create a presentation. Can you like talk to dashboard that, okay, I'm going to talk to my research software engineers and give me a bit more technical information. Or if you say that I'm going to talk to the management board and gimme high level information, can you have this kind of feature in your dashboard or not yet? 

**Speaker 2 (536:43:05 \- 537:40:45):** Uh, uh, it's not yet. No. Mm-Hmm. \<affirmative\> that doesn't exist yet. 

**Speaker 0 (537:56:55 \- 544:40:45):** Right? Thanks. Sorry, just one last question about, uh, uh, if there's like no, uh, hurdle from technical or finance point of view, uh, what, what, what would be your wishlist do you want to see from AI to solve, uh, research at scale, uh, both for quantitative and qualitative? What are the things that you really want to see if there is no bond, but, 

**Speaker 2 (544:59:15 \- 565:40:05):** Well, I'd like, I'd like more interdisciplinary collaboration. Mm-Hmm. \<affirmative\>, right? So when, you know, because we've got so many domains in Canada, and we've got so many experts in so many, um, brilliant people from different fields, um, it would be good to, to to have, uh, an AI general platform that makes it easier to tap into everybody's brains. Like, you know, if you wanna, you want to collaborate with somebody who's in shopper, you wanna collaborate with somebody who's a creative expert, you wanna collaborate with somebody who's in media and with consulting to be able to, you know, if you've got a, an AI platform that facilitates that seamlessly, that makes it also easy to build on each other's expertise and work to create something brilliant for the client, that is something that would be really good. Mm-Hmm. \<affirmative\>. Um, I think, uh, more it's, it's, it's also about sort of, you know, whether we are actually, and I keep thinking like, like I said, I'm always, I always feel that, um, the technology that is just being launched, it takes a little bit of time for big companies to sort of get to that. 

**Speaker 2 (565:46:55 \- 575:08:45):** Right? Now, I'm not saying Canta iss behind Cantas, actually, uh, among the companies that's innovating quite fast, but I always feel a little disappointed. I'd like, you know, uh, you know, what can we do, um, to, to, to create more enhanced insights? What can we do to, to upgrade the tech so that we can sort of integrate it into everything that we do from a process perspective to a brainstorming perspective, to a meeting, uh, facilitation perspective, uh, to just, because again, I'm not looking at AI as a replacement to humans. I'm looking at it as a way to augment humans. 

**Speaker 0 (575:15:55 \- 575:26:05):** Sure, sure. 

**Speaker 2 (575:32:35 \- 587:37:55):** Just like it's augmented me, right? Over the last one and a half years, I have, uh, I, I, I'm, I'm proud to say that my performance has been augmented by ai. It's not replaced me, but it's made me a better version of myself. Yeah. So, so that's how I look at it. Uh, personalized research, adaptive AI methodology methodologies, right? So when you're speaking to people, um, you want to, and you're interviewing somebody and the interview is going a certain way, but they bring up a different topic to be able to, on the spot change the questions based on that conversation Mm-Hmm. \<affirmative\>. So that you can, you can make the question and more adaptive, uh, and actually make it more meaningful. Yes. Uh, from an insights perspective. Those are some of the things that I think would be interesting. 

**Speaker 0 (587:45:35 \- 591:40:35):** Yes. I think those are the things that we also see the rise of a small language model or personal language model. Right now we are highly dominated by large language models, but slowly, when people want a bit more personalized experience, we are heading into that direction as well. 

**Speaker 2 (591:52:25 \- 592:29:55):** Yeah. Very good. 

**Speaker 0 (592:41:35 \- 594:56:05):** Uh, yeah, that's, I think, uh, everything from, from my side. Uh, thank you very much for your time. Do you have any question for, for me? 

**Speaker 2 (595:15:35 \- 599:51:15):** Uh, so you, you mentioned, uh, you're gonna be launching, uh, products shortly. Yes. Um, and, uh, is this, uh, are you looking to launch in specific markets or is it just all over the, all over the world at the same time? Something that people could just download from, from your website? 

**Speaker 0 (600:05:45 \- 614:18:35):** Initially we are targeting, uh, the uk, uh, and mostly this sector. And, uh, since it's a market research tool, so, uh, currently we are not saying that this is more specific to healthcare or FinTech. Currently we are open, it's a minimal viable product. Uh, we are trying to do a little bit better than other tools existing in the market. Uh, we are not just providing transcript and a summary. That's definitely the part of it. But, uh, we also want to help with the, uh, things like a project planning. You're starting from product, uh, what kind of things you want to plan. And once your product planning is ongoing, uh, you want to recruit participant, we will have a, a very smooth integration with the other, like a prolific center, uh, Athena, those kind of tools that are providing you, uh, participant. And then we provide, we will provide overall video recording tool. 

**Speaker 0 (614:28:55 \- 630:03:15):** Uh, so you can also integrate it with the Google Meets Zoom or Microsoft team. But if you want, you can use our own video recording tool, uh, that give you the possibility of even better resolution in case, uh, better facial information is a necessary for you to run some kind of, uh, you know, facial sentiment analysis that, uh, as well. And then, uh, you will have your, once you are recording your data, then uh, we are developing, uh, bit more automated data analysis framework. Uh, you will, uh, we are trying to get as much as possible quantitative information from qualitative, uh, research as well, so that, uh, you can turn it into some kind of numbers and plots and figures. And, uh, then at the end, uh, depending on the target audience, it can give you personalized, uh, uh, you know, presentations. Hmm. So that thing will happen slowly. But yeah, we will launch the MVP in a couple of months. 

**Speaker 2 (630:10:25 \- 630:41:55):** Okay. Very good. Going 

**Speaker 0 (630:41:55 \- 630:44:35):** In. 

**Speaker 2 (630:54:55 \- 636:03:55):** So this is, this is gonna be, so you're saying that right now in the UK you don't have an end-to-end solution. Yeah. And you wanna be the first solution? Yeah. Yeah. Okay. Well, good luck. You know, I mean, I think it's a great, uh, it's a great initiative. Uh, I'm always supportive of ai, so I hope, uh, I hope you succeed. 

**Speaker 0 (636:18:05 \- 636:55:55):** Thank you very much, RA. Yeah, we will keep you posted. 

**Speaker 2 (637:08:05 \- 638:04:35):** Thank you. Appreciate, thanks. 

**Speaker 0 (638:14:05 \- 638:19:55):** Thank.