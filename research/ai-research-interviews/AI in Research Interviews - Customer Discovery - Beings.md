# Market Research Interviews: Qualitative Researchers

## Customer Discovery

# Key Ideas:

* List of references for AI-suggested ideas: Rytis, Lisandro    
  * Better results with Perplexity.ai, haiper.ai   
* Synthetic data, simulations: Sarah Smith, Tak Ha   
* Multilingual Translation features: Vishen   
* A quick potential use case could be at the Project planning stage: Most of the participants    
* Data security and privacy. Ensure that there is NO AI model training on personal data  \- George, All   
* AI tools giving an expected timeline of projects, etc. (redefine question…)  
* Find a way to increase trust in AI outputs, reduce any kind of bias in the results   
  * Provide a feature like: Human approved AI insights (?)  
* AI tools should understand context and accents and give a uniquely appropriate response to questions in different fields.   
  * Pre-training, prompt engineering, small language model, etc   
* Possibility to record only audio of the video meeting if the participant is ok with sharing the video in the meeting but doesn’t want to let it stored in the video recording. E.g.  
  * Audio+screen only  
  * All: Audio+video+screen  
* TBC

# 

# 

# 

# 

# Ionela bulceag

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:00:  Research and insights at Beings. It's a startup company. We are developing AI products for market research, and we are talking to experts like you, trying to understand that, uh, uh, we are learning more about your jobs and responsibilities and how you think you can use AI to facilitate various tasks, uh, associated with the, with your job. So please let me know a little bit more about your job and, uh, day-to-Day responsibilities, and then I can start asking more questions.  
**Speaker 1** 00:00:31:  Sure, sure. Um, I'm a research manager, uh, within the qualitative team, um, within an international market research company based in London. Um, I've joined the company just last year. Um, so I've been involved in, um, in, in, in managing qualitative studies from like the start writing proposals all the way up to, to the end. So reporting and presentations to clients. Um, we conduct lots of, uh, interviews, focus groups. Um, we also do some more, um, innovative. We use some, some innovative methods as well, such like online communities. Um, uh, we, we use an app as well, uh, to collect data from participants. Um, and we also starting integrating AI as well, uh, in our surveys, um, to collect qualitative data, um, as well as for, uh, data analysis.  
**Speaker 0** 00:01:27:  Great. Thank you very much for covering a lot of topics. Uh, so when you say that you're using AI in, uh, in your, uh, can, can you be a bit more, uh, can you gimme some examples? How are you using it?  
**Speaker 1** 00:01:40:  Um, so we've started, um, so we, we work with, um, an AI company and they've developed a tool that's, uh, it's for market research. Um, and it's very, um, secure. It's, yeah, it's, um, it doesn't, um, share data like, um, externally, I guess like with other, like other US companies. So it's very secure, which is great for us. Um, so how it works is that we, uh, we add qualitative questions at the end of a survey, um, and then we, we use AI to analyze this data. So we have a platform that we share with clients as well. And, um, it shares like common themes, um, like how strong, um, those themes are coming through. Uh, and the client has access to like the, the open end, the quotes, which is yeah, great for them. And it's really, um, it's really, um, robust and it's also, um, yeah, it's quite time seeing as well, so we can, um, we can do the analysis like in, in two days maximum and we can analyze, yeah, hundreds or thousands of, um, responses. Mm-Hmm,  
**Speaker 0** 00:02:47:  \<affirmative\> very good. Yeah. That's very interesting. And so you're using it only at the analysis time, or are you also taking help to create your service, uh, from ai?  
**Speaker 1** 00:03:00:  Um, so we sort of like testing, um, some AI tools that could help us with that. Uh, so we have like a, um, like key people within, within the team, uh, across the company that are currently testing some AI solutions. I've tested that for a qualitative, uh, work, but, uh, I think like some people in the quantitative team have tried to play with it as well.  
**Speaker 0** 00:03:25:  Mm-Hmm, \<affirmative\> Okay, great.  
**Speaker 1** 00:03:27:  But we haven't, uh, we haven't implemented, uh, any tool for, for that at this point.  
**Speaker 0** 00:03:33:  Right. Thanks. And, uh, what about the qualitative side of this research? Uh, how are you using AI in that aspect?  
**Speaker 1** 00:03:44:  Um, so as I've mentioned, like we, we are using this platform to, um, to analyze data we collect in our surveys. Um, and we use, use this for like, uh, yeah, for the, uh, qualitative, uh, questions that we add to surveys. Uh, we've also tried to, to, um, to analyze, um, a larger volume of data for, for instance, like from interviews or like from focus groups. Uh, but yeah, the platform is quite slow with that. So we've tested other solutions, but again, like we haven't implemented any within the team. I think obviously there's some concerns around like security, um, and also, um, yeah, how data is going to use to train models. Um, so yeah, we have like decided on that. And also, um, there's also, uh, the issues around like, uh, like the models, um, making like generalized statements that are not based on the data. So, uh, I'm not sure what's the industry term for that. Um, yeah,  
**Speaker 0** 00:04:45:  I think it's, they call it like hallucinations. Hallucination,  
**Speaker 1** 00:04:48:  Exactly. Yeah. Um, so there are some concerns about that, but obviously like we cannot share key findings or insights with clients that are not based on, on the actual data. So, um, yeah, we're trying to test a few solutions for that.  
**Speaker 0** 00:05:05:  Right. Great. So, so, so currently you're using it, uh, for analyzing servers because you feel the information is more streamlined there and you're just trying to get meaningful insight there. But when it comes to video, uh, you don't, you can't differentiate, you can't have trust that if it's still talking about your product or if it's growing random information about from its previous knowledge. Did, did I understand correctly?  
**Speaker 1** 00:05:32:  I think it's more about the volume of data. 'cause obviously for the surveys, we don't ask participants to write like paragraphs and paragraphs, uh, to answer those qualitative questions. So when it's a smaller volume of data, I think that platform can manage it well. But when we have like transcripts are, uh, a few pages, like 10 20 pages or the transcripts only focus groups, um, it just takes, takes it a lot longer to analyze it. And, um, yeah, it's very slow and it doesn't really help us much with that. And since you've been mentioned like videos, we also do a lot of online communities, um, and like one of the platforms that we're using is also implementing semi solutions. So we haven't, um, actually tried, um, started to using them. I think they, they're still trying to, to add it to the platform that, but that could be like another solution. Mm-Hmm. \<affirmative\>. Um, so it's going to, all the data is going to be automatic and analyzed in that platform. Um, and then, um, so yeah, but that was going to be managed by, by the platform, not by us, if that makes sense.  
**Speaker 0** 00:06:34:  So, no. Yeah, that, that's totally sensible. When you say platforms, are you talking about the external platform that's provid you, the services and, yeah. Okay. Right, thanks. Uh, okay. So, so if, uh, yeah, I'm just trying to understand that. What are the different ways for you to, and, uh, to continue your research? One is surveys where you can take help from AI to analyze. Another one is, uh, uh, audio or video recording when you're talking to people. Yeah. And, and the third one is, uh, online communities. Right? How, so how, how would you, uh, differentiate what kind of data you collect from online communities? Uh, I mean, how does it complement the other two parts? What kind of data you collect and how do you analyze information that you're collecting from online communities?  
**Speaker 1** 00:07:24:  So on online communities is we have like a wider range of tasks that we give participants. So, uh, we can, uh, have like polls, we can have creative tasks. We do a lot of like comms testing, uh, in online communities. Um, so it's very like visual. Uh, they can also upload different documents, audio. Um, so it's very, yeah, it's very interactive. It's very visual. So yeah, it's going to help us with like summarizing all that data that is captured, um, yeah. In, in that community.  
**Speaker 0** 00:07:58:  Yeah, that's, yeah, that's very interesting. I, and, sorry,  
**Speaker 1** 00:08:02:  Sorry. And the communities that are also for at least like 4, 3, 4 days. Um, so the AI is going to like, analyze Yeah. For each day. And also like, we have like different segments within that community, so it's going to give us like a more competi view of everything that's being uploaded, everything that participants say. Um, so it's going to speed up the process a lot for us.  
**Speaker 0** 00:08:24:  Yeah. Very good. Thanks. And, uh, what would you say about, uh, the overall share of these different ways, uh, of collecting data, either survey or video recording or, um, or online communities? What is the share of these three things in market research?  
**Speaker 1** 00:08:44:  Um,  
**Speaker 1** 00:08:47:  Yeah, that's very difficult to say. Um, I think like the, the ai, um, added to the service is just like a new feature that we've, we've tried to like sell to clients. So I think that's something that we've done in the last few months. So it's very new. So I think like it's, that's less, less than 10% right? Of the whole project. Um, I would say like online communities have been very popular in the last year, so I think like half of the projects that I've done, I think they've been online communities and yeah, we had to do like the analysis manually. Obviously there's some features on the platform, but it's not, AI is not enabled, so we had to do everything manually. And then, uh, yeah, focus groups, interviews. Yeah, they're still very, yeah, still very popular. So maybe like 40%. Yeah. Uh, yeah, because like colon communities have been very attractive to clients in the last year, but yeah, it's quite hard to, to answer it, it really depends.  
**Speaker 0** 00:09:44:  Oh, sorry, you're saying that it, it's hard for AI to facilitate focus researcher discussion?  
**Speaker 1** 00:09:51:  No, sorry, were you asking like the share? Um,  
**Speaker 0** 00:09:55:  No, sorry, yeah.  
**Speaker 1** 00:09:55:  Of these methods or,  
**Speaker 0** 00:09:58:  No, I wanted to understand like when we say research in market research industry, and I wanted to ask that, uh, how many percentage, uh, of the work is done through surveys? Like you said, less than 10%. That's a great answer. Then another 40 is probably focus group, and then maybe another percentage is like one-to-one interviews like we are having now. And then there is probably another share in market research industry when you rely on your data that you're collecting through online communities. Right. So I wanted to understand the different ways data is coming under this big umbrella.  
**Speaker 1** 00:10:32:  Okay. So I'm not sure if I can generalize the market research sector. So obviously like in qualitative, in the qualitative team that I'm in, and yeah, like surveys, yeah. There very, uh, small parts like we add, we've added this feature Yeah. Just like very recently. And, um, yeah, we added to like, we work with the quantitative team, like when they implement their surveys, we add this qualitative questions to that, so it's quite new. Okay. So maybe like 10% is that, or maybe even less than that, online communities, I would say like, yeah, 40, 40% of the projects use this methods, this method, and then, uh, I think like focus group would be, um, yeah, maybe like a 30% and then the rest is just interviews. Um, so does that make sense?  
**Speaker 0** 00:11:23:  Sure, sure, sure. Very. Yeah,  
**Speaker 1** 00:11:24:  It's a very like, rough estimate. They just, yeah. So  
**Speaker 0** 00:11:27:  No. Yeah, definitely. I mean, I'm not looking for like concrete number, I mean, whatever is your experience, uh, particularly through your job. I was trying to understand that. What is your perception? Uh, so yeah, thank you very much for that. And, uh, about online communities, uh, uh, is your current company running those online communities or you rely on some other existing, uh, online communities and you go there and you ask questions or, you know, try to get feedback from, uh, people who are not directly part of your company? How does it work? Who's the owner of those online community websites?  
**Speaker 1** 00:12:01:  Uh, so yeah, we work like with a third party and we have an, uh, a, um, a license for that. So we work with them, but like the participants are recruited from like our panel, um, and then like we interact with them, so they just provide the platform and that's it. Like they don't really inter interfere with the community at all.  
**Speaker 0** 00:12:21:  Great. Thanks. And can, can you, is it possible for you to tell me the names of, uh, some tools or platforms that are providing you participant provid for focus group, provid and for service and for online communities? I believe they're probably different one.  
**Speaker 1** 00:12:37:  So like we have our own, my company has their own panel of participants.  
**Speaker 0** 00:12:43:  Okay. So  
**Speaker 1** 00:12:43:  These are recruited from our panel. So this our, our own participants.  
**Speaker 0** 00:12:48:  Well, even, even through online communities, even if it's a third party? Yes. Yeah. Okay, great. Thanks. And, uh, uh, what, what is the incentive for them? Why do they want to participate in different kind of research?  
**Speaker 1** 00:13:03:  Um, I mean, obviously like we, we give them a voucher for any activity that they take part in. Um, and then obviously like we, we work on like a lot of like important issues. Like we produce a lot of like p public sector. So yeah, I've seen like, are very interested to give their opinion on like, current issues. Um, like the projects are super interesting, so they seem very engaged, like throughout. Uh, so I think like, just like the nature of the projects, the nature of, uh, the research that they're doing is very interesting to them and obviously like we try to maintain that relationship with, with them. Uh, and yeah, we try to keep them engaged. Mm-Hmm. \<affirmative\>.  
**Speaker 0** 00:13:41:  Yeah, that sounds great. Thanks. And, uh, would it be possible for you to tell me the name of the AI tools that, uh, you have, uh, that you have been using or the platform?  
**Speaker 1** 00:13:54:  Um, yeah, I'm not sure if I can give you that. Like, one of them is obviously like, yeah, it's the feature that we use, like we're trying like to sell it to clients as well. So I'm not sure if I can Yeah. Give it the name.  
**Speaker 0** 00:14:06:  Okay. Yeah, no, sure. Yeah, we can, we can, uh, we can ignore the  
**Speaker 1** 00:14:10:  Name. I mean, we, I can give you some of like the general ones that like we've tested, like we haven't implemented for instance, right? Like, uh, we've looked at like Microsoft, uh, copilot for instance. We've used, we tried to like use, I try to look at your GPT if Yeah. Secure, we can, yeah. Implement it. Obviously that that's not an option. We try to, like yabo for instance. Mm-Hmm. \<affirmative\>, uh, um, uh, uh, which one, um, clouds as well for analysis. So yeah, just we've, we try to look at like a wide range of options  
**Speaker 0** 00:14:47:  And, uh, no, that, that, that's, I think even better than the names of one you're using. So why didn't you, uh, why didn't you go for move forward with the either of, uh, these four options that you mentioned? Were there like some obvious concerns?  
**Speaker 1** 00:15:02:  Yeah, I think our data security is like an, um, a really big concern for us. We, like, we want to keep, uh, the data of our participants secure and we don't want to, to share that, or we don't want them to use the, the data to train their models. I mean, that's very important for us. It's important for the clients. Uh, so that's like a massive concern. Uh, there's some instances like where we didn't find the AI to be very effective and analyzing like a huge volume of data, so that was like another concern. Mm-Hmm. \<affirmative\> and I, as I've mentioned, like, um, hallucination and making like very general statements, uh, they're not based in the data and like we have no, um, guarantee that, um, like the, the, the insights that we're sharing with clients or like the findings that we're sharing are actually based on, on the data.  
**Speaker 0** 00:15:54:  Right. Thanks. Yeah.  
**Speaker 1** 00:15:56:  Yeah,  
**Speaker 0** 00:15:58:  Yeah. Great. Thanks. And now about the tool that you have been using or integrated, I'm not asking the name. You can ignore the name. Can you highlight, uh, the type of, uh, work it's, uh, doing for you? What are the main target, uh, like for qualitative research? Are you using it mostly for transcription or thematic analysis or sentiments? So can you gimme these kind of example where the AI tool is actually helping you?  
**Speaker 1** 00:16:25:  Yeah, so it's, uh, the tool that I mentioned that we are adding to our surveys. Uh, so yeah, it's doing thematic analysis, I think like sentiment analysis as well. Um, so it has like a dashboard where you can yeah, see the strength of that like sentiment and like the clients can see that as well and all, there's also an option to chat with the, uh, AI function. Um, and like the clients can do this, sorry, the clients can do this themselves and they can ask questions and interrogate the data further.  
**Speaker 0** 00:16:55:  Mm-Hmm, \<affirmative\>. Right. Thanks. And for, for sentiment analysis, visualization, is it just giving you like a, uh, that okay, 60% positive or 30% negative, 10% neutral. Is, is it giving you like a pie chart like this or it's a bit more serious, uh, also throwing you a word cloud or telling more about, uh, more specific about the meeting that what was positive or what was negative?  
**Speaker 1** 00:17:19:  Yeah, I think it can be more specific than that. Um, like yeah, there's, there are this general statement that it makes, uh, but then you can, yeah, like there's a chart and you can click on it and see like in more depth what the data is saying. You can also like add it, um, those categories, uh, and you can yeah, adjust it through the research, like the know Yeah. Based on the knowledge that you already have, so you can play with it a bit more.  
**Speaker 0** 00:17:47:  Yeah. Very good. Yeah. Thanks. Very interesting. And, uh, and  
**Speaker 1** 00:17:51:  I do think it has a workload as well. Uh, I haven't used it, but yeah, I, I remember seeing this function. Yeah.  
**Speaker 0** 00:17:57:  Okay. Great. And, uh, for like, if you want to do some kind of coding and tagging at the end, uh, with, to, to understand, uh, uh, something more to not just finding meeting summary or key insight, but, uh, running the coding and tagging analysis based on some few words, can you still play with that and, uh, run that?  
**Speaker 1** 00:18:21:  Um, no, I don't think, I don't think you can do tagging with this. Um, I think you can go in more depth and explore the categories or the subcategories, and then it going to link the, the actual transcript, so you can see on like, on the right side, like the actual transcript and the quotes. Um, but I don't think like you can Yeah. Tag, tag it yourself or  
**Speaker 0** 00:18:44:  Right. Thanks.  
**Speaker 1** 00:18:44:  Yeah. But like you can chat with the ai for instance,  
**Speaker 0** 00:18:49:  You can chat, you can ask questions. Yeah. And it'll still give you more or less similar answer of, uh, about the initial information that it has given to you, right? Like meeting summary, key insight and sentiment analysis, or can you get something more, can you get some kind of recommendation or the list  
**Speaker 1** 00:19:09:  Of Yeah, you can, yeah, you can, it usually gives recommendations and like key actions and yeah. Suggestions as well. Yeah, you can, you can.  
**Speaker 0** 00:19:16:  Right. Thanks. And can you give me, uh, one example of like whole product? Like if we zoom out from transcription and we look at the bigger picture, let's say you are planning a search project, like qualitative research project. Uh, what would be the ideal journey of this project? Starting from project planning to finding participant, then data coding and then data analysis, and then at the end, sharing those result with somebody. So what are the tools that you use at various steps? And, uh, what is the average time that you have to spend? If you can just give an example of something, um, from product planning to  
**Speaker 1** 00:19:56:  Where you use AI or some,  
**Speaker 0** 00:19:58:  Uh, well, you can give dramatic example, then you can also tell that, okay, here AI had helped, or here AI could have helped. Yeah. And in whatever way you feel. Okay. Or you can give two different example, one with ai, one with non-AI.  
**Speaker 1** 00:20:14:  Um, so we start with like writing the proposal, uh, for the client, either like in Word or PowerPoints obviously, like using AI for this could help or having like an AI tool to perhaps, um, propose like a discussion guide for a project in doubt that could be useful. I think this is like done by the researchers at the moment based on like previous experience, like previous projects. So like once we, um, we, in that project, we, we start like with the recruitment, and obviously as I've mentioned, like we have our panel, so like we recruit participants based on like a screener, which is like a short survey, um, ensuring that they meet all the characteristics that we need, and then we yeah, start designing the research tools. Um, yeah, this is, is just done in board and I, as I've said, I could, if, uh, we had AI to perhaps provide suggestions or, um, explore or like have a look at all the previous, um, documents previous, like research shows that we use, I think that could be useful.  
**Speaker 1** 00:21:15:  Um, so yeah, once like those are, um, developed, like we, we have the, the client, uh, signing them off. Uh, we start field work. We are the con like conduct interviews, focus groups or the communities for the participants. Uh, and this, these are done, yeah. On different platforms, as I mentioned for the online communities. We have, um, um, a third party that we are using. Same for like text-based focus groups. We have a third, third party that's providing that for interviews. We just do it on yeah. Zoom teams. So yeah, there's no need for AI there. And then, um, yeah, we move into analysis, uh, here. Yeah, it's usually done, um, on, on the platform. So like on the online community, they do have some function for the analysis, but it still still has to be done manually. Um, they're trying to implement some AI features, which, yeah, again, I, if they could do that, that would be great.  
**Speaker 1** 00:22:11:  Uh, but, uh, for, for others, like, for the focus groups, for the interviews, this are just done manually by, by the research team. Um, and yeah, we, as I've mentioned, we try to test a few AI tools, but like, we haven't implemented any at this point. Um, I think like this could be like, uh, a major area where AI could be really useful. Slice, scan everything and provide suggestions, provide recommendations to clients based on the data. Mm-Hmm, \<affirmative\>. And then we move into reporting, which is done either in in words or in, you know, PowerPoint. And again here, uh, if AI can, yeah, select those key insights from the analysis and then move it into different reporting template. I think that could be like an area which would be great to have support. I think like, um, we tested Microsoft Co-pilot for this, but it's just not advanced enough to, uh, to do that for us. And then at the end, we just present it to, to the clients. So this, does this help or  
**Speaker 0** 00:23:08:  No? Yeah. Very helpful. Thank you very much for, for very elaborate answer. And when you mentioned that, uh, for online meetings, you're using either Zoom or Teams, and there is no need for AI there. So how do you record that information and then how do you feed it back to your AI tool?  
**Speaker 1** 00:23:27:  Um, actually for, uh, we do have, um, so Zoom has a feature with AI transcripts, so yeah, that's, that's a good point. So we do have the AI transcripts from Zoom, um, and then we, the researchers analyze that. Those, they're not perfect. Uh, they still have some errors, but yeah, they're still, yeah, they're free or anything, like, they're a lot cheaper compared to professional transcripts. So yeah, we do use that.  
**Speaker 0** 00:23:51:  And what about teams and how do you record that or bring back to your AI model? Uh, sorry, your AI tool.  
**Speaker 1** 00:23:58:  Uh, teams, I think like the team also up uploads teams, interviews as well to Zoom and then receive the transcripts, I think.  
**Speaker 0** 00:24:06:  Okay. I  
**Speaker 1** 00:24:07:  Think sometimes we use like professional transcripts if we have to share them with clients.  
**Speaker 0** 00:24:14:  So the professional transcript means like, uh, some human you hire  
**Speaker 1** 00:24:17:  Yeah, yeah. A different agency. Yeah.  
**Speaker 0** 00:24:20:  Right. I thought that it's like, uh, some online transcription tool that you invite to meetings and that will automatically record transcript.  
**Speaker 1** 00:24:29:  Yeah. I think like some parts of the company use that as well. Uh, we thought, yeah, we, we use Zoom transcripts, which are okay enough for us. Yeah. Uh, then there's, we still have like some errors here and there, but it's, yeah, it's enough for the analysis.  
**Speaker 0** 00:24:44:  Right. Great. Thanks. And, uh, so like you started with the words like, sorry, no words means that, uh, either Google's, uh, Google Docs or Microsoft Word that you use for product planning. So sometime you go to chat DPT to ask for suggestion for product planning. Right. And then you have more than one type of, uh, sources that can provide you participant, I'm just trying to summarize if I understood correctly and, uh,  
**Speaker 1** 00:25:14:  No, uh, project planning. Yeah, I think that's still done by researchers, like on Excel, uh, on words. It's  
**Speaker 0** 00:25:26:  Totally manually,  
**Speaker 1** 00:25:27:  Sorry,  
**Speaker 0** 00:25:28:  It's totally manual still project planning.  
**Speaker 1** 00:25:32:  Yeah. Like we, that's done with the client, so it's, um,  
**Speaker 0** 00:25:36:  Sure,  
**Speaker 1** 00:25:36:  Sure. It depends on, yeah, it very much depends on the project. So we do have like a, we have step by step, um, yeah. Plan that we use for, for projects. So, but that's like an excel, like a gone chart. Yeah.  
**Speaker 0** 00:25:49:  Yeah. Right. Thanks. And, uh, in your qualitative, uh, in, in any given project where you are mostly conducting qualitative research, do you have to actively collaborate with quantitative researchers and, uh, how much information you share with them? Or what part of, uh, the work in a given project is played by quantitative researcher?  
**Speaker 1** 00:26:13:  Um, it's very much depends on, on the client and the project. So some projects are mixed. They use a mixed, um, methodology. Um, yeah, so that, like, usually like the, the surveys come first, uh, and then like we do like an in-depth exploration for qualitative research, or we try to do some initial exploration and we do some interviews or some focus groups, uh, and then the quantitative time, the quantitative team takes over and then they, um, implement the survey with like, uh, yeah. Mm-Hmm. Thousands of people. It, it very much depends Yeah. On the project.  
**Speaker 0** 00:26:49:  Yeah. Right. Thanks. And, uh, if there is no problem with the technology or funding, uh, and, and you have to give your wishlist, so from product planning to the end result of PowerPoint presentation, you mentioned few plus points that very AI could help, but if you have to design like, uh, your own AI tool that could help you, not just with the research and analysis, but also for the project planning, since you're a manager now. So you also talk, uh, with the, a lot of other people, different kind of stakeholder. So research plus management, if you have to look at the bigger picture. And, uh, tell me your wishlist. What do you expect from ai? Uh, where do you think AI could help you, uh, across this whole journey?  
**Speaker 1** 00:27:35:  Yeah, that's a good, that's a good question. I think like, uh, everything starts like from the proposal writing. So, um, it can, I think like AI could be used to maybe like identify, uh, like, uh, the proposal that have been won, uh, and perhaps like you can write like, I need this, I need this type of methods for this project. Um, what are some of like the, uh, examples from like the, the winning proposals that we used and like, can you adapt it to this, this project? And you can give it maybe like the background, the context with the client is what they're looking for. Uh, I think, yeah, that could be like a, a great addition. Um, I think like, um, yeah, idea. I, I think that would be like, uh, the project writing, uh, also like the analysis probably like the, the biggest area where like we would need AI to, to, to support with that. So really like summarizing all the data. Um, 'cause I think like that we take a lot, a lot of time to, yeah. To go through to everything. Uh, and then like write the reports. I think like that area could definitely be done more effectively using ai.  
**Speaker 0** 00:28:49:  Yeah. Right. Thanks. Yeah, that's brilliant. And at the end, when you're sharing information, uh, on average, what are the expertise of different stakeholders whom, uh, you have to share this result at the end? So you might have clients, you might have, uh, people higher up in the management, uh, roles in your team, and probably other researcher as well. So how do you, uh, communicate the same product with a different kind of audience that, do you have to prepare a separate presentation for all of them or do you have all this information shared somewhere on the cloud that you just share the link and, uh, anybody can see it? So how do you share the information at the end?  
**Speaker 1** 00:29:33:  So we don't really share a lot of the information internally, unless it's some best practices, um, that we share with the whole team or like other teams. Uh, so it's not really done internally, like all the insights are shared externally. So for instance, like public sector, uh, yeah, we work with like, um, the director of that particular department within like, yeah, within the government. So yeah, really high up stakeholders. Um, and then this, the reports have to be like very formal. They're, uh, they're usually done in board, so just like very long, uh, very like professional. And then like on the commercial side, uh, like we work with lot like companies, like within different sectors, so it has to be adapted to their, that particular needs. Uh, we used to, we need to use like the terminology that they use. Uh, they have to be more action orientated, more insightful. Like for instance, like the short sentences have to be shorter, more Yeah, just sharper in general. Um, yeah, it's very much depends on the client. Like we can be dealing with someone in their research team or their marketing team or, yeah, it varies I guess.  
**Speaker 0** 00:30:46:  Very good. Yeah. Thank you very much. So just last question now about the potential concerns. Do you see any potential concern like ethical or technological or any other concern that you feel like, uh, you're happy to go towards any new tool available in the market that can make my life easier or, but there is this potential concern that I'm really worried about? Uh, can you think of something like that, that could really hold you away?  
**Speaker 1** 00:31:14:  Um, I mean, we also do a lot of like, um, sensitive research, really like children, young people. So I wouldn't want AI to be involved in that. Um, I'd like conducting the analysis or, um, being too, too involved in the analysis perhaps. So it's, I would have concerns of of yeah, maybe about that.  
**Speaker 0** 00:31:44:  So does, does. Sorry.  
**Speaker 1** 00:31:47:  Yeah, I'm not really, I don't think I have, yeah. Any concerns because like on the recruitment side, we have the systems in place, I think like these are working like really great. So I don't see AI like stepping in there. Um, mm-Hmm. \<affirmative\> and beside that, um, it's just, yeah, analysis, reporting, I think as just as long as the data is not shared externally and, um, we have some guarantees around security, I think that's fine. I don't really have any other concerns, so. Yes.  
**Speaker 0** 00:32:16:  Right. Thanks. Sorry, just to clarify, when you said that, uh, sensitive data and working with children, so data related to children, uh, so is the main issue behind that still the data security that since this is very sensitive and AI could, uh, may not have control of that data security. That's the only reason?  
**Speaker 1** 00:32:37:  Yeah, I think  
**Speaker 0** 00:32:38:  That's, or or did you need something?  
**Speaker 1** 00:32:42:  Um,  
**Speaker 0** 00:32:44:  I understand your concern.  
**Speaker 1** 00:32:45:  Yeah. We, we also are discussing about like AI potentially like conducting the, the focus groups or like the interviews in the future. So I'll definitely have concerns about, um,  
**Speaker 0** 00:32:58:  Oh, okay. Okay. Yeah.  
**Speaker 1** 00:32:59:  And AI tool, like yeah, for instance, we do text-based focus groups. I would have concerns if AI would be Yeah. Uh, speaking with children, for instance, in, in that focus group.  
**Speaker 0** 00:33:11:  Oh yeah, I totally agree. Yeah. Thanks. Yeah, thanks for clarifying. Um, okay. Yeah, that was, uh, mostly, uh, those are mostly all of my questions. Thank you very much for your time. I really enjoyed talking to you. Do you have any question for me or our company?  
**Speaker 1** 00:33:31:  Um, so can you share about the objectives of, of this research and then how it's going to be, how you're going to share the insights or,  
**Speaker 0** 00:33:40:  Uh, yeah. The, uh, since we are developing the AI driven products for market research, and currently we are focusing on qualitative research area, we have developed our own video recording tool as well. Uh, and our own transcription services, which is ida, you can invite it into meetings and it can give you transcript, key insight, a meeting summary action item, and like you can talk to, uh, ai, uh, helper of the, uh, the tools that you're already using here. You can also talk to ida. So currently we, is it  
**Speaker 1** 00:34:15:  Like, like gong something?  
**Speaker 0** 00:34:18:  Sorry, what, what is gong?  
**Speaker 1** 00:34:20:  It's like a software we're using as well. I mean, not our team, but like other teams are using to Yeah. Do exactly the same thing. Like it's invited in meetings and then it provides like a summary, um, key action points.  
**Speaker 0** 00:34:34:  Yeah, yeah.  
**Speaker 1** 00:34:35:  Something like that. Yeah.  
**Speaker 0** 00:34:36:  Yeah. But, uh, but, but this is like just one product out of our whole business idea. So we are basically trying to provide end-to-end analysis, uh, that you can have product planning, uh, on, on our website. You can ask, uh, questions to AI that, uh, this stuff that you're doing on, in your World document or on Google Docs, you can do it on a website. And, uh, our IDA tool, the AI assistant will help you to, you don't have to copy paste the stuff to chat d to your Gemini. You can get AI assistance, uh, right in the platform. Uh, we will provide, uh, uh, ways to connect with a different participant through like prolific or respondent or acne or any other tools available in the market that can give you a link to connect to different kind of audience. Mm-Hmm. \<affirmative\>. And then if you're using Zoom team or Google Meet, you can invite IDA to their meetings or you can use our own video recording tool.  
**Speaker 0** 00:35:31:  Uh, and uh, then you can also use our tool for product launch as well, because it provide a continu, uh, simultaneous streaming on different social media platform as well. Uh, and once you have data, uh, it'll, uh, give you the possibility to talk to it, uh, to get sentiment analysis. Uh, we are planning to launch this product in, uh, in next couple of months, or most probably in September. So the whole, uh, idea of this discussion is that trying to understand from experts like you that, uh, what are your main, uh, way of working, how do you collect different kind of data? How do you work and, uh, what are the main pain points and, uh, are we going to solve, uh, those issues, uh, through our tool or not? Mm-Hmm, \<affirmative\>. So basically we are trying to understand if we can use your insights in the, uh, in defining our product roadmap. So this whole information, it'll be strictly, you know, used by me and, uh, other product manager and, uh, member of our, of our team internally. Uh, it'll not be shared publicly, it'll not be used for any kind of data training.  
**Speaker 1** 00:36:43:  Okay. Sounds good.  
**Speaker 0** 00:36:46:  Okay, great. And, uh, yeah, thanks a lot. And, uh, I will, uh, share the Amazon voucher on your email. Uh, it might take around eight to 10 days because, uh, yeah, our CEO is a holiday and once he's back, yeah. So yeah, uh, that's all.  
**Speaker 1** 00:37:04:  Yeah. Thanks. Yeah, it's very interesting interview.  
**Speaker 0** 00:37:08:  Thank you very much Jan, the rest of the day.  
**Speaker 1** 00:37:11:  Thanks, you too. Bye-Bye  
**Speaker 0** 00:37:13:  Bye-Bye.

# Ella Robertson-Bonds

Senior Research Associate at APCO 

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:01:  To, uh, Gemini or chat GPT.  
**Speaker 1** 00:00:04:  Yay. That's cool.  
**Speaker 0** 00:00:06:  So yeah, these are the kind of things we'll do. Uh, okay. Yeah. Thanks again, Ella. Uh, so I'm \<inaudible\> and my, uh, role in this company beings is the product manager in research and insight, uh, developing. Mostly we are developing mostly AI products for market research and, uh, talking to experts like you. What kind of, uh, different things, uh, could be solved Mm-Hmm. With the, with the help of ai Mm-Hmm. And I will just ask you, uh, a couple of questions along that direction. So can we start with you? Uh, you can explain, uh, please tell me about your job and, uh, what are your day-to-Day responsibilities and what kinda work you do, and then I can start asking more question after that.  
**Speaker 1** 00:00:51:  Yeah. Um, so I feel like my job is, um, so I work with the Insight team, uh, in London, well, um, I'm based in London, uh, but my insight team is, uh, we're like a global, a global team, part of a global company. Mm-Hmm. \<affirmative\>. Um, so, uh, so apco does, uh, sort of communications and public affairs and pr. Um, our team is global, so we're mainly based in the us. Um, and me being based in London as like a team of one, um, uh, I'm part of the Europe team, so there's four of us, and we basically cover Europe and the rest of the world. Um, so it's quite, in terms of kind of my job role role, it's really, um, is really varied. I do a lot of, uh, like I jump between kind more digital, um, digital research. We do quite a lot.  
**Speaker 1** 00:01:45:  Like there's a lot, there's a big appetite of that in London. Um, and that tends to be around sort of like corporate communications, um, and, and thought leadership for companies. Um, and I also do, uh, some, so like opinion research, so conducting, uh, interviews or conducting surveys, um, of various audiences. Uh, and that, that tends to be for either organizations or, um, or it can have like a governmental focus. So, um, in terms of kind of my career, I'm trying to steer it more towards like, working on those sort of governmental projects. Um, doing some sort of like development work, um, working on kind of like developing tourism strategies and, uh, and sort of like economic development, um, Mm-Hmm. \<affirmative\> as well. Um, yeah. And then also sort of in terms of, uh, in terms of kind of my role within London. Um, so I have a, uh, like a responsibility to, um, to communicate offering to, uh, the different verticals within the London team.  
**Speaker 1** 00:03:01:  Um, and try and integrate some sort of insight offering, um, within either the existing client accounts, um, or within, um, business pictures when they're coming along. Um, so we'll often see that there's, that people request maybe like a survey or interviews as part of something. Um, and so then that's when I'll be consulted. Or if we see that they want to, a client wants to, um, conduct a strategy, then maybe we can add, we can persuade them to incorporate an insight element, um, which will be something that may, that informs the strategy. Yeah, I think that question \<laugh\>,  
**Speaker 0** 00:03:47:  No. Yeah. Thank you very much. Yeah, thanks a lot for, for very elaborative answer. Uh, so when you said that you team of one here, uh, so do you have to like decide if, if they want you to do a survey or a qualitative research interview, uh, are you deciding everything by yourself, the content of survey or how whom to target, uh, who should be your participant, uh, how to run the interview? How do you manage all these, uh, technical details when you go for actually conducting the research?  
**Speaker 1** 00:04:17:  Yeah. Um, so I have a, so it's a team of one here, but we're very well connected to, um, so my director is in Paris, um, so I'm very closely connected with her. Um, and also the insight team that's in the us. So typically I would try and answer those questions myself based on, um, kind of the past projects that we've been working on. Um, and that does tend to be like a bit of a rhythm. Um, it tends to, yeah, things don't tend to vary that much. Um, it tends to sort of, yeah, follow some sort of pattern. Um, but then I would always check it with, um, check it by my manager, uh, my, uh, director or, um, another insight colleague in the us.  
**Speaker 0** 00:05:04:  Okay. Brilliant. And, uh, in terms of AI users, uh, have you been using AI at any of these steps of your work?  
**Speaker 1** 00:05:13:  Yeah, I feel like maybe I overly rely on ai. I feel like when think chat GBT went down for a little bit and I was, um, in a bit of a panic, uh, yeah. So I mean, uh, we use it. Um, I, I'd say, so the common times that I would use it are in, so like proposal writing, I find it very useful to, to sort of get \<inaudible\> on the page. So, um, we have, we have a system that we use internally that's a copilot, which, um, is like anonymized. Uh, and yeah, I dunno, there's some sort of connection with our company with, with that, for that to all be sort of okay in terms of the company data. Um, so we're encouraged to use that one. Uh, and I would use that to sort of get words down on a page basically of, um, I would like, I would draft something up, um, in a long-winded way, and then, uh, and then can like ask the AI model to make it a lot more simple.  
**Speaker 1** 00:06:21:  Um, and then I can kind of go in and adjust. Um, and then also in like data analytics, um, we use it for, so we conduct a lot of, um, we use, uh, what's it called? Um, the Open, yeah, open ai, the playground. Um, we use that one for, for interviews. So interview data, um, because it tends to be quite long-winded answers. We do quite a lot of interviews, then we can kind of ask it to pull out the key themes. Um, so we tend to find that our projects are very, um, they're quick turnaround. Mm-Hmm. \<affirmative\>. So, um, it really helps with that is sort of when, um, either once we've done an interview, then you can kind of copy and paste and, uh, yeah. And sort of get the highlights that we can then feed into the team very quickly. Um, or in terms of, yeah, sort of finding overall themes, looking at, um, conversations between interviews and working away.  
**Speaker 1** 00:07:29:  Um, yeah, sort of question by question. Um, then we use that. And then also for the quantitative side of things, then we use, um, we've used like a variety, uh, open AI we use, uh, sometimes, but I think we find that we, that one's better for qualitative. And then, um, Claude is a lot better for, as a platform for, um, uh, analyzing quant data we found. So that will be sort of like copying and pasting a, um, like a data set, uh, like a table a data, table one. Um, and then kind of asking, again, it's like those quick turnarounds when we're asked to provide like a top line finding, um, before we delve in into the, like, the nitty gritty of stuff. Um, then that really, like, that helps us fill out sort of, you know, we've done, we've done a 30, 40 question survey, and then we want to just get like one a four document that has the top line findings, and we'll use that to, um, to sort of very quickly work through the analysis.  
**Speaker 0** 00:08:38:  Right. Yeah, that's very interesting. You're very happy to hear that you are trying to use it as much as possible, whatever is allowed by them. So in terms of, uh, so could you please remind me the name of the tools that you used for different one? You said about OpenAI, but, uh, OpenAI means that you are asking CG, PT to  
**Speaker 1** 00:08:57:  Yeah, I mean, we use the playground one because apparently that's got our, I think we've got some sort of agreement with our company that it's, um, it doesn't, it's in like a closed loop and that it doesn't feed into, um, the, yeah. So we can use like company data basically without it feeding into the, I mean, I hope this is right \<laugh\>.  
**Speaker 0** 00:09:22:  So you mean that, uh, uh, if you're asking Chad GPT, they could use that data for their training purpose, but you have another day, so then you have a data, uh, data privacy or trust, uh, that your data, uh, your private data is not used for training of those AI models. So Yeah.  
**Speaker 1** 00:09:41:  Yeah, yeah. Exactly. Yeah, that's right. Yeah.  
**Speaker 0** 00:09:44:  Okay, great. Thanks. And, uh, when you say that you're copy pasting stuff from one place to another one, uh, so now you're mostly relying on open AI and, uh, it's, uh, you know, supporting tools. Uh, but how are you using AI for any other, uh, aspect? For example, directly a recording of the data, how do you record it? How do you, what kind of tools you use to talk to people? I understand one way is a survey and, uh, what kind of tool you use for this kind of a video recording. Do you use a video recording to talk to people for qualitative research and what kind of tool you use? And, uh, is there any automatic AI involved there to facilitate your interview process?  
**Speaker 1** 00:10:26:  We have, um, so we use like otta ai. Mm-Hmm. \<affirmative\>, um, and we would use, so I'm, I've conducted interviews, um, like this week and we, we've never had it as, um, I think I find because they, the interviews are, um, they're with, uh, they, they tend to be with people. I mean, the projects that I'm on at the moment, they're with, uh, like African government officials. And so we wouldn't have, I think they can be a bit more, um, wary about having someone that's like on the call recording. Okay. So, um, the way that I would do it is to record the interview and then upload it into, um, open, uh, into auto ai, um, which would then sort of create a transcript that's like AI generated Mm-Hmm. \<affirmative\>. Um, and also it has a summary as well, which is useful. Um, yeah.  
**Speaker 0** 00:11:25:  So, uh, have you used your re or any other tool as well? Uh,  
**Speaker 1** 00:11:29:  \<inaudible\>,  
**Speaker 0** 00:11:31:  Sorry,  
**Speaker 1** 00:11:32:  \<inaudible\>,  
**Speaker 0** 00:11:34:  Sorry, I couldn't understand you.  
**Speaker 1** 00:11:36:  What was, uh, what was, uh, what, repeat the question, the tool that you mentioned.  
**Speaker 0** 00:11:40:  Oh, yeah, I was saying that, uh, uh, have you used only yo or is there any other tool in the market that you, you, you have used or you heard about?  
**Speaker 1** 00:11:50:  I think that's the only one for that purpose. I've heard of other ones when they've sort of joined the call. Um, but there's no names that I can think of off the top of my head. Mm-Hmm.  
**Speaker 0** 00:12:02:  Right. Thanks. And, uh, for, for video calls, what kind of platform? You usually use Teams. Okay. And, uh, right, and, and for a, at the end. Okay. So auto can help you, uh, with the, with summaries or key insight. And, uh, do you sometime, uh, still create some kind of a graphical or representation of that data, like, uh, drive, uh, getting pie charts of, uh, sentiment analysis? Do you run sentiment analysis, uh, uh, on, on this research and, uh, do you try to get some kind of, uh, analysis, uh, in the picture or representation, trying to get some figures or?  
**Speaker 1** 00:12:46:  No, we don't. And like, yeah, I mean, that's something that I had never thought of actually, in terms of like the qualitative conversations. Yeah. We, yeah, we really don't, we don't do that. It's all, um, I mean, if we were to assess that, that would be, uh, like we, we, we write up our findings, um, and we, we tend to include and note on that. Um, so we'll write up a finding saying overall, like people were, um, had a positive tone about this, but that's all done. Um, uh, like if I'm the one that writes the research report and say, I know like 10 people had done, um, had done an interview, then I would send a note to, um, like a a, a chat note to all of those people saying, um, uh, could you just like, comment on what you thought the tone was when, uh, respondents asked, answered this question? So it's all very, like, it's after the fact. Um, and it's very like, yeah.  
**Speaker 0** 00:13:42:  Mm-Hmm, \<affirmative\>. Right. Thanks. And, uh, so yeah, you mentioned about survey and this qualitative research interview. Do you also, uh, for, for same product, let's say for one market research project, do you also use quantitative research, uh, to complete your product? Or it's, are, are you mostly qualitative person?  
**Speaker 1** 00:14:05:  No, we definitely, yeah, we use, um, we definitely use both. Yeah. It's, um, I'd say it's probably like 50 50, um, of, of quantum qual.  
**Speaker 0** 00:14:15:  And, and in terms of AI application, uh, where do you think AI could, uh, is helping you more or could help you more qualitative or quantitative research?  
**Speaker 1** 00:14:27:  That's interesting. I mean, I think the, I think the, the thing that you mentioned about sentiment, I hadn't, I hadn't really thought about. And I think that's really interesting. And would I think the, the thing about qualitative research is that we, um, at the moment it takes quite a long time to analyze things and, um, we're, that's, that's sort of understood. Um, and it would be good to be able to like, have a quicker finding, but, um, something like that sentiment analysis I hadn't thought of. And I think that would be useful. The quantitative side is, uh, I think, I think more, um, it would have more reliable results if, um, if we were to involve ai, I feel like I would probably trust those more. I think you can, um, sort of fact check things a bit better. So it'd have some sort of AI component helping out on the quantitative analysis. I think that would make sense more. I feel like the, from having used, um, uh, we use, or like digital analysis, we use talk order, which is, um, is sort of scrapes, uh, scrapes the web for, you know, mentions of whatever, like our client. And then looking at the sentiment analysis that they do there, um, it can be a bit patchy. So I think the, um, I think more there would be a bit of skepticism around, um, having a sentiment analysis, let's say, on qualitative interview data.  
**Speaker 0** 00:16:13:  Oh, okay. Yeah. That's interesting because yeah, that's, that's very different perspective I'm hearing from you now. Like, because many people have very, uh, different, uh, side on that thing. Uh, there's that as long as the year two can understand the context that they will be a bit more cautious about, uh, or they, some of them can trust, some of them cannot trust. So in your line of work, you will have the trust issues about sent sentiment analysis. And would you say that the context would be the bigger reason?  
**Speaker 1** 00:16:42:  I think, I mean, I think, I think trust element is very over easily overcome. Mm-Hmm. \<affirmative\> if when we're sort of testing this, we can see that it works. But I think that, um, I think people are very easy to not trust. And when it comes to, so when we use talk worker, like we use it, um, company wide, and across the company people will say like, oh, you've included, um, the sentiment analysis, but should you really have, like, it's not like you should probably have done your own, your own assessment rather than including the, um, uh, what it's the automatically generated one. Yeah.  
**Speaker 0** 00:17:25:  And, uh, so if you're doing everything by yourself, again, I mean, you are trying to use AI wherever possible, but then again, there, there's certain aspect where there would be trust issue and you would like to do it manually, uh, but then you're going to spend a lot of time there. Uh, but if I, if you have to look at the bigger picture, for example, you have an analysis, you performed your study, and now you're presenting it to different stakeholder. Uh, some of them may not like sentiment analysis. Some of them are interested only in the, you know, summary of the meeting or key actions, uh, just to the point. Uh, but, uh, how often do you think people want to see more than key insight and action items and, uh, items? What is the definition of that more, uh, when they ask you to do something more, what kind of further insight they want to get out of that, uh, survey or, uh, or interview?  
**Speaker 1** 00:18:20:  Yeah, so, so do you mean in terms of, um, so when we do like a, uh, so when we're conducting a research project, um, do you mean in time? What, what more would they want out, um, outta sort of like the final product? Or do you mean as, as sort of like the quick finding?  
**Speaker 0** 00:18:40:  Uh, uh, I think as the result of, uh, final analysis. Okay. Uh, depending on different stakeholder, I think they might want to, uh, get different things. For example, if you're talking to another research manager, you would be happy to talk about, uh, technical details. But if you're talking to your boss or somebody else in the management letter, they might not be interested in technical details. Right. They, they would be probably interested only in one PowerPoint slide that gives you the summary of your whole product that you have worked on for weeks. So my question was that, uh, uh, what kind of different information, uh, you usually drive at the end of the project?  
**Speaker 1** 00:19:22:  Okay. I mean, I think it would be like key themes. That's like a, that's like a massive part of it is, um, there'll be, I mean, it's, it completely depends on the project, but broadly, we're always looking at, um, we'll always be looking at kind of e themes that are important to the client. Um, and then within those themes, what are sort of like the key, um, like what are presented as maybe the key, uh, areas of opportunity or white space. Um, and that will be either something, yeah, that will be something that is, uh, sort of like high in volume in terms of, um, people are talking about it quite a lot. Um, so there's like a lot of interest, but not, there's not, it's not sort of overly saturated. So it's kind of like that sweet spot. Mm-Hmm. \<affirmative\>, um, that's something that we look at and that's where we would feed into, say if we're presenting this to them, to the client, then we would, um, we would be looking on, uh, yeah, we'd be looking at what these, um, what these overarching themes are. Um, and then what within that topic is, uh, is getting a lot of interest.  
**Speaker 0** 00:21:55:  Oh, hi, welcome back.  
**Speaker 1** 00:21:59:  My connection's a bit shit in the office. \<laugh\>,  
**Speaker 0** 00:22:02:  \<laugh\>. No, no worries. Can you hear me well now?  
**Speaker 1** 00:22:06:  Yeah, yeah, yeah, that's, um, yeah, I dunno where it caught me off, but,  
**Speaker 0** 00:22:11:  Uh, just I think, uh, around 20, 30 seconds back when you just started to explain.  
**Speaker 1** 00:22:17:  Okay, great. Um, uh, what was I explaining? Oh, yeah, so, um, we would tend to be looking, when we look sort of in detail, the part that we wouldn't be necessarily walking a client thorough, like the, the, uh, executives through is, uh, are the steps beforehand where we'd be looking at what the overarching themes are. Um, and, uh, and then kind of ranking those. And then the ones that have, um, sort of like the highest volume in terms of, um, as an issue that's being talked about, um, we would then look within that to find key topics. Um, and then we'd be finding topics amongst those that are, um, either kind of like, yeah, basically not overly saturated, um, and would use those as sort of recommendations for thought leadership for the client. Um, but that would require, um, that would require us looking at, um, like us considering what the topics are that a client should be talking about. Um, because say if it's like in the run up to an election, then there's gonna be more policy related topics. And so although that, um, uh, that's the topic of the moment, so the client, we would recommend that they should be referencing some sort of aspect related to that, um, because then they will be, uh, they will be like more visible in the news and the conversation of the time. Um, so that's sort of like the recommendation that we would lead to give to the client, but we wouldn't, um, sort of say about the backend. Did that sort of answer the question or is that  
**Speaker 0** 00:24:09:  Yeah, yeah. Yeah. Very, very well, thank you. And, uh, can, can, can you walk me through, uh, an example of your typical qualitative research project? For example, starting from your product planning, then you are recruiting participant or finding a way to record data, and then your data recording and then analyzing, and then at the end you are doing the interpretation to talk to different stakeholders. So can, can you gimme an example and also tell that, uh, what would be the average time you spent on each of these steps and, uh, also the names of the tool that you are using for different steps?  
**Speaker 1** 00:24:49:  Um, yeah, so from the get go, we would, um, I mean, when it comes to, when it comes to, so interviews, we would, uh, we'd start out by building a list and this is all done, um, either manually, um, we'd be looking on, we'd, yeah, we'd be like manually searching, trying to build together a list of, um, of potential stakeholders to reach out to. Um, or we'd be using talk to, uh, try and to like create a Boolean to find, find people. Um, but definitely not using AI there. Um, and then once we have our stakeholder list, uh, then again, it would all be sort of like manual outreach, um, or we would employ another partner, so another, um, agency to be going up for us. Um, but I think the hard part about, um, compiling the stakeholder list is, um, is finding all the details.  
**Speaker 1** 00:25:54, when it comes to, when it comes to, so interviews, we would, uh, we'd start out by building a list and this is all done, um, either manually, um, we'd be looking on, we'd, yeah, we'd be like manually searching, trying to build together a list of, um, of potential stakeholders to reach out to. Um, or we'd be using talk to, uh, try and to like create a Boolean to find, find people. Um, but definitely not using AI there. Um, and then once we have our stakeholder list, uh, then again, it would all be sort of like manual outreach, um, or we would employ another partner, so another, um, agency to be going up for us. Um, but I think the hard part about, um, compiling the stakeholder list is, um, is finding all the details.

**Speaker 0** 00:27:24:  Hello? Hi,  
**Speaker 1** 00:27:26:  Sorry. Um, yeah, my internet is terrible here.  
**Speaker 0** 00:27:29:  Yeah, it, it was off for about 10, 15 seconds. Uh, but yeah, now I can hear you well. So you were explaining that, uh, yeah, uh, it was, can you hear me?  
**Speaker 1** 00:27:42:  Yeah, I can, I just, um, realized that I have a, um, another meeting, but that's okay. I can push it back.  
**Speaker 0** 00:27:48:  Sorry. Yeah, I will take only like a couple of more minutes then.  
**Speaker 1** 00:27:52:  Um, yeah, so, um,  
**Speaker 0** 00:27:57:  Yeah, uh, sorry, please go ahead.  
**Speaker 1** 00:28:00:  The, um, so from this stage of, yeah, all manual up until we get to creating the discussion guide, um, creating the discussion guide we use, uh, at GBT to kind of more for like idea generation, um, thinking of sort of like alternative ways to ask a question. Um, and, but I feel like there's potentially something, there's some sort of way to make that process a lot easier. Um, I haven't seen any tools that are out there that would help with discussion guides. I've seen them for surveys, um, but I haven't got around to testing any. Um, but the discussion guide point, it's there, like our discussion guides always quite similar, so we tend to, the best way to do it tends to be to, to think of a project that we've done that's similar and then go through a similar discussion guide and like pull maybe three or four together and then sort of look through, um, questions, um, to try, yeah, to sort of ideate.  
**Speaker 1** 00:29:05:  And then we'll tend to formulate that on our own or use the help of like tattoo bt to, you know, improve the language or make this less direct or something. Um, and then in terms of the interview, um, we book this in or manually, um, or our partner would do it. And then, yeah, the transcription is then the, the next time. But we'd, um, involve ai. Um, and I mean, I personally don't tend to look at the summaries. Um, I find that there, I I find that they're quite long-winded, and I would prefer to, um, I would prefer to sort of look through things myself, um, because they, yeah, the summaries on Otter AI are, I mean, they're, they're big. They're sort of like, it's like a, it's sort of a, it's probably like a, a couple of eight sides of four or something. Um, so I would rather speak to the person that's conducted the interview and, um, and get them to tell me their key findings or there's anything interesting.  
**Speaker 1** 00:30:15:  Um, and then they can direct me or they'll, they'll be writing up notes themselves, um, during the interview, and then we'd put these basically into a matrix. Um, and yeah, so this would all be in a matrix. You'd have like interview long hair and questions going down and columns. And then when it comes to the reporting, I'll look downwards and look as like, um, the key along interview questions for, um, full of common themes. And then I'd probably enter that into a, um, like an AI system to try and, uh, generate key themes. Um, but then I'd also look myself to double check if there's anything that's missed.  
**Speaker 0** 00:31:07:  Hello?  
**Speaker 1** 00:31:10:  Hello.  
**Speaker 0** 00:31:11:  Yeah. Can, can you hear me well now? Can you hear me?  
**Speaker 1** 00:31:19:  But, um, yeah, yeah, that's perfect. One moment.  
**Speaker 0** 00:31:31:  Sorry, I cannot, uh, I can  
**Speaker 1** 00:31:32:  Hear you.  
**Speaker 0** 00:31:35:  Okay. I will also switch off my video then for a minute. Uh, how, how about now? Can, can you say something? Okay.  
**Speaker 1** 00:31:43:  Can you Yeah, go.  
**Speaker 0** 00:31:48:  Uh, yeah, sorry, I, I don't really understand. You still, do you want to like, uh, uh, leave the meeting and come back again? Okay. Hello? Hello. Hi. Uh, yeah, now it's much better.  
**Speaker 1** 00:34:19:  Okay. That's good. Yeah, I think these,  
**Speaker 0** 00:34:22:  Yeah, thank  
**Speaker 1** 00:34:22:  You. The wifi is terrible.  
**Speaker 0** 00:34:25:  No worries. Or do, do you have to run now or do you you have like four or five minutes? I just have like,  
**Speaker 1** 00:34:29:  Yeah, yeah, that's, yeah. Yeah, that's fine.  
**Speaker 0** 00:34:33:  Okay, great. Yeah. Uh, thank you very much. Uh, so you were mentioning something about the problems with ot, so I just want to clarify that. Uh, did you mean that, uh, uh, the information, like these summary or insight that auditor is giving you are not enough and you still feel like there is no way to get it more, and you will prefer to talk to the person who interviewed actually instead of taking it as it is?  
**Speaker 1** 00:34:58:  Yeah, I think it's the fact that, um, when we do, uh, when we do these interviews, we want to, we are running through, like the way that we analyze it is on theme. So we have, say, three themes per interview or more whatever. So you have three. And then within that, within those themes, we'll have, uh, we'll have like three to five questions. Um, so I, reading summary doesn't necessarily help because it's looking at too much of an overview. How I need is to look at those, I need to look at themes individually. Um, and then talking to the person is really useful because they can maybe, maybe an interview hasn't followed the appropriate, um, flow of the discussion guide. Maybe they've, they've had to sort of like go with the flow a bit more, and they've had to, they've had to deviate from that.  
**Speaker 1** 00:36:00:  Um, so then they're able to, um, to pick up on things, um, which yeah, which I, maybe it's something that the interview emphasized at the start and then maybe they emphasized at the end. Um, but it's, yeah, it's, it's like part of a different section. Um, and there was one other thing that was something \<inaudible\>. Um, oh yeah. So the other, the other good thing is, um, when I would use it is because you can, um, you can search for a word. Um, so when I go to, uh, analyzing across interviews, we'll upload all of them into one folder and then type something like, like a client's name, and then you can find quotes really easily, because that's like another part of reporting is, uh, is, is picking out sort of these key points that, um, that we can then, um, send over to a client that maybe portrays the point that we're trying to, um, we're trying to portray in like a better way.  
**Speaker 0** 00:37:04:  Mm-Hmm, \<affirmative\>. Right. Yeah. Thank you. And the, the last line that you mentioned, uh, uh, is it the future that you have in order,  
**Speaker 1** 00:37:15:  Is it a feature?  
**Speaker 0** 00:37:17:  No. Like, like you said that, uh, there are like a lot of interviews in the, in the same folder, and you can drive, you can get like key insight, uh, from, but just the type name of the, and you can get like different insight from all of these. The answer could have been better. Did you explain it that this is something in OT or this is your wishlist?  
**Speaker 1** 00:37:40:  No, that, yeah, so that's something in Otter. Um, I mean, the way that they do it is that you type in, say you type in the client's name. Yeah. Um, then it will literally just give you the place in the interview that the client was mentioned. Yeah. Um, and then you can go through, say if we do like 40 interviews Mm-Hmm, \<affirmative\>, um, then straight off the bat, you can find straight off the bat. That means that you can, you can basically count to see how many times that, um, that they say the client mentioned. Um, say, one of our questions would be something like, um, word, uh, our questions would be like unprompted, uh, does the clients, does the client come to mind? And so we would, we would search the name of the client, and what I, what otta does do is it, um, brings every time that it was mentioned, or what it would be better if it could do was, um, if you could somehow plug in where our question is, because that is like, and then to be able to find, easily look at like the quantitative data of, we've done 40 interviews, 10 of them knew question one, they mentioned, uh, Pfizer straight away.  
**Speaker 1** 00:39:06:  So that's like whatever per whatever percentage that is. Um, that's something that Okta doesn't do, but it would be really useful.  
**Speaker 0** 00:39:15:  Mm-Hmm. \<affirmative\>. Right. Thank you very much. Uh, I, I just like, good. Now, so what is the average cost of your one, uh, qualitative project usually?  
**Speaker 1** 00:39:25:  Uh, uh, wait, sorry. The, out of, um, the qualitative project?  
**Speaker 0** 00:39:30:  Yeah. What is the average cost in terms of time and, and money? Uh, is it something that you can tell?  
**Speaker 1** 00:39:36:  Um, I mean, I would probably, hmm. The average is quite, like, they range a lot, I would say. So we tend to charge, uh, we, we charge probably 25 to 25 would be, um, like a very small sample. Um, uh, and our benefit that we're always driving is, um, like as an advisory. So we like our, that whole element of, of advice and key recommendations is like a strong part of that. So, uh, in terms of kind of looking at a small sample of pizza, um, that would be like a low amount of our time doing actual interviewing. The main part is that we'd be charging because of our time of like developing the advice and doing the analysis. Um, so we between like 20 to would probably cost, um, around like 70 to upward of that.  
**Speaker 0** 00:40:43:  Okay. Yeah. Very good. Interesting. Thanks a lot for that. Well, this last question about your wishlist now, uh, uh, and if you have to tell me that, uh, if there is no problem from technology point of view or funding point of view, what would be your wishlist as a researcher from, uh, from AI that, uh, what are the different things that it can solve for you? How can it, uh, not research, but also management, like you have mentioned from one place to another place, and then you're sharing, you also, uh, spend time on moving from one project to another for search and project management. How can ai, uh, what are your wishlist from ai? You can say anything. Um,  
**Speaker 1** 00:41:39:  Yeah, I feel like that's really hard because I think there's, there's a lot. Um, I feel like, I mean, in terms of, yeah, I feel like there's a lot, there's, um, there's just so much kind of, uh, like copying and pasting and, um, what did I say as my wishlist? I feel like, I feel like a lot, like a lot of our, um, our problems come from, uh, as being part of like a research team as part of, and like wider, uh, a wider like communication company. So we do a lot of like crisis communications, um, and like a lot of like issues and, uh, yeah, sort of issue management. And that all requires a lot of like fast, fast-paced reporting. Basically, the whole company, apart from our team are a lot more used to, um, and they, and they want, um, this sort of like fast-paced, um, report delivery that I feel like sometimes holds our team back, um, in terms of people who don't want to work with us because they think that we're always like in the weeds and we're always taking a bit too much time with either the data analysis or the setting up of a project, and they want us to sort of like, immediately when an issue advisors, they would like us to do like a cook and dirty interview thing or, or something.  
**Speaker 1** 00:43:10:  Um, and I think that's what, like, a lot of the, we're held back a lot by, um, uh, by our own processes. Um, so I feel like there's, like, there's a lot of ways that AI could help us out, which is why, um, I'm relying on it a lot for kind of, uh, for doing a lot of the, um, either like report creation in terms of just getting like, text on a page or the same with like, new business because I need my time to be focusing on the thoughtfulness with the strategy, um, rather than on more like, mundane things. Um, so yeah, I feel like anything that can kind of help with help withing like a more streamlined, um, analysis or with like project set up, uh, in terms of like creating discussion guides, creating, um, creating questionnaires, at least, just like basically just getting something on the page that then we can react to. I think that's like the main, the main thing is, yeah. And also in terms of like, yeah, that's,  
**Speaker 0** 00:44:48:  Yeah. Thank you very much. That was very elaborate.

# Kieran Griffiths 

Client Insight Manager at Irwin Mitchell

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:01:  \<inaudible\> products for market research. So we are talking to experts, people like you, trying to understand that, uh, what could be different kind of pain points that we can address with the help of ai. So I will, uh, ask questions, uh, uh, along the similar line, so maybe we can start with, you can explain, uh, your job and, uh, your day-to-day responsibilities. And then I can start asking more questions.  
**Speaker 1** 00:00:26:  Yeah, that's fine. And just to mention as well, so it was 50 Pan Amazon actually taken part, is that right?  
**Speaker 0** 00:00:32:  Sorry,  
**Speaker 1** 00:00:32:  50 Pan Amazon actually taken part in this, is that correct?  
**Speaker 0** 00:00:35:  Yeah, yeah, that's definitely true. Uh, however, uh, it'll take some time because Dave is on holiday now and, uh, he has to  
**Speaker 1** 00:00:43:  Check, I remembered it correctly. That's, that's fine now.  
**Speaker 0** 00:00:46:  Okay, very good. Yes, that,  
**Speaker 1** 00:00:49:  Yeah. Great. So you, are you asking about a bit of background to my role? Was it Sorry.  
**Speaker 0** 00:00:53:  Yeah. Yes, please. Yeah, just to explain a little bit about your job and day-to-day responsibilities and yeah. What do you do?  
**Speaker 1** 00:01:00:  Yeah, so I work at a company called Erin Mitchell, who are essentially a full service legal law firm in the uk. So they, they offer legal services across all the different areas, um, at the legal spectrum, specializing in, um, sort of personal injury medical negligence, um, also covering lots of sort of private client matters like divorces, wills, and also have like a, a, a business offering as well. So employment matters, real estate and so on. But I, I could spend probably the whole interview listening list, listing all the services that they offer. Um, so I work for Area Mitchell. I've been working there for almost a year and a half now within the client Insight team. So I guess my role typically is, um, to carry out projects, typically ad hoc projects, to look to understand whether sort of new service products and services that we're looking to offer, whether they may meet the needs of our clients, um, but also, um, how our, what our client experience is like. So work with the difference, um, Fiona's lawyers across the different areas of our business to report back to them and say, here's how the experience looks in this area. Here's how experience looks in that area, area through our NPS program. And also carry ad hoc projects as well to look to, to understand that better. Um, so yeah, lots of ad hoc projects, but a lot of NPS as well, um, also work on brand tracking a little bit. Um, so I think that's, that's mainly my role. Yeah.  
**Speaker 0** 00:02:17:  Yeah. Very good. Thank you, Ken. And, uh, is it, uh, mostly, uh, could you please explain a little bit about the nature of your work or what kind of, uh, research work? Are you managing it, say qualitative, quantitative, or bit of both?  
**Speaker 1** 00:02:32:  It's, it's a mixture of both. So I have ownership for the NPS program, so that's obviously, um, quantitative. We do have like a, a verbatim question within it. Um, then we also cut video interviews a lot of the time. So we have a panel of, uh, our former clients who have, um, gone on store them this database where we look to on occasion speak to 'em for video interviews. We also use video interviews as part of ad hoc projects, um, occasionally. So yeah, a mixture of qu and qualitative techniques, I'd say.  
**Speaker 0** 00:03:00:  Mm-Hmm, \<affirmative\>. Oh yeah. Very good. And, uh, how, how big is your team that you're managing now? Uh, are you mostly working on your own or how well you collaborate with the other sections of your company?  
**Speaker 1** 00:03:14:  So the insight team sits within the wider client experience team. So we've got like a, a few journey managers, design managers. We also have people work in propositions. We have people from our knowledge team, and so essentially looking at secondary source information. But the part I work within is the insight side of things. So essentially primary data, looking to speak to clients or people in the market to understand their perceptions of our Mitchell and that team. We have about seven within it. And I, um, kind of have responsibility for client insight side of things. Anything that's relating to understanding the perceptions of our clients or experiences of our clients, that kind of falls into the area that I manage.  
**Speaker 0** 00:03:52:  Right, thanks. And, uh, when you're talking about data, like of course I think quantitative data is probably that's already provided to you by the different section of the company. And, uh, how do you collect the qualitative data? Uh, is it also your responsibility to talk to people or design the data collection methods or somebody else's, uh, handing it over to you?  
**Speaker 1** 00:04:16:  Um, so we have a bit of both. So work with our partner agency, um, on some of the video interviews. So they'll go out, carry the interviews and report back to us with, with show wheels and insights. But also a lot of the time we carry the video interviews myself. But within the team, I do, I do a lot of that work personally myself as I've got their experience or qualitative research, but others in the team can also sort of work on it as well. So yeah, it's, it's predominantly we do sometimes work with our partner agencies on it as well.  
**Speaker 0** 00:04:40:  Mm-Hmm, \<affirmative\>. Yeah. Very good. Thanks. And, uh, at your role, at, at your current role or on the previous role, have you used any AI research tools that, uh, could have helped you with your productivity?  
**Speaker 1** 00:04:56:  Um, so I've used ai, I say sparingly so far, so it's, it's never been used in isolation. It's kind of been used to complement, um, all over analysis or production of survey, whatever it may be. So to must give give ideas to try and sort of reduce some of the, the manual labor work. So throughout the project, we'll always like consider using ai. So when a brief comes through, we'll, we'll, first of all, um, sometimes use AI to ask how it, that, how it would approach the project. We may also get inspiration in terms of the survey questions that we ask for a discussion guide. Um, and then, and when we get the, the data back. So again, with it being sort of either typically qualitative, we'll we'll feed that into AI to ask it to summarize the key themes, I, if I've heard in your interview, I like to get a quick summary, your stakeholder, after the meeting, I'd possibly copy and paste it into ai, ask it to summarize the quick themes.  
**Speaker 1** 00:05:42:  Then I'd, I'd have to have a glance over it to make sure it was actually reflecting the conversation that that took place and add anything that I felt was missing, which does seem so it does seem to mess things on occasion. So yeah, we, we do consider AI at all different stages of the research process, I would say, but it's never used in isolation. I don't think we have enough trust of it yet to be able to say, create a survey for me or analyze these results and report back to me and not check it ourselves. We feel the need to have like that human interaction with it at the moment.  
**Speaker 0** 00:06:08:  No, yeah, I, I totally agree. Yeah, I mean we, this is still an evolving field. I think we are like far away from having that kind of trust. Also, depending on the choice of your field, uh, it may be more advanced or less advanced. For example, if you go to healthcare, things could have been both bit more different and, and, uh, uh, can, can you name some of the tool? When you say that you have used ai, uh, what are the tools that you're using?  
**Speaker 1** 00:06:37:  It's been chat that we tend to use, um, but we've also used a version of chat, GPT, which is our own internal tool. So we've used that. It is just those two really. It's nothing too complex in terms of AI tools yet. We have, we have, um, seen other tools be mentioned by our partner agency there, so that they've shown us, um, how one AI tool would work. But we're still very initial conversations around that, so not actually used it ourselves, but yeah, it tends to just be, um, being chat, chat, chat to GPT. So essentially AI solutions which are available to the public.  
**Speaker 0** 00:07:07:  Mm-Hmm. \<affirmative\>. Correct. Thanks. Uh, so, uh, yeah, you mentioned that either you are asking AI some help about creating the discussion guide or basically helping you with the, the project planning or, uh, understanding the transcript. Uh, is that correct? Uh, yes,  
**Speaker 1** 00:07:27:  That's, that's right.  
**Speaker 0** 00:07:29:  And if I have to like zoom out a little bit and look at the overall project journey, like from project planning to then you are, uh, figuring out that how, what kind of data you want to use, either it's quant or qual and how you want to recruit your participant, and then you record data, then you analyze and then you go towards the end, uh, to drive some meaningful insights from that at the end. And then you're sharing it with the different members of the team or maybe the management board. So where do you see, uh, the AI tools could be more helpful in this whole journey? I mean, you have mentioned, but Yeah, other than that, if you have to tell me over the, uh, about the whole project planning,  
**Speaker 1** 00:08:14:  Um, I'm not too sure. I don't do lots of product planning, um, really, so I'm not, not really too involved in that. It's not too sure if AI could potentially assist. That's kinda outside my role. Product planning. I'm more just looking to understand people's experiences and whether there's, there's a gap in the market for, so potentially rather than get involved in, in the plans. But now, as I say, in terms of how AI could help us more, I don't think there's something necessarily that we're crying out for like a new tool. It's more just getting better confidence in the ones that currently exist. So when, when I do like feed a, a, a transcript from a video interview into it, that there are occasion things that I misses or it's, it's an accurate it's summary of things. So I think getting better at that, um, would be good.  
**Speaker 1** 00:08:51:  And similarly at the initial stage of like, create a survey, um, being able to like sort of understand our business a bit better, um, and create a survey in, in light of that understanding would, would also be useful. So I'm, I'm not too sure. There's lots in terms of additional things, it's more getting great, greater confidence from what's already available, potentially one additional thing. So, and the partner in work with ViiV, who based in Manchester and London, they've been talking recently about being able to essentially, um, have AI participants in research. There's no longer the need to actually recruit participants, take a part in research because you can actually feed in, um, lots of information about, say clients in your different segments, give it lots of information about them as as people. And then it will create essentially a sample, um, of, um, AI participants within that platform. So when you do have, um, new propositions to test or you want sort understand how they feel about marketing communication or whatever it may be, you can feed it into that platform and get responses back from them, um, as to how, um, someone in that particular segment will be likely to respond. And so something like that might, might be of interest. You know, aside from that, I can't think of anything I'm sort of crying out for as such.  
**Speaker 0** 00:09:58:  Mm-Hmm, \<affirmative\>. Right. Thank you. Yeah. Um, yeah, that was very interesting point, but I'm just a little concerned about you. You, you also talk about trust in ai and then, uh, sorry, did, if I understood correctly, did you also mention it as a AI research participant? Like you will interview the AI tool with some questions to get back data?  
**Speaker 1** 00:10:20:  That's something that they've are currently working on. So it's a solution that they offer and they've made us aware we haven't used it. They've just made us aware that exists, essentially.  
**Speaker 0** 00:10:30:  Okay. And, uh, what would be your concern there? Uh, because it's accuracy. So how would you trust on the information that you will get back from an AI tool that's more like a synthetic data? It's not like the real data, right? So yeah, what would be your main concern when you're trying to adopt it and test it and then eventually you're gonna believe whatever it's saying?  
**Speaker 1** 00:10:57:  Yeah, I think if you kind of summarize the concern in the question, there it is, the accuracy of it. That would be the concern that does, that does what's coming outta this AI tool accurate, accurately reflect someone who would be in one of our segments. So yeah, it all comes down to accuracy. They've talked about an 80% level of accuracy with it, and I'm, I'm a bit skeptical as to whether it's always 80% or just like that's like the best case scenario. But no, that's definitely, accuracy is a problem. I guess the, the po the positive side of it is that, you know, having to try and recruit people in hard to reach segments, which can be difficult to do and quite time consuming and quite costly on occasion with incentives. So you save a lot with the money. But yeah, definitely accuracy is something that we'd be skeptical of and want to have a, um, lot of great reassurances of before we implement it.  
**Speaker 0** 00:11:44:  Right. Thanks. And do you have some ideas that, uh, how would you like to, what would be your threshold? I understand you're not happy with 80%, uh, of course we can play with the number. We can even say that it should be like a 95% accurate. But then what would be your, uh, main hurdles that you wanted to cross? Uh, what would be main concern by which you will say that okay, now it's hitting the 95%.  
**Speaker 1** 00:12:11:  Um, yes, it's hard. It's, I think 80% is okay. I mean, in research we tend to wear like 95% confidence level of like quantitative analysis. So we, we e even when we do it ourselves, there's like that, that 5% chance that what we're saying is not reflective of who we're talking about. So we do have to accept the fact that we're not always gonna get a hundred percent true. I don't think 80% is necessarily too problematic. I just need reassurances that it's always at least 80%. It's not the case that sometimes it's 10% of the time it's a hundred percent I'd, I'd need to know. It's consistently sort of above 80%. But in terms of exact figure, I don't think 80% is too far. Obviously the higher, the better, the closer to like 95 or hundred would the better. But I think 80% is, is reasonable. It's just, as I say, how often is it 80% and do I actually believe 80%? It's quite hard to test whether it is actually, um, accurate of the, the sample that you are, um, looking to reflect.  
**Speaker 0** 00:13:00:  Right. Thanks. Uh, yeah, thank you very much for explaining that. I actually never heard about the, that if, uh, uh, that kind of tool is already existing in the market. Do you mind telling me the name of, uh, the tool that's providing you the syn that could provide synthetic data with 80% accuracy?  
**Speaker 1** 00:13:16:  Yeah, so I say we're not currently using it, so we work with a partner agency called Viv, and they're based in London and Manchester. So it's a UK research agency. The tool I believe is called Intelligent Personas. And yet something that they're, they're talking quite a bit about at the moment. Um, but we haven't actually used it. We've just seen demos of it. So how it can potentially work. We never actually used it ourself, but for our own clients.  
**Speaker 0** 00:13:36:  Okay. Great. Thanks. So Dean of the company's word, like, uh, can you spell it?  
**Speaker 1** 00:13:41:  Yes, it's V-E-R-V-E.  
**Speaker 0** 00:13:44:  Oh, okay. Okay, right. Thank you. And, uh, so out of this qualitative and quantitative research, uh, where do you ex uh, where do you see the AI tools could be more helpful and how, for both of these sectors,  
**Speaker 1** 00:14:02:  Um, where do I think they're more helpful?  
**Speaker 0** 00:14:06:  Yeah, for both quantitative and qualitative. So where do you think it could be more helpful?  
**Speaker 1** 00:14:13:  I think it'll be more helpful and qualitative. 'cause it's much, it's much more effort to analyze, say, 200 pages of text from a video interview. Whereas AI could literally do it in, in about five seconds. So it'd be more helpful in that setting. But it's probably, I'd, I'd have, I'd have greater confidence in it with quantitative because when it's actually, you've got the numbers you are feeding into it, that those actually, that actually is like the truth as we see it. So it's, it's interpreting quantitative data. I'd have less, um, issues with than, than qualitative.  
**Speaker 0** 00:14:47:  Right, thanks. And, uh, in your current job, when you are talking about qualitative analysis and, uh, people who are actually using, uh, AI or basically trying to get data from your agencies or those who are also recording, uh, interviews by themselves and at the end they're analyzing what is the level of, what are the expertise level of those people? Or are you as a manager still involved in, uh, that hands-on experience or it's mostly like a junior, uh, experts in your team who are conducting those interviews and analyzing what, can you tell me a bit more about the expertise of those people?  
**Speaker 1** 00:15:24:  Yeah, so, um, as I mentioned, a lot of the, we have carried out from our partner agency, so it'll tend to be, um, a lady at Verve who is, is very experienced. So she is like direct level think possibly the next one above direct level. Like she manages like a whole department at Verve. So she's got a lot of experience. Um, internally, um, I tend to carry the bulk of the interviews. I've got 13 years experience in market research and a lot of that has been qualitative but not exclusively, um, qualitative. Um, so yeah, I'd say that I, I have good level experience with it as well. But we do also have some more junior members of the team who will on occasion carry out, um, the video interviews. So we've got someone who's maybe been doing it for about probably about a year. She's been carrying out video interviews, um, and a few others who are sort of at, at a similar junior level. Um, but yeah, tip, the majority of it's carried out by myself, who I feel good, good experience with it. And also, um, the, the, our partner agency of the, you have good experience as well.  
**Speaker 0** 00:16:14:  Mm-Hmm, \<affirmative\>, thank you very much. And, uh, yeah, thanks for mentioning that you have like a very, uh, very rich experience in the field. So over the last, uh, you said 13 or 13 year experience.  
**Speaker 1** 00:16:27:  13\. 13\.  
**Speaker 0** 00:16:28:  Well, uh, how would you like to comment on, uh, the, uh, you know, uh, evolution of technology, especially in this 13 years when you first entered the market research and now, uh, what are the main things that, uh, you have seen disrupting the market research industry in, in a very positive way?  
**Speaker 1** 00:16:50:  I don't think I've seen too much change in market research since I started, except for ai. AI seems to be like the, the big thing, which has like really shook up the industry and really challenged how people approach research, analyze research. Um, so I can't think of beyond ai. I can't think of anything that's really sort of shook the industry as such. Um, tools have got better. So things like Qualtrics is, is has got better, like platforms, managing data have got better visualizations, tools have got better. Um, but nothing's ever come in as groundbreaking as ai. So it's hard to think of a, a, a, a second really after ai, I'd say yes, it's been the past year, year and a half, I've seen the explosion in generative ai. It's been the, the main thing which has changed over my career. Otherwise, it's just been essentially things that already existed, getting a bit better, a bit faster, a bit more visual.  
**Speaker 0** 00:17:35:  Mm-Hmm, \<affirmative\>. Yeah. Right. Thanks. Uh, thanks for mentioning Qualtrics. Uh, have you been, uh, is your company mostly using Qualtrics or this is something that you just heard of?  
**Speaker 1** 00:17:45:  It's used it in a prior company, um, when I worked for British Car Options. So I used it day to day. Um, in my current role, we don't use it currently, but we're looking to integrate it,  
**Speaker 0** 00:17:55:  Uh, integrate. It means still Qualtrics or any other tool in the market you would be open to explore.  
**Speaker 1** 00:18:02:  We're currently looking to work with Qualtrics, so it's, it's been an implemented in other areas of the business. So we're looking to see if we can sort of work, um, include it in the, the client insight team as well.  
**Speaker 0** 00:18:10:  Mm-Hmm, \<affirmative\>. Mm-Hmm. \<affirmative\>. So for these kind of decisions when you have like, uh, find something new in the market and, uh, then is, is it here, who will, uh, make that decision that, uh, I, I found a new tool in the market and I want to integrate? Or you have to go through like a whole, a lot of discussion in your, with your management board before you, uh, start integrating external tools in your software?  
**Speaker 1** 00:18:33:  Um, budget's quite limited to us at the moment, so when we do look to get new tools, it will tend to need to get sign off from someone at director level, which is not myself sadly. Um, so no, I'm, I'm more of like an, an influencer I guess, with these sorts of tools. So I'll come to the business and say, there's this, this tool and I think's really useful, and I'll make a case for it, and then it'll get signed off by someone with a higher pay grade than myself \<laugh\>.  
**Speaker 0** 00:18:55:  Okay. Right. Thanks. And, uh, from, from your, uh, knowledge of AI tools, uh, what do you think are the biggest challenges? Thanks for mentioning the trust as, uh, one of the possible factor, uh, but if you have to like look at the bigger picture, uh, what do you think, what are the big challenges that, uh, these tools should solve before, uh, those are becoming very useful for you?  
**Speaker 1** 00:19:21:  So what are the challenges that AI should solve before it become useful for like, insight teams?  
**Speaker 0** 00:19:25:  Yeah. Yes, yes.  
**Speaker 1** 00:19:27:  Um, so yeah, as mentioned, accuracy is, is the thing that sort of holds me back a little bit. Um, but essentially I, I see AI as like being able to reduce the amount of time that research is spend on the more basic tasks. So it's just like giving you sort of creative ideas, which might take you ages to think up. It's been able to analyze lots of data, which will take you like a data read through and be able to summarize it really quickly. So it's saving a, a lot of time, hopefully with more basic tasks which anyone without a background in research could potentially do. And allowing research to focus on the more sort of added value areas, um, like sort of, um, how we then tell that back as a story. Um, how we make sure the insights land, how we visualize it. So I think it's time saving. Um, it's hopefully, um, coming with accuracy and frankly what else it would be in terms of what we're looking at AI to solve. Um,  
**Speaker 1** 00:20:15:  I think also there's, there's, there's so much data that's available to companies now, uh, that's, that's definitely obviously, um, seen a massive increase in recent years with like big data and all the data we hold internally, but we don't always necessarily make use of it. So it's, it's rare that we'll kind of look at our internal data as part of projects, and that's due to like time constraints and also getting people up to a knowledge level where they're able to understand it and sort of integrate it as part of the project. So I guess hopefully AI would be able to take all the source of information that we currently have access to, but not the time to use and, and make sense of them. So multiple tools, being able to reduce the amount of sort of basic work by, by, by doing that itself and also hopefully deliver accuracy while meeting those first two objectives. I think those are the main things that we, we'd really look forward with ai.  
**Speaker 0** 00:20:58:  Yeah. Right. Thanks. So just to, just to understand, uh, so tools like Qualtrics or any of the tool available in the market, they can help you, uh, one interview wise, like you are trying to save your time and you're analyzing an interview or you're analyzing some information related to that particular project. But you're saying that you have been, uh, collecting a lot of similar data over the years and there is nothing available in the market that can look at the bigger picture and still try to spit out some useful information. Is that it might like data science,  
**Speaker 1** 00:21:32:  It might, it might not. Something exists, but I'm, I'm not aware of it. So yeah, essentially it's an AI torch is able to like and take all our data sources, so our internal data, um, profit, everything. So like, um, all our primary research, what we hear from our clients, taking all information and be able to like, create an overall like holistic picture would be something that's useful. So like in my website as well, the click rate, the, and the conversion rate brand tracking. So I could take all that information and make sense of it. I think that that would be, um, a good use of ai. Yeah,  
**Speaker 0** 00:22:02:  No, I totally agree. And, uh, to drive more meaningful insights from your existing data or from the recent product, from qualitative or quantitative side, uh, have you explored the use of data science there, uh, in, in any way?  
**Speaker 1** 00:22:22:  I, I, I've never used Qualtrics for, for AI in, in the past, if that, that's what you're asking. Was it a previous role before AI exists? I worked with Qualtrics, um, mm-Hmm. But no, there's, there's nothing, um, of that nature which we looked, we've looked at currently.  
**Speaker 0** 00:22:36:  Right. Thanks. And what are your thoughts on, uh, some ethical considerations or sentiment analysis related to, uh, insights that you can get through AI tools? What are your thoughts about that  
**Speaker 1** 00:22:49:  Ethical concerns with with ai?  
**Speaker 0** 00:22:51:  Yeah.  
**Speaker 1** 00:22:52:  Um,  
**Speaker 1** 00:22:54:  I think that the fact that I'm having to think suggests that I don't really have any top of mind ethical concerns. I guess maybe you're feeding some, some personal information into a software tool, so you would want, obviously the data to be protected, but I think that the onus would lie on the research is to make sure it's anonymized before you feed any information into the tool where possible. But yeah, if it's not possible, you need to feed in personal information. Obviously you want assurances that that will go no further than than the AI tool. It might be sent to the companies or whatever. So, yeah, um, I hadn't thought of ethical concerns of AI previously, but yeah, I guess if you are using personal information, you definitely need reassurances around that.  
**Speaker 0** 00:23:30:  Right, thanks. And, uh, let's say that you're using an AI tool in your interview. For example, we are using AI now. It'll give us some kind of information at the end of the tool transcription and analysis, and it can, uh, uh, take some, uh, prompts, uh, that you have to feed to open AI or Gemini to get the information. And then you can get the same kind of stuff on your website as well. So, uh, if you're using the, that, that kind of tool to facilitate your interviews, uh, what kind of information you want to see firsthand, uh, to take a quick look at your, the result of your interview, uh, for example, key insight or action items or some sentiment analysis along that line. So what would be the most important information that you want to see from this tool to give to you at the end of your interviews?  
**Speaker 2** 00:24:24:  Um,  
**Speaker 1** 00:24:28:  I don't think I've got anything set in stone as such, but just as, as, as, as long as it captures the whole conversation, I guess being succinct is quite key. So we want to be able to go to our stakeholders and just have like a bullet list of 10 interesting things that came out of the interview. So working for a law firm, a lot of the, our stakeholders are, are lawyers obviously, and they tend to like, have, um, fees of around like 3, 3 50 pounds an hour that they charge to their clients. They, they really are protective of their time. They really value their time. So with any insights we share with them, we try our best to be succinct as possible and not share anything that it won't be of interest to them. So I guess with that in mind, I'd want any output from AI to, to be succinct and yeah, to be straight to the point, summarize what the key interesting things are, um, and maybe be sort of re more recommendations focused.  
**Speaker 1** 00:25:14:  So not just like saying here's the interesting things, but kind of like, here's what came out of the, the conversation and therefore that means to your role you should be doing X, Y, Z just to make it more meaningful. And to this, the, the stakeholders, obviously that's a little bit tricky 'cause the AI tool therefore need to know who the stakeholders are and what, what they're working on, what the goals are. I think anything like that would, would be of interest. But I think the main thing for me is being succinct, but also being exhaustive. So as I mentioned, I have a couple concerns about using AI currently video interviews. And that seems to be that when it does, um, summarize a conversation, it's good and it is reflective majority of the conversation. But there are things that I think should have been like, brought out which weren't brought out. And that's not because the AI tools, um, not good at summarizing that, the text, but I think it doesn't like understand the emphasis that our business will give to certain things that are said or the, the way someone said, the emphasis they put on it, the tone of their voice, the way they were looking. And you can really mess, you can really miss kind of how important certain things are to individuals when you're just analyzing text. So I think, I think those sorts of things would, would, would be useful as well. Mm-Hmm,  
**Speaker 0** 00:26:14:  \<affirmative\>. Yeah. Right. Thank you very much. Uh, yeah, that was very important point. And, uh, at the end of your analysis, uh, uh, when you have to analyze the data and then, uh, get some meaningful, uh, information in terms of, uh, plots or figures, how do you usually visualize your data? Uh, do you use similar tools that you have used to record the data or it's always a different tool you have to download and, uh, set up your study somewhere entirely different? So, uh, yeah. Can you tell me a bit more about the data visualization and then at, at the end, the data presentation to different stakeholders? Yeah,  
**Speaker 1** 00:26:52:  So we might be quite, um, limited in our data visualization in that what what we tend to do is analyze the results ourselves, work out with the key in the site site, and then we'll visualize that manually through a PowerPoint presentation. Typically, um, there are other tools I've been used in the past, very, very rarely. It tends the majority of our presentations will simply a PowerPoint presentation, sometimes maybe a video show reel. So pulling up with the key insights and being able to have a client in their own words, say it in a video show reel. Um, but other than that we don't really use any, any other sort of visualization platforms. It tends to, tends to be mostly PowerPoint really.  
**Speaker 0** 00:27:22:  Uh, so you, you, you didn't have that kind of functionality in Qualtrics when you were using that?  
**Speaker 1** 00:27:28:  When I used that and at BCA we had that, so I'm talking about current role and I used a Qualtrics at BCA, we had dashboards in the backend from Qualtrics. So we had a survey, went out to our contacts, someone called a contact center, complete a survey to say how the experience was and give a verbatim and response. So we'd use Qualtrics to sort of, um, visualize that in a dashboard we send out to different stakeholders. And also sort of qualitatively it would pick out the key themes in the word cloud and use other kind of sentiment analysis as well. Um, but yeah, we did didn't use it loads you, we just created a couple of dashboards then just sort of made the different stakeholders aware of them. But yeah, in my current role it's, it's more basic, I would say. So we tend to, like NPS for example, we, we will create a dashboard in Excel. It's just simple like bar charts really, and no real sort of the data analysis. We, we'll, I, I guess we'll use AI for coding occasionally. Um, with, with it, with NPS look, we'll look at what the key themes are that coming out in the responses, but we'll never kind of use it for visualization. So yeah, it's possibly a gap, um, of us at the moment, the visualization that we can certainly get better at. And b, correlation just manually, um, up creating a story through PowerPoint, really.  
**Speaker 0** 00:28:30:  Mm-Hmm, \<affirmative\>. Yeah. Right. Thanks. And, uh, what were, uh, if you have to give me some number that, how much time do you usually spend on all these uh, uh, projects like from product planning to recruiting the participant, then data, uh, recording, then data analysis, and then at the end, visualization and reporting. Can you gimme like a rough estimate of the numbers for each of these steps? So like how much time you usually spent and, uh, yeah, how AI could be bit more faster? Uh,  
**Speaker 1** 00:29:08:  Yeah, and so it does vary project to projects and that's no two projects. So I really the same I'd say on average. And kind of when a brief comes through it maybe be like a, a day and two days to actually sort of plan out the research, design, the discussion guide, design the survey, and also kind of get it scripted and ready to go. And then when it's in field, it's kind of minimal inputs and from client insight over then sort of reminder emails, monitor and responses. And then when we get the data back, I'd say the analysis reporting is the probably the longest stage and that we work on. So that, that must be like a week, two weeks, um, for more sort of complex projects. And sometimes you have more than that, it's a much more in depth project. Um, but yeah, that, that's definitely the bulk of the, the work lies and that's bit where we're looking to understand what our clients have said, but also think of a compelling way to tell that back to the business and, and, and also be succinct so that we're not wasting people's time, but also as I say, and tell it in a compelling manner that the insights land and remembered  
**Speaker 0** 00:30:01:  Mm-Hmm, \<affirmative\>. Yeah. Right. Thanks a lot. Just, uh, one of the last question. Uh, uh, how do you recruit different kind of participants,  
**Speaker 1** 00:30:12:  \<inaudible\> stuff? The research projects we carry out, certainly in my area with our own clients. So we have access to all the details and then working with us, we'll go out to them and on occasion we'll look to speak to the market. I've not been involved in that too much, but we work with panel providers. Um, when we do go to the market, so for a brand tracker, I believe we work with Motif, um, who a London based agency, um, Savannah, now actually they've been bought by Savannah, so they'll, they'll, um, provide the panel for that. Um, but if, if we had a new brief come through tomorrow looking speed to the market, we'd, I guess we would speak to panel providers and see which one is the most economical way of speaking to the clients of interest.  
**Speaker 0** 00:30:47:  Mm-Hmm, \<affirmative\>. And do you get, uh, uh, do, do you get often the participant of your choice, they, their expertise wise or background wise? Are these two agencies good enough for you or you? Uh, you always have to, you always feel like, uh, there is something missing and I might explore another like prolific or respondent or any other agency that might provide you more recruiters, sorry, uh, participants.  
**Speaker 1** 00:31:12:  Yeah. So, um, in internally that, that, that's rare because I say the clients we have are the clients. So if, if it is a difficult to reach group, it's just kind of that, that that's the case. They're, they're a small group, we're not able to reach them. But externally, we're looking at the markets. Uh, I say limited, um, examples of this. 'cause we don't really speak to the market too much that we tend to be able to target the people that, that we are interested in. But when we go for market projects, it tends to be just a re sample, really. On occasion we'll look for like legal decision makers. And we, we've not yet had any issue trying to find the people that, that way we're speaking to.  
**Speaker 0** 00:31:42:  Mm-Hmm. \<affirmative\>. Yeah. Very good. Yeah, thank you very much for, for, for giving really nice overview of many things. No problem. Do you have any question for me or about our company?  
**Speaker 1** 00:31:54:  No, no, not at all. That's, that's, that's, that's been, uh, let's have conversation with you. I'm gonna have to shoot up in a second since get my daughter from nursery, but no, that's, hopefully that was all useful.  
**Speaker 0** 00:32:03:  No, it was really useful. Uh, thank you very much for your time. I will, uh, you will get, uh, the Amazon voucher on your email, uh, that you'll use to register for this talk. And uh, yeah, sorry, it might take around eight to 10 days. Uh, it, it'll get approved soon. I will, uh, it right away. Uh, but yeah, yeah, that's, uh, yeah, that's  
**Speaker 1** 00:32:22:  Fine. Eight to 10 days did you say?  
**Speaker 0** 00:32:24:  Yeah, yeah, around that. Yeah.  
**Speaker 1** 00:32:25:  Yeah, that's fine. Thank you for that.  
**Speaker 0** 00:32:28:  Thank you very much. Have a nice rest of the day.  
**Speaker 1** 00:32:30:  You too. Nice. Speak to you. See you later. Bye bye. Thanks.  
**Speaker 0** 00:32:33:  Bye bye.

# Rob Hill 

Customer Research Manager at Entain 

## Meeting summary: To be filled 

## Transcript:

**Speaker 1** 00:01:36:  Hello? Hi, Rob.  
**Speaker 2** 00:01:38:  Hello.  
**Speaker 1** 00:01:39:  Can you hear me well?  
**Speaker 2** 00:01:40:  Uh, yep.  
**Speaker 1** 00:01:41:  Very good. Thanks. How are you doing?  
**Speaker 2** 00:01:43:  Yeah, all good to have you.  
**Speaker 1** 00:01:45:  Good, good. Uh, yeah. Uh, so, uh, my name is, uh, re I'm a product manager in the research and Insights at Beings. It's a startup company. Uh, we are developing a AI products for market research. Mm-Hmm. \<affirmative\>. And, uh, our CEO Dave who was talking to you, he is, uh, he is on holiday now. It's only me, uh, talking today. Sure. We have another participant called ai. It's our, uh, tool, well AI tool that will, uh, be recording the meeting and then giving transcription at the end. Are you okay with that?  
**Speaker 2** 00:02:17:  Yes, fine.  
**Speaker 1** 00:02:18:  Thank you. Uh, okay. So yeah, we can start with the, uh, please, uh, tell me a little bit more about, uh, your job and, uh, what kind of work you do and, uh, what are your day-to-Day responsibilities? A little bit about that, and then I can start asking a bit more tailored questions.  
**Speaker 2** 00:02:36:  Sure. Um, so I work at a company called Tain, uh, who own a number of gambling companies. Um, I'm a consumer insights manager for sportsbook. So a lot of my role is just working out, like, uh, getting a better understanding of consumers in general, um, and specifically our customers as well. So the main way we do that is through surveying either our customers or external, um, consumers. Uh, and it could be on like a range of different topics. So it could either be based on something that we're, um, planning as a wider business, or it could be based on like specific requests that come through from different teams. They might say, I wanna understand X, and then we'll basically design a questionnaire and send a survey out based on that and analyze the results for them, send them back.  
**Speaker 1** 00:03:33:  Very good. Yeah. That's very interesting. Mm-Hmm, \<affirmative\>. And, uh, like if I look at your overall experience, you have quite a lot of experience at the big agencies like I and the yonder and then and chain. So how do you see the use of technology or in particular AI is, uh, is differ, uh, is different between these settings?  
**Speaker 2** 00:03:54:  Um, to be honest, I didn't see it a lot when I was at agency side, so there wasn't, in my job, there wasn't a lot of use of it. But since coming to Tain, and particularly in the last sort of six months, um, there seems to be a lot of companies selling sort of tools that are aggregating data, for example, online for the use of ai. Um, it seems like a bit of a, a bit of a race to sort of monetize that at the moment.  
**Speaker 1** 00:04:23:  Oh yeah, that's true. Uh, and, uh, with your, uh, in your current responsibilities, uh, what's the nature of your research? Is it mostly qualitative or you already deal with existing data sets? Like quantitative side?  
**Speaker 2** 00:04:38:  So it's, it's predominantly quant, but we do some quote as well. Uh, it kind of depends on the request, but a lot of it'll be sort of new research. So what if we can get the answer from existing data, then we'll do that. But a lot of the time it requires new research from us.  
**Speaker 1** 00:04:56:  Okay. Very good. And have you started, uh, using AI for any of these sites?  
**Speaker 2** 00:05:02:  Um, not specifically for any of these yet. We have recently signed up to a couple of different sort of AI based platforms, um, but I haven't had much use of them yet.  
**Speaker 1** 00:05:15:  Right, thanks. Could, uh, would it be, uh, possible for you to tell me the names of those platforms or,  
**Speaker 2** 00:05:21:  Uh, probably not, to be honest. \<laugh\>.  
**Speaker 1** 00:05:24:  \<laugh\>. Okay. Right. And, uh, so like this is, you said that's the gambling industry, and do you are, uh, studying consumer behavior? Mm-Hmm. \<affirmative\> and every time the data could be a little bit different, uh, trying to understand the different aspect of that consumer behavior. And then for that project planning, do you always, uh, uh, do the planning by yourself or you take help from some behavioral scientist or some psychologist?  
**Speaker 2** 00:05:56:  Um, not at the moment. That is something that we're starting to look more into in some of the work that we've got planned. But predominantly the, the stuff we do is, um, more about like, let's understand what customers think of this current product or something like that. So it, it's, um, a bit more straightforward. But some of the bigger pieces we've got planned, we are sort of involving behavioral psychology and yeah.  
**Speaker 1** 00:06:24:  Right. Okay. Thanks. And, uh, would it be possible for you to gimme some kind of example of your work, uh, basically from product planning to data recording, data collection and Yeah. Is to the end, uh, basically the whole product journey where you have used any AI on the way, or if you think that, uh, it could be useful, uh, in some way if you haven't used it so far?  
**Speaker 2** 00:06:54:  Yes. I think the, the main place that we would use it and have used it has been sort of as part of the initial brief. So the work tends to work as a stakeholder will send us a brief. We will then sort of interrogate that brief, um, to work out what's the best, uh, way to go forward to answer their questions. Um, then we'll also maybe use some of the AI tools that we've signed up to, to see if we can get any information on that already. Um, so if it was, for example, I had a request recently where someone just wanted to know sort of the percentage of gamers in the uk. So there was a couple of internal bits and pieces, bit, uh, bits and pieces that we had on that, but also could have used, uh, ai there didn't on that occasion, but that's sort of where it would come in in the project. Just sort of the initial brief and sort of scoping out some of the stuff that they wanna talk about.  
**Speaker 1** 00:07:47:  Oh, very good. Thanks. And what do you think about, uh, the quality of possessions or the participation of AI there?  
**Speaker 2** 00:07:56:  Um, I'd say it's mixed. So some of the tools that we sign up to are better than others. Um, like I said, it, it does seem like a bit of a race at the moment. So I think a lot of the companies have sort of half built tools and they're not necessarily that user friendly or they don't necessarily show you exactly what you want. So there was an example of one the other day where they were demoing, um, how you'd find a certain number. And then the data that it pulled out was from like 2016\. And I was like, okay, well that's probably not really usable. It's eight years ago now. Uh, you've had covid in between that, it's kind of redundant, but yeah.  
**Speaker 1** 00:08:36:  Yeah, yeah, that's true. And, uh, what, uh, like this is a race, like you said, that a lot of companies are trying to use? Uh, I mean, in, in principle the foundation of ai, like data science, machine learning and many other things has been already around people were using. It's just the layer of generative AI that changed the whole perspective. And now everybody trying to use it because the applications become more clear. Mm-Hmm. \<affirmative\>. Uh, but depending on the industry size, for example, Ipsos or the other bigger companies, they might be developing their own tools and uh, sometime they're taking help from the other tool tools that are already existing in the market. Uh, so if, uh, you have to get into this race, uh, what would be your, uh, view, uh, would you prefer to develop something in-house by your company? Or you would be more confident to pick the best thing available in the market for ea?  
**Speaker 2** 00:09:33:  Um, I'd say based on what I'm seeing at the moment, the preference would be in-house. But maybe that's because the products at the moment don't feel like they're fully finished. Um, I think in the future I'd probably prefer to pick the best product on the market, but at the moment I would probably prefer in-house just because I want to use it the way I wanna use it. Um, yeah, I don't, I don't feel like the products that I've seen so far aren't that good.  
**Speaker 1** 00:10:05:  So, so, uh, yeah. Thanks. Yeah, I totally understand. And that they're not that good from, uh, uh, the features point of view, the all the functionalities they're providing or, or the quality of information they're spitting out?  
**Speaker 2** 00:10:19:  I'd say both. So quality of information is sometimes not very good, like the example that I just mentioned. But I'd say also because they are trying to sort of mass market a lot of these tools, they're not bespoke. It's a kind of one size fits all. Um, and I think also there's one in particular that, um, we've signed up that's sort of been developed for a particular market, so sort of more financial services. Um, so it's probably great for that. But as a gambling company, it's kind of okay and does a reasonable job, but is not gonna be bespoke to us. It's more tailored to the financial services companies 'cause that's mm-Hmm. \<affirmative\> where most of their client base is.  
**Speaker 1** 00:11:01:  Yeah. Right. Thanks. And can you, can you dig into more detail there that, what are your fear that you think, uh, could not be solved by AI specific to your industry?  
**Speaker 2** 00:11:14:  So a lot of it is just around general numbers on, so for example, one of the big problems is finding market size of different brands for gambling companies because there's a lot of, there's a lot of different data sources that you can use. You can use survey data, which is often unreliable. There's also some companies that produce market size, but it's quite outdated, so they don't release it until like a year after. Um, so obviously it's quite difficult to find what you want. Now there's also sort of conflict in terms of market size. Do you want volume or do you want value? Obviously two very different things in our industry, like volume of players versus the value of those players. Um, so yeah, I think that that's probably one example that it can't solve at the moment.  
**Speaker 1** 00:12:04:  Yeah. Right. Thanks. And, uh, so so you think that, uh, to get an answer to get decisions about these issues quicker, you will prefer the manual interference or humans over AI to give you the best fit?  
**Speaker 2** 00:12:21:  Yes. For now, yeah.  
**Speaker 1** 00:12:23:  Right. And, uh, what do you think about the use of synthetic data or simulations to simulate the behavior of consumers or trying to understand, uh, the, uh, uh, perception about it? Again, in, in some industry, uh, based on the past information,  
**Speaker 2** 00:12:42:  Uh, I've not seen it done anywhere in anything I've used. I'd be open to looking at it. I think they'd probably be quite a lot of distrust in it if it is infect data. But yeah, I mean, I think for me, it'd probably be one of those things that you'd have to run alongside maybe some research to look at validating how accurate that infect data was. So yeah, let's spit out the synthetic data and then let's also run something alongside to, to validate that before we then use synthetic data in the future again.  
**Speaker 1** 00:13:19:  Right. Thanks. And at some point you mentioned about, uh, that the, some of the A tools was trained on 2016 data and you didn't trust it because it was too long, uh, too long ago. Uh, so is your data very frequently changing and it's really fast paced and uh, you can rarely use the old information? Is it the nature of your research?  
**Speaker 2** 00:13:43:  Yeah, so in terms of that specific example, that day would've changed a lot. Like Covid had a massive impact in Mm-Hmm. In the industry. So, uh, it would've changed a lot. There's also regularly sort of new entrance to the market, um, that can have an impact as well. So the 2016 data would've been yeah, redundant.  
**Speaker 1** 00:14:07:  Right. Thanks. And, uh, so yeah, like it, I, I think slowly in the coming years, I, I'm hearing a lot of, uh, distrust about the quality of the information and uh, also the way, uh, AI tool are friendly, user-friendly or not friendly or too way too complex for using. So there are certain, uh, reasons for resistance there. Mm-Hmm. \<affirmative\>. But I think these things will be overcome in next few years. Uh, but in that kind of ideal scenario, uh, what kind of problem you think, uh, you can think of with that could be solved by, by ai, uh, rather than, uh, spending human time there?  
**Speaker 2** 00:14:52:  I think it'd predominantly be sort of desk research in my role. So it would save a lot of time in desk research if these things worked better. So at the moment, they basically, the ones that I've seen are basically the same as doing a Google search. Um, they're not necessarily much better than that. Um, obviously you don't get the ads at the top. Apart from that, it's pretty much the same from my perspective. So I think that would be the biggest use case for it in the future. Just saving a lot of time on desk research and background information.  
**Speaker 1** 00:15:25:  Right. Yeah. Thanks. Just to understand a bit more about d uh, research. So, so your definition of desk research is like when you're planning the project, finalizing the information, or you're talking about analysis at the end? Because I think, uh, depending on the job role, the desk research could be different. Meaning  
**Speaker 2** 00:15:43:  Yeah, it could be, could be both. So it could be for me getting additional context on maybe a brief that they're asking for. Or if someone comes to me and wants a quick turnaround number, like the, the gamers example that I gave, if they said how many gamers are in the uk, that's something that I could probably quickly do with ai. But I would also say if someone comes to me with a brief that I think probably the data already exists in like the external world and on the internet, that's definitely something that I could use AI for and go, okay, this is my question, what is the answer to that? And then use that for analysis.  
**Speaker 1** 00:16:24:  Yeah. Right. Thanks. And, uh, about, uh, the ethical side of the things, uh, uh, have you encountered any ethical challenge or concern, uh, when using AI in research or, uh, especially regarding participant data or potential biases in AI algorithm? Can you gimme some example if you have experienced or if you are just concerned about the future?  
**Speaker 2** 00:16:50:  So I think I haven't really experienced any problems, but I think the main concern is just about how much, um, we use AI and how much information we feed it. So I obviously deal with like personal information of, um, customers and consumers, for example. So there's, I think that would be my main skepticism, like how much data can I feed it, do I trust it, that it's gonna keep that data safe and protect it? If I did feed it that information, obviously I wouldn't at the moment it's obviously our policy not to do that sort of stuff and push that out. But I think that would probably be the danger in the future that people could easily, uh, get that data if people use it, like putting it in the wrong place and then yeah, it could be taken basically or stolen by someone online.  
**Speaker 1** 00:17:45:  Right. Okay. And, uh, when you're dealing with this customer research, uh, uh, what, what do you think how much AI could help there to provide personalized experience, uh, for, for customers? Do you usually just analyze the data or the result of your data are also used to provide personalization, uh, experience to, to your users as well?  
**Speaker 2** 00:18:13:  Yeah, they can be. So a lot of the time what, what my job is, is to basically analyze the data and provide insight on and recommendations on what the different teams should do with that insight. Um, it's then their job to sort of build that personalization, if that is the example or build a new product or whatever the insight might be. Um, but yeah, I sort of passed that part onto them.  
**Speaker 1** 00:18:40:  Mm-Hmm. \<affirmative\>. Right. Thanks. And, uh, for qualitative research, can you, uh, highlight, uh, some of the, uh, key insights that, uh, you usually draw from that research? Uh, what are the main messages you want to see from the qualitative research?  
**Speaker 2** 00:18:59:  Um, I don't really have an answer to that. I've not really done a lot of qualitative research in my career. It's predominantly been quant.  
**Speaker 1** 00:19:12:  No, yeah. Sorry. Uh, I then probably misunderstood. You mentioned in the beginning, uh, that there was a bit of qual involved.  
**Speaker 2** 00:19:18:  Yeah, there's a bit of qual, but uh, yeah, not much.  
**Speaker 1** 00:19:22:  So is there like somebody else doing that and you rely on the results? Uh,  
**Speaker 2** 00:19:27:  Yeah. So usually it'll be a, an agency that we'd hire to do that sort of stuff.  
**Speaker 1** 00:19:32:  Oh, okay. So for all the numbers related stuff and quant related stuff, you are doing it in, in-house and for anything qualitative, you hire an agency and they try to provide you  
**Speaker 2** 00:19:43:  Yeah, for the, for the quantity, sort of half and half. So half of it we'll do in house, half of it'll probably hire an agency. It depends who we need to speak to. So obviously we have access to our own customers. So if it's specifically about our customers, we can usually do the research ourself. If we need to speak to like a wider consumer base, then we'll have to engage with an agency to help us do that. Um, qualitative, we tend to go through agencies. 'cause within my team we don't really have the skillset of qualitative. We're predominantly quant researchers.  
**Speaker 1** 00:20:18:  Oh, okay. Right. And, uh, what if in the coming days, uh, there is a, uh, some AI platform, uh, that can, uh, empower you from qual research point of view as well, will you be interested to explore or you will stay, uh, in the way you're working now that always outsourcing to agency?  
**Speaker 2** 00:20:39:  I think if there, there was something available, I'd definitely be interested. I just dunno what that would look like. So I suppose maybe it comes back to your earlier point about like synthetic data. Like if there's something quote that you could produce that could then be translated into a quant number, that's something that would be really interesting, I think, to us.  
**Speaker 1** 00:20:59:  Yeah. Right, right. Very good. Thanks. And let's say that, uh, uh, you have asked the agency to provide something they carried on their qualitative research then in what format or, uh, they, in, in what format they communicate their results back to you? Uh, like the, uh, word document, PowerPoint presentation, PDFs, how do they share their observation and Yes.  
**Speaker 2** 00:21:26:  Yeah, usually PowerPoint. Um, so that tends to be the way we present internally. So it'll usually be a PowerPoint that we'd review and then it would then get presented back to stakeholders.  
**Speaker 1** 00:21:38:  And so there's like usually less information in the slides than a proper document and you have asked somebody to do the research. So they have, they are sharing only the final information with you. And if you have a follow up question, how do you access the, the follow up information? Is it like, do, do you get the access to some kind of dashboard where you can see more information or, um, how do you get the information out of the agency?  
**Speaker 2** 00:22:03:  Um, usually just by asking them to be honest. So if it's not on the deck, then yeah, it would just be a case of we've got these follow ups. Uh, so the way I would usually work, they would present or I would present would then go back out to the stakeholders a week or two later mm-Hmm. \<affirmative\> just to check in and say, is there anything else that you needed that's come up since, um, pass that back to the agency and then we'd get them to answer those.  
**Speaker 1** 00:22:30:  Uh, very good. Thanks. And can you tell me a little bit more about the expertise of these stakeholders? Uh, are those always, um, or management board or if I'm missing something?  
**Speaker 2** 00:22:42:  Yeah, so they'll, it'll usually just be people from different teams. So they might be, uh, they may be working in like the marketing team for example, and they wanna test an ad, um, or they may be in the product team and they want to test one of our products. So their expertise will be on the actual product they'll, or the ad they'll tell us how it works, what they're trying to do with it. And then it's basically our job to test whatever they want us to test. So if they, if if it was just general like, test the performance of this ad, this is what we're trying to do with then design the research around getting them the best answers to whatever questions they want. Mm-Hmm.  
**Speaker 1** 00:23:22:  \<affirmative\>. Yeah. Right. Thanks. And, uh, can I ask a little bit more about the metrics or the kind of information you see in those PowerPoint presentations? Is usually like transcription or sentiment analysis or what, what, what are the different kind of metrics that you see in those presentations from agency?  
**Speaker 2** 00:23:47:  Uh,  
**Speaker 1** 00:23:47:  Gimme an overview of the things  
**Speaker 2** 00:23:49:  In qual or quant?  
**Speaker 1** 00:23:51:  Uh, yeah, in both. Yeah,  
**Speaker 2** 00:23:53:  In both. So  
**Speaker 1** 00:23:54:  Qu Yeah. First. Yeah,  
**Speaker 2** 00:23:56:  In qu I would say it's predominantly just their interpretation. So the main qu that I've done since I've been here has been focus groups. Yeah. So it's predominantly just their interpretation of the group as a, a whole, uh, what the key findings were. Um, and sort of any recommendations that they have around like improvements. Um, for quant it's obviously a bit more specific. So a lot of the stuff we'll be doing will be like, um, appeal, how much they like a product or don't like it, why or why not they don't like it. Like are they likely to play again in the future? Um, why they using certain brands over our brands, uh, what makes them choose a brand? Like it's all that sort of stuff. Yeah. Usage, consideration, all those sort of metrics  
**Speaker 1** 00:24:50:  And yeah. Right. Thanks. And do they try to give you like additional information on top of, so these, the things you mentioned are, we can call them key insights of a meeting, the main lessons that you have learned. Uh, but other than that, do they try to give you the sentiment analysis or the tone of the meeting or people were happy or sad or when they talked about some product feature they were extra happy or some additional information like, uh, like you trying to make sure that, oh, you were also sitting there in the focus group and it's not just a presentation they try to take with them, uh, bit more personalized experience, uh, providing information in a way that ah, you are not missing anything. You got the key insight, but then you also got something on top. So I'm wondering what was in addition to key insights?  
**Speaker 2** 00:25:37:  So tone of, I'd say yes, that would definitely be given like party agency on the focus groups for example. But sentiment analysis, no, it's not something I've ever done, uh, since I've been here.  
**Speaker 1** 00:25:49:  Mm-Hmm. \<affirmative\>. Right. Thanks. And, uh, at the end of those presentations, have you ever felt like if there was something more that's usually missing?  
**Speaker 2** 00:25:59:  Uh, not really. Most of the agencies we use are pretty good. So we've, we've kind of built out a roster of good agencies over the last couple of years. So yeah, the bad ones don't tend to come back and get any work from us again.  
**Speaker 1** 00:26:17:  \<laugh\>. Yeah. Yeah, that's right. Thanks. And, uh, can you, can you tell a little bit more about the quant results as well? If you ever hire agency, uh, do they run surveys for you or They still run qualitative interview but you get information only in terms of uh, tables, plots or spreadsheets?  
**Speaker 2** 00:26:38:  Um, so they will run surveys for us as well if we have an agency. So that would be if we wanted to contact people that weren't our customers. Um, in terms of access to the data, the agencies that we use are pretty transparent with that. So if I said I wanna see the raw data, they'd happily send that to me. If I wanted to do my own analysis again, they'd be quite willing to let me do that. Um, it's a very collaborative approach. So the way that I tend to work with the main agency I work with on quant is, um, I'll be having sort of weekly catchups with them. They'll be telling me everything that's going on, what stage they're at in terms of field work, et cetera. In the analysis stage. We'll probably have a couple of meetings just, they'll start it off and basically say, look, this is what we think. Is there anything you can think of internally that like, does this ring true? Does it not? Is there anything else that you want us to look at? So yeah, I'd say it's very collaborative and they're quite transparent. Mm-Hmm,  
**Speaker 1** 00:27:39:  \<affirmative\>. Right, thanks. And, uh, if we talk about the tools used for quantitative analysis, uh, do you also have, uh, tools developed by your company or you mostly rely on the outside quantitative ANA analysis tools in the market?  
**Speaker 2** 00:27:55:  Uh, so it's a mix. Um, not in my team specifically, but there are teams within the business that, um, have created dashboards, for example, or, um, sort of big Excel sheets with like sort of pivot tables where you can sort of drag and drop different, um, sort of financial metrics and stuff like that if you wanted to see anything in particular. So yeah, but then we also rely on external data. So we've got a few dashboards that we're signed up to in terms of like consumer data where we can look into it and get a quick answer on different things if we need it. So more sort of attitudinal, um, yeah, stuff like that.  
**Speaker 1** 00:28:37:  Right. Thanks. And, uh, if we came out of, uh, the type of research but focus more on your project management that how many people, different kind of stakeholders you have to talk to, uh, do you think at your project management level AI could be helpful? Uh, and save you some time? If you can gimme some examples, how it can save your time at your management level?  
**Speaker 2** 00:29:01:  Um, save me time in terms of dealing with stakeholders?  
**Speaker 1** 00:29:06:  Yeah, dealing with stakeholders, talking about project result interpretation, uh, uh, basically all kind of meetings that you have with people around you or with agencies.  
**Speaker 2** 00:29:16:  Yeah, so I think it, it would mainly be maybe in how the results were accessed. So if they could almost self-serve a lot of the results, if we were able to give them access to a platform that had answers to their questions, um, that could maybe be a way to save time 'cause it would stop from coming to me. Um,  
**Speaker 1** 00:29:37:  Yeah,  
**Speaker 2** 00:29:38:  Yeah, that's probably the main use case I can think of.  
**Speaker 1** 00:29:41:  Uh, so right now the information is uh, uh, kind of as, uh, under uh, each person's authority or under each person's, uh, uh, responsibility. And if you want to know something, it's not like, uh, uh, openly available even within your company, it's like siloed information and  
**Speaker 2** 00:30:01:  Correct. Yeah. So a lot of different departments hold different bits of information. So if I, if I want XI have to go to this team over here. And likewise, a lot of people will come to our team and say, have you got information on this? Uh, and it might be that we have to go somewhere else to get that, whether that's one of the platforms that we sign up to or one of someone in a different team that we know has that information. So yeah, it's quite disjointed I would say.  
**Speaker 1** 00:30:28:  And, and we are still talking about the information that's strictly needed about different kind of research. Right. And  
**Speaker 2** 00:30:34:  Yeah, it could be, it could be anything. So the research information is kind of all held with us, but we, at the moment, we don't have a platform internally that we use to sort of publish that all. It's kind of, it will get sent to the individual stakeholder that requested it, but then if another stakeholder came to me the week after and said, oh, have you got this information that I'd already given to the other one the week before, I'd then have to sort of resend that again.  
**Speaker 1** 00:31:01:  Yeah. Right. Yeah. That, that seems like a lot of companies, uh, have that kind of system. So I'm wondering that, what, what could be the reason in your company? Is it like some data trust or privacy issues that what, what could be the reason that they're preferring that way?  
**Speaker 2** 00:31:16:  Yeah, I'd say part partly they of trust. I think partly also the way the team was managed in the past. So we've had a lot of changes in the team and the, the management of the team and um, there was a quite a lot of reluctance from some of the previous managers in the team to share data. So it just meant that all kind of sat with us. Um, it's something we're looking into at the moment, um, as a part of the new team that we're set up to have some sort of platform where people can start to self-serve and it'll hold sort of all the research that we've done previously.  
**Speaker 1** 00:31:55:  Right. Thanks. And can you give me just this last question, uh, that, can you gimme name of uh, some of those, uh, qualitative or quantitative tools that you often use?  
**Speaker 2** 00:32:07:  Um, so they're all kind of dashboards that different research agencies run. So we've got, we've got a few different ones at the moment. Um, but yeah, they're all just sort of general dashboards. They're all kind of one and the same, I'd say. Um, yeah, so one of the companies we use is YouGov, for example. They've got a couple of dashboards that we sign up to. Um, the, yeah, one of the offer agencies is koro. Um, so they provide a couple of different things as well. They kind of provide this sort of, uh, monthly report that we sign up to that gives sort of general consumer information. So it's all, all stuff like that.  
**Speaker 1** 00:32:48:  Oh, very good. So yeah, that, that was mostly about the interpretation level. When you see the results in the dashboard, what were the tools that are used to analyze? Uh, I mean you can forget what the agencies, uh, what what your in-house, uh, tools, uh, what could be the name of those tools, uh, that you are  
**Speaker 2** 00:33:06:  Using. Oh, okay. So you're thinking like Tableau, stuff like that?  
**Speaker 1** 00:33:10:  Yeah, yeah. Like, uh, yeah, for Power bi Tableau or?  
**Speaker 2** 00:33:13:  Yeah, so we use  
**Speaker 1** 00:33:14:  Bi,  
**Speaker 2** 00:33:16:  We use Power bi. Uh, one of the ones that we've got is actually just an Excel, uh, that's got Pivot table information. You just drag and drop it. Uh, that's one of the most commonly used ones that I have. Um, but yeah, I'd say the main ones are Power bi, I probably,  
**Speaker 1** 00:33:34:  Okay. Very good. Thanks. And, uh, yeah, if we are to like zoom out, uh, out of your project management and research and look at the entire, uh, picture, uh, for all of your day-to-day responsibilities. So, and if there is no technical problem, there is no funding problem. And what would be your wishlist from AI to how can AI should help you? How, how can AI help you for various steps so that uh, you can have a bit more augmented experience where you can feel that, okay, AI is really helping me to do my job quicker and uh, to getting results faster. So what would be your wishlist for various steps of the product planning?  
**Speaker 2** 00:34:12:  Uh, so I'd say at the beginning just to be able to find information a lot quicker. Um, in terms of background, I'd say once I got to the analysis stage, more sort of, um, automation of reports. So I have to like write and um, design a lot of reports myself. So in terms of charting, so if AI could do a lot more of the charting, I know there obviously are automation tools out there, but if there was an easy one where I could just plug in the data and it spout out the information in the best way in the best possible charts, that would be good. Um, and then I suppose part of the analysis as well, if I could give AI a data set and it could spot trends, for example, like opportunities, um, within that data, that would probably help as well. 'cause it saves me working out what they are.  
**Speaker 1** 00:35:10:  And how, how do you work out now? Like is there some, uh, writing some code or manual analysis? How  
**Speaker 2** 00:35:15:  Do you, just manual analysis. So I'll be charting the data and looking for the trends myself or looking for the interesting insights. So if AI was able to do that for me by just sending it the database for example, that would be good.  
**Speaker 1** 00:35:29:  Alright, thanks. And sorry, just one last question about the type of data sets that you feed to your analysis framework. So can you gimme an example of the type of data sets you feed? You have Excel sheets, uh, what, what else? You can feed it there.  
**Speaker 2** 00:35:45:  It's basically all Excel. Most of what I deal with is Excel. Um, so yeah, it would just be a large data sheet with all the different questions and all their different answers, all them on Excel.  
**Speaker 1** 00:35:55:  Okay. Right. Thanks. Thank you very much. Uh, think, uh, yeah, that was more or less all of my questions and uh, I really enjoyed talking to you. Uh, do you have any question for me or our company?  
**Speaker 2** 00:36:08:  Uh, no, I just suppose there was the mention of the, the voucher. So just how I, I claim that would be good to know.  
**Speaker 1** 00:36:14:  Uh, right. Yeah. Thanks. So I will, uh, yeah, send it to your email. Mm-Hmm, \<affirmative\>, uh, the one that you use to communicate. Uh, but uh, it has to be approved by Dave who's on holiday, so it might Yeah, no worries. Few more days. Uh, but yeah, definitely I will order it right away now.  
**Speaker 2** 00:36:29:  Okay, perfect.  
**Speaker 1** 00:36:30:  Yeah, you will receive an Amazon voucher, uh, in your email.  
**Speaker 2** 00:36:34:  Perfect. Thanks a lot.  
**Speaker 1** 00:36:35:  Thank you very much, Robert. It was really nice talking to you. You too, rest of the day.  
**Speaker 2** 00:36:40:  You too. Cheers. Bye bye-Bye.

# Bethany Smith 

Customer Research Manager at Flutter Entertainment Plc

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:00:  Manager in the research and insights at Beings. It's a startup company. We are developing AI products for, uh, market research. And, uh, uh, you might have been con uh, you were probably contacted by Dave and he's the CE of the company. He's, uh, he's oncologist. So it'll be only me, uh, talking to you. Um, yeah. Uh, so yeah, maybe we can start with the, uh, knowing a little bit more about your job and you can tell me, uh, what are your day-to-Day responsibilities and what kind of work you do, and then I can ask, uh, a bit more questions.  
**Speaker 1** 00:00:34:  Yeah, of course. Um, so I'm a research manager and so manage very small team. It's just myself and um, a senior analyst. Um, so we would tend to some of the, some of the stuff we do is quite strategic, so there's a combination of looking at like external research papers and academic research, um, and then also doing our own primary research as well. So whether that's surveys, focus groups, depth interviews, online communities. Um, I'm slightly less hands-on, um, with the research than my senior analyst. Like she would tend to do a lot more of the executional stuff. I'll do some of it. Um, and my job often is to synthesize the insights that we get from the different sources, um, and try and tell, uh, sort of concise and engaging insight stories and share those to stakeholders.  
**Speaker 0** 00:01:28:  Very good. Thanks a lot. Uh, you mentioned also ac uh, about academic research. So are you like, uh, working both with industry partners and uh, universities?  
**Speaker 1** 00:01:38:  Um, no. So it'd be more just accessing these papers online and, and state of state with, uh, yeah, what the latest academic research is.  
**Speaker 0** 00:01:47:  Okay, good, thanks. And, uh, how often do you use AI for various kind of steps in your research? And if you can give examples, how do you use it?  
**Speaker 1** 00:01:59:  Honestly, we barely use it and we feel like we should be doing, and I think that's probably, that seems to be a trend among my wider team of researchers. So I many team of around 17\. Um, I feel like it's something we should be doing, but we've quite busy and it takes that time to step away from what you're doing and learn how you can employ those tools and it just feels like none of us have the time to do that. Um, or that's what we tell ourselves. Um, so I've used chat GPT, um, a little bit over the past few months and maybe I would use that to help me, like make my, um, questions more concise or re reword things. Or sometimes I'll kind of ask it some of the questions that I might have in a brief just to see what it comes back with for that background context. Might prompt it to try and direct me to like certain pages, um, or certain sources, but just take a lot of it with a big pinch of salt because you've, we've obviously heard it can't be like a hundred percent reliable. So chat GPC can be a good like jumping off point, um, but it's not something I over really rely on to be honest.  
**Speaker 0** 00:03:05:  Right. Thanks. And, uh, at which step do you usually, uh, use it when you're planning something or when you're analyzing, how do you use chat GPT?  
**Speaker 1** 00:03:14:  Yeah, so I tend to use it more for like ideation. Um, so like planning, um, stages and setup, even though it's just getting to know my research topic a little bit better and getting more background information on that, um, I would use it for some analysis but not others. And I guess I wouldn't want to be putting sensitive data into chatt again. I'm not sure really how confident I would be in terms of like how ethical it's and whether you could trust it. Um, I'm probably sounding quite naive now, but  
**Speaker 0** 00:03:49:  No, no, it's not naive. I mean when uh, there is a new emerging technology coming into market, everybody else there on concern and it'll take some time to get the confidence there. Yeah, that's why we are trying to understand that what could be the different kind of, uh, uh, problems that could be solved and in parallel how to build trust that you can find it really useful. Uh, can you walk me through some example of your project from, preferably from the beginning, like, uh, when you plan the project and then how do you, uh, think about uh, recruiting participants or the wi any other way to collect data and then after that how do you analyze and Yeah, basically the whole journey?  
**Speaker 1** 00:04:31:  Yeah, so a typical project would probably look like we, um, only because I'm client side, so engaging with stakeholders on a particular business challenge and helping them to flesh that out to create a research brief. So that's where I might engage with chat GPT, kind of like I would a search engine to try and find out more about that topic, um, and kind of educate myself so that I can build out and enrich the brief with like additional context. Um, which would again help us to form more pointed research objectives and research questions. Um, if I was then moving on to creating like a survey or a discussion guide, um, again, if I get stuck I might just sort of riff off chat GPT and ask for prompts or suggestions to help me build out my research materials. Um, in terms of sample, we tend to go for our internal customer database, um, and send a survey out through that.  
**Speaker 1** 00:05:30:  Uh, and then once I have the data back, if I was confident in the, in the type of data, if I was happy to input it into chat gt, I might ask it to help me synthesize things or ask it what felt the common themes were. But I think 'cause I've not had training in leveraging chat GP PT properly, I don't really think I'm getting the best outta it. It's helpful to riff off. Um, but I think I could probably utilize it better as a tool if I had a bit more training on how to prompt it and navigate it.  
**Speaker 0** 00:06:01:  And what would you say, if there is a tool that will, uh, not ask you to learn these kind of things and deliver the, uh, right, choosing the right prompt or basically giving you some kind of automated information that, uh, is quite common for you then do you think you would be less reluctant to use AI technology when you don't have to spend a lot of time on uh, bridging the gap?  
**Speaker 1** 00:06:24:  Yeah, definitely. I think it's, um, it's about making it easy for people. Sorry, I've got in the background. Um, yeah, it's definitely just about making it easier. I feel like researchers have such busy lives, um, and it can just feel like it's another thing you don't have time for. Whereas if the technology was simplified and like focused specifically on researchers, so like you say, there's very like common prompts that would be useful to researchers. Um, if you, for example, even just have like a list of those pumps and how to get the best outta the technology. Yeah, I think that would remove one of the blockers.  
**Speaker 0** 00:07:01:  Right. Yeah. Thanks. Yeah, that would be brilliant. And uh, you mentioned about, uh, some customer data sets and databases and the servers also who is usually, uh, providing you that data. Do you also sometime try to find ways to collect new data by yourself? Like, uh, designing surveys or talking to people like qualitative research interviews, one-to-one or focus groups. Do you also have experience in that section as well?  
**Speaker 1** 00:07:29:  Yeah, I'm just going to monologue doing doing dunno, battery. So, so you talk about the different methods of data collection, right?  
**Speaker 0** 00:07:51:  Yeah, yeah.  
**Speaker 1** 00:07:53:  Um, yeah, so we typically, we, we do do mostly quantitative work, um, as a research team. So largely quant data, some open-ended questions, but I actually think we would do way more qual if it wasn't as time intensive. So if we could do more like online focus groups or online depths and then use AI to yeah, synthesize that data or even use AI to run, um, run interviews. I know I've, I've seen some options to do that lately. Like that's quite appealing to us as a team because the big block of the call is just like, we don't have time.  
**Speaker 0** 00:08:31:  Oh, okay. Okay. Yeah. So you would be interested to use, uh, going to that direction, but only the, a lot of time spent on that preparation and interviewing people and analyzing the key insight from that information is time consuming, but you are interested to explore that, you mean?  
**Speaker 1** 00:08:48:  Yeah, yeah. So I think we've had, we've had conversations as a team where we said we would be interested to explore, um, like qualitative AI technology, but I think some of the concerns we'd flagged, which might be interesting for you to know, um, is like basically trust in the AI to do its job. Obviously as a qualitative researcher you read people and you can change t and like is the AI sophisticated enough to do that? And then another small thing is around accents. So regional accents in the UK can be pretty crazy. Um, and what we have heard is that like AI, when picking up and synthesizing, um, like the outputs of an interview might struggle with some UK regional accents. And so that's turned us off as well being a UK company.  
**Speaker 0** 00:09:38:  Oh yeah, that's very interesting because, uh, uh, I think the, uh, the overall structure, uh, for these AI development is that, uh, the more time it'll have to learn different kind of accent and its own data set will keep improving, then slowly it'll start adapting that as well. So I think we are basically going into that direction, but yeah, I'm glad to hear that that is like one of really good road blocker for you that, uh, since you're using that and, uh, what kind of qualitative research you think, uh, your company could be doing still? Uh, are you planning to still mostly rely on online interviews or there could be some in-person interview as well?  
**Speaker 1** 00:10:19:  Mostly online interviews, to be honest. It's just found it's cheaper and easier tool.  
**Speaker 0** 00:10:25:  Right. Thanks. And, uh, what are your thoughts on sentiment analysis or other, other kind of, uh, you know, like thematic analysis, sentiment analysis, uh, what are your thoughts on that?  
**Speaker 1** 00:10:41:  Again, I think there's like a lot a slight level of skepticism. Um, but this might just be, I've not had lots of dealings with that technology and a couple I have or like hearing colleagues stories is, my sentiment is either not really nuanced at all. It's just, you know, good, bad neutral, which isn't always that helpful or seeing certain things being bucketed wrongly as well. And I think, again, I don't know if this is more of a UK problem, but 'cause UK people can often be like quite sarcastic or like the use of swearing and stuff, I think, um, yeah, I've forgotten what it was. Now there's a really funny example when I worked at Yorkshire Water, um, something being described as like a negative sentiment, but I think the customer had said like, Yorkshire Water's effing brilliant, um, which obviously is good. So \<laugh\> Yeah,  
**Speaker 0** 00:11:36:  No, yeah. Uh, but I think if we look at the bigger picture, basically when you're having a serious interview, people can still be sarcastic. But yeah, I think swearing could be still like, uh, uh, do you think people can still swear in, uh, if you're telling them, okay, this is a qualitative research entry, we are trying to collect data, we maybe people will be a bit more serious if you're defining like a lot of conditions before that?  
**Speaker 1** 00:11:59:  Yeah, yeah. Maybe it's about defining conditions. Um, but yeah, I think it's just you, when it comes to sentiment, you lose out on like a lot of nuance that human researcher could unpick. Um, but like the benefit of it obviously is the time saving of delegating things to ai. Um, so it's just, it's a balance.  
**Speaker 0** 00:12:22:  Yeah. And, uh, so, so you said that you're mostly working with quad sites so far, like analyzing and other kind of customer datas. Uh, could you please tell me a bit more about, uh, the, the, the format in which you have that data recorded or how do you usually analyze and, uh, what kind of tool you you're using?  
**Speaker 1** 00:12:43:  Uh, yeah, so we, we use two survey platforms, um, Alida on Qualtrics, so that captures all of the data, don't tend to do much analysis in there because the, um, capabilities aren't brilliant. So it would get downloaded, I think first into an Excel spreadsheet and then we'd data clean it, and then we would upload and do our analysis in queue.  
**Speaker 0** 00:13:06:  Okay. And, uh, do you get some kind of dashboard at the end from them, uh, where you will have all the output, uh, plots, numbers and figures or what, how do you, what, what kind of information do you get at the end when you say that I finished by analysis and now I'm ready to share it with us?  
**Speaker 1** 00:13:26:  Yeah, it's, um, so it's mainly just like cross tabs that we would, we would build in queue. You can, um, create chats in there, but we prefer to create our own charts like within, um, our point because we do most our presentations within PowerPoint. And yeah, I'll be honest, it's probably not the most efficient way of doing things. It's just the way that we've, we've learned to do it as a company. Yeah. And there's survey software that we use, um, for scripting. So Alida and Qualtrics, I think both of those have dashboard capabilities within the platform, but again, they're not very, um, the, the capabilities aren't great. You can't really shop and change the data to the level of detail that we want to, which is why we export it and we a analyze queue.  
**Speaker 0** 00:14:15:  Oh, okay. Uh, yeah. So you record it from them and then you, you are analyzing it in a different software.  
**Speaker 1** 00:14:22:  Yeah, yeah, because within the survey platform, Alida and Qualtrics, it's fine for getting like your headline insights, but once you want to start, sorry, I'm just gonna, sorry. I'd like to think I'd be a much better research participant being a researcher myself, but \<laugh\>, no, not, um,  
**Speaker 0** 00:15:12:  No, no, don't worry at all.  
**Speaker 1** 00:15:15:  \<laugh\>, I'm back present. Um, sorry, what was the question? Could you repeat it?  
**Speaker 0** 00:15:19:  No, yeah, we were talking that, uh, why do you, uh, move away from Alida and Qualtrics then the reasons to, uh, analyze it in a totally different platform instead?  
**Speaker 1** 00:15:30:  Yeah, so it would be, once we want to start like cutting the data to multiple different, like demographics or like behaviors or attitudes, it's just a lot easier to do that within q our analysis platform, um, to export and do it in there. So yeah, **until the analysis and like dashboard capabilities in Alida and  Qualtrics got better,** we would always, yeah, download and do more in more detailed analysis within queue. And another benefit of doing it that way is that we'll often, um, download internal data as well that we have on our customers and we'll append that to the data set. So again, we've got a, a lot more like rich data that we can analyze about, um, the respondents than if we just have the survey data captured in the platform.  
**Speaker 0** 00:16:19:  Mm-hmm. \<affirmative\>. Right, right. And uh, so you mentioned earlier like a couple of tools that you're using for different steps. So if you have to gimme a number, uh, in your whole project journey from planning to completion, like, uh, what could be the total number of tools that you're using in?  
**Speaker 1** 00:16:38:  So it would be, yeah, the survey, like script and dis distribution would be the, um, Qualtrics or leader, whichever we choose to go with. Um, and then download it to Excel data cleaning, upload it to Q or analysis, and then we would report in PowerPoint presentation format. So that's, yeah. Yeah, all comes there.  
**Speaker 0** 00:17:03:  And what, what about before when you are working with your team, when you're doing the product planning and uh, defining the nature of your project and working on setting up hypothesis, then where do you do that?  
**Speaker 1** 00:17:15:  Um, we just plan our research briefs within web documents.  
**Speaker 0** 00:17:19:  Oh, okay. And then you shared via emails or you share with the Google doc or something?  
**Speaker 1** 00:17:24:  Yeah, yeah. Um, and then the only other like platform or software we would use would be to orchestrate the, um, the survey send. We work with our CRM team and they use a platform called Workfront. Mm-Hmm. \<affirmative\>. So once we've got our research brief and our sample request, we would use the Workfront platform to that into the CM team.  
**Speaker 0** 00:17:45:  Yep. Very good. And, uh, if we zoom out a little bit, and you look at qualitative and quantitative, uh, both fields, uh, uh, where do you think AI could be a bit more beneficial? Uh, if they have to save your time or reduce the admin work and, uh, you know, trying to help you to get, uh, get to the, uh, completion stays or to get to the insights as as possible? Uh, which field you think, uh, could be, uh, could, could better, uh, profit from ai?  
**Speaker 1** 00:18:17:  Um, so I think with one AI could help us to turn out like cross like cross paths at a quicker rate for one. Like it could potentially interpret if you had your research objectives and you input them into AI and then it could, um, gather from that what kind of cross tabs you would want making and automate that process, that would be brilliant. Um, so you didn't have to build those manually. Um, and then, yeah, I guess I mean if it could do some of the data analysis as well and, and pull out like key themes or key statistics, um, that answer the research objectives, um, I think it's mainly like I can see those kind of like quicker wins on the QU side, but then with Qual I think there's almost more benefit because of how time consuming that is. Like it's probably more complex for AI to master that arena 'cause it's not just dealing with like cross tabs and, and stats.  
**Speaker 1** 00:19:17:  But if, yeah, if AI could actually run your like depth online depth interviews for you. So I could, it could be you right now, it might have a human face, it might not, maybe you are ai, um, \<laugh\>, but it could, you know, flash up prompts and allow me to respond to it and it could interpret as well, like when I'm running out of steam, when the right time is to interject, um, whether it could probe like that would be a really great, like, developed version of awake to help I guess. And then the obvious one of like synthesizing, um, the insights, which I feel like ai, AI seems to be like quite more developed at that point. That seems to be what AI tools are doing at the minute in research is it could synthesize the output of this conversation and potentially like pull out different themes, but whether it can like actually interview a person and know when to pause or probe or share the next question, I dunno whether this tool for that. Yeah.  
**Speaker 0** 00:20:19:  Um, I think we, I think in industry there could be other competitors that could go into that direction, but our initial goal is not to replace the humans, uh, because, uh, we have heard a lot of concerns about, uh, the trust, uh, about the information that you can get from ai. So I think, uh, at least for the next few years, the AI could be used as, uh, your really smart personal research assistant. Mm-Hmm. \<affirmative\>. But I think, uh, it will still take like a lot of time for industry to move to that place where you can really, uh, ask AI to, uh, interview you. Uh, because for that you need like adaptive AI that will learn a person's behavior, the interview style, and to understand on the way that, uh, how to ask more follow up question. For example, if I'm asking you a question and, uh, then ai, uh, if I'm an AI tool, then I should know that okay, the answer is not complete and I can go more detail, I can ask follow up question.  
**Speaker 0** 00:21:23:  So yeah, that, that's definitely a nice idea. But yeah, that's, I think, uh, at least from my understanding, that's still like far away. Uh, but uh, if you have to like tell me a bit more about the, these kind of ideas, uh, what could be your wishlist from ai? Uh, right now I just try to throw some technical jargon back at you, but if there is no technical problem, there is no funding issue. So, uh, in your quantitative plus qualitative work, what should be an ideal AI tool for you that could really save your time and get you insights?  
**Speaker 1** 00:21:59:  Yeah. Um, so it sounds awful 'cause I feel like most, a lot of researchers you talk to will probably say kind of what you just said, that we don't want to replace the researcher. You want to have that human conversation. Part of me agrees, um, but being in a team who's like our resource is very restricted.  
**Speaker 0** 00:22:19:  Yeah.  
**Speaker 1** 00:22:20:  Interviews is very time consuming, even parking the analysis side of it. Right? So I think the most important thing it could do would be like the synthesizing of information and pulling out key themes. Like that's the core thing that I think most research professionals are after will be a time saver. But I do think like really future thinking, if you could get to the point where AI could hold interviews, even if it's like simple interviews, maybe not complex and subject matter, like the amount of time that that could save, um, because I work on like often sensitive projects, so it might be around like people's gambling behaviors. So I can also see a benefit to that in like, there's downside where you can't build that rapport with a person, but is there an upside where people are more open because they're not talking to a human so like they feel like they can share more.  
**Speaker 1** 00:23:16:  Um, and then yeah, basically just the time saving as well. Like we often want to look at behaviors among multiple demographics of people, um, but it's a sensitive topic so we can't just do focus groups. So the amount of depth interviews that you have to go through when you're a team of two, it's just like becomes unfeasible. So I think, um, yeah, teams that resource differently will probably wanna stay hands on with that part of the process. But the teams that are strapped to resource, actually being able to delegate that element of the process too. And then we focus on the business challenge and the briefing upfront and then the sort of taking that synthesized in insights and creating a story for the business and just kind of top and tailor, um, like that would be good for us.  
**Speaker 0** 00:24:04:  Yeah. Yeah. Thank you very much. Thank you very much. Putting it, yeah, that's a very interesting perspective. I didn't think it before. Yeah, that could be very useful for smaller teams when there are a lot of constraints in terms of budget and your time, especially, uh, when you talk about these sensitive topics and people would be more comfortable in talking to an AI tool. Uh, but when, when you're talking to an AI tool that might be recording that information, that might be saving some information. So how do you, uh, but yeah, this is just a hypothetical example, but in your day-to-Day life, how, uh, in your work, how do you ensure the data privacy or the trust, uh, between those people who are giving you the data?  
**Speaker 1** 00:24:47:  Mm-Hmm. There's usually, there's always kind of verbal reassurances upfront in an interview. So I think like you would have to deliver those reassurances. I guess it would be in form and C's, but given that people don't really read T's and C's, you would have to find some way of like communicating that and offering reassurance, I guess. I'm not sure what that would look like, um, if you were, if you were dealing with ai.  
**Speaker 0** 00:25:15:  Right. Yeah. Thanks. And at the end of your project, when you're after the analysis and you're sharing it with your different colleagues, how often do you have to, uh, you know, reoptimize the final PowerPoint presentation for different kind of audience?  
**Speaker 1** 00:25:35:  Yeah, quite often to be fair. So we might have like a debrief with a strategy team, a product team, and then like a leadership team and a brand team occasionally for like, this is two projects we've done recently and there of interest to all these different areas of the business. So yeah, we often do have to make like d different of different angles  
**Speaker 0** 00:25:58:  And all that work is manual. Like you have to think that okay, this is where it's going and this is how much information could go in and mm-Hmm, \<affirmative\>, uh, do you often need like, uh, uh, reanalyze your, your results to drive different kind of metrics for them as well? Or is it always like a very common way of presenting that information over and over? You're trying to drive like information in same pattern, the similar pie charts or tables or some figures and plots?  
**Speaker 1** 00:26:29:  Um, no, we'll often have like a lot of follow up requests that maybe are quite like exploratory in nature and then we'll have to reive into the research. Um, and often we actually have to bat off some of these requests, um, and like prioritize them again from a resource perspective, we can't go after all of them.  
**Speaker 0** 00:26:48:  Mm-Hmm, \<affirmative\>. Right. Okay. Yeah. That's very good. And uh, about, uh, the use of a natural language processing, like the ability to talk to, uh, tools to give them command. Uh, have you thought about that? Do you have any experience, uh, like that?  
**Speaker 1** 00:27:05:  Uh, no. Can you, can you explain that functionality a bit more to me?  
**Speaker 0** 00:27:08:  Well, like how can we, uh, leverage this natural language processing, uh, facilities to get more insight from AI tools? Have you have any experience about that or you think it take it is something that you could use?  
**Speaker 1** 00:27:23:  Um, no, it's not something that I've had any thoughts on really. Sorry,  
**Speaker 0** 00:27:28:  \<laugh\>. No, no, no. That, that's totally all right. I'm just trying to learn that, uh, how much, uh, you have used so far and, uh, if you'd be willing to do something in the future, for example, if, uh, if I have to ask that, uh, uh, I mean, you, you have already, uh, covered a lot, uh, but uh, if you have to give your version of, uh, the future developments in, in AI that you would really like to see, that you think could, uh, help you quickly, not just for your research, quantitative and qualitative research, but also for project management. So how do you think it could help even there just to reduce your admin time,  
**Speaker 1** 00:28:07:  I guess? Yeah, so reducing the admin time is the big one. Synthesizing that insights, pulling out themes. Um, but then I guess if you wanted to go a step further, like you said with the audiences, if you could explain to the audience, to the ai, I've got these four key audiences, like please flag which insights you feel would be most relevant to each audience or Yeah, please. Like, here's their level of interest in the project, please. Um, provide like a shortened deck or like, this is the amount of time I've got with these stakeholders. For example, you know, sometimes you can get an hour in, sometimes you need to do it in 15 minutes. How do we shorten the stack? Those could all be good functions.  
**Speaker 0** 00:28:50:  Oh yeah. That's very interesting. Yeah. Thanks. Uh, do you think any ethical considerations or any concern when you hear the word AI or the AI helping you for research and do you think any visible or uh, very quicker ethical concern that might take pop up to your mind and your concerned?  
**Speaker 1** 00:29:13:  Yeah, I think there's, there's the two things. There's whether it's replacing people's jobs, so being short to position in a way where it's like a tool that allows us to have more free time to work more strategically and have more impact. So it's taken away those menial, more menial or time consuming tasks. Um, yeah, otherwise there's a fear of putting people in the industry out of work, I guess. Um, and then like probably the, the whole data breach conversation. If you are holding like personal sensitive information, where is that being stored? What are the risks? Um, and then I think for me there's, there's almost like an ethical thing societally of like, how healthy is this? Like we are just removing bit by bit human interactions from society. So like we've got their self checkouts in the supermarkets now and you know, like how as a society, what is that doing to our health? Um, yeah, but that's kind of broader, more existential question I guess.  
**Speaker 0** 00:30:16:  Yeah, that the, that's, that's an interesting point. Of course, I mean slowly, uh, the technology is coming more and more into our lives and then, uh, yeah, everything needs to be adapted accordingly. So you are doing something more useful for your time. The admin or uh, the repetitive task, it could be outsourced to machines. Mm-Hmm. \<affirmative\>. Now, I don't know, uh, how else that person could be spending their time. That's a, that's a big question that changed from person to person. Right. Thanks. Uh, uh, that was more or less everything from uh, my side. Uh, do, would you like to talk, uh, anything else you think it could be useful for this topic to know more in market research? Or do you want to share any other thoughts?  
**Speaker 1** 00:31:02:  No, I feel like we've covered everything from my perspective.  
**Speaker 0** 00:31:05:  Right. Thanks. Do you have any question for me?  
**Speaker 1** 00:31:09:  Uh, no, no questions. Just the, the practicality of uh, the incentive. But other than that,  
**Speaker 0** 00:31:15:  Yeah, uh, uh, I will send an Amazon voucher, uh, to you. Uh, but our CEO Dave is on, on holidays. It might take few days. I will submit it right away. It might take a few days.  
**Speaker 1** 00:31:28:  No worries. Alright, thanks for that,  
**Speaker 0** 00:31:30:  Your email.  
**Speaker 1** 00:31:31:  Thank you. Alright. It's been interesting to know. Be on the other side of the call. So \<laugh\>, thanks a lot and I hope some of it was helpful.  
**Speaker 0** 00:31:40:  Thank you very much. No, uh, that was definitely helpful. We are trying to address, uh, this whole product journey, starting from product planning to participant recruitment, and then we have our own video coding tool and the, uh, we are providing functionalities for synthesis. And then on the longer term we will also, uh, have a dashboard where you can, uh, optimize your PowerPoint presentation. You can download it for different kind of audience, quicker, the same place, but yeah, that's the goal.  
**Speaker 1** 00:32:13:  Okay. Well, yeah, good.

# Varun Puri 

Research Director at Kantar

## Meeting summary: To be filled 

## Transcript:

**Speaker 1** 00:00:52:  Hi. Has, can you hear me?

**Speaker 2** 00:00:58:  Hi, repeat.  
**Speaker 1** 00:01:00:  Hi. Uh, do you also want to ask any, any question or you just want to see, uh, what are we're talking?  
**Speaker 2** 00:01:11:  No, I think I'm just going to follow your, uh, lead. Hi, Varun.  
**Speaker 1** 00:01:16:  Hello. Hi, Varun. Welcome.  
**Speaker 3** 00:01:19:  Thank you. Thank you. How you, how you \<inaudible\>. Sorry. As Ida as well on the call. Thank, thank you for having me.  
**Speaker 1** 00:01:26:  Yeah, we are. Well, uh, thanks a lot. Thank you very much.  
**Speaker 2** 00:01:29:  Very good. Thank you.  
**Speaker 1** 00:01:31:  And yeah, uh, I'm gri I'm a product manager in research and insights at Beings, and Hershe is our marketing lead. And, uh, ADA is not a person. It's our, uh, note taker. Oh, right. Yeah.  
**Speaker 3** 00:01:47:  Well, I've known how to be polite to AI as well, so  
**Speaker 1** 00:01:50:  Yeah. \<laugh\>, we appreciate that \<laugh\>, so she'll be happy. So yeah, she'll be recording our, you know, uh, meeting. Are you okay with that? It, yes. Not taking purpose. And this is, uh, strictly for our internal use.  
**Speaker 3** 00:02:04:  Of course.  
**Speaker 1** 00:02:05:  Thank you very much, much. Uh, okay. Yeah. So yeah, a little bit overview of our companies that we are developing, uh, AI driven products for market research. Uh, and we are going for product launch in couple of months. So we are just trying to understand how we can make our products a little bit better to some of critical concerns that people like you can have, uh, in the, in, in the field. Right. So, uh, yeah, uh, over to you. Please explain us a little bit more about your job and, uh, what are your day-to-Day responsibilities? And then I can ask a bit more tailored questions afterward.  
**Speaker 3** 00:02:43:  Not a problem. So I am a research director at Cantor, uh, based outta London. And, um, I am mostly a quantitative researcher. I believe, uh, I'm trying to remember. Uh, the person who got in touch with me, uh, uh, mentioned that this is mostly about qualitative solutions, but he said, I might be able to give you some useful input. Um, and I have been working at market Research for about, uh, 13 years now. Um, and it's mostly FMCG, um, range of products from yogurt, dairy, to alcohol to, um, uh, to even, um, brands like L'Oreal, so personal care, uh, things like that. Um, but yeah, I think that summarizes what I do for a living.  
**Speaker 1** 00:03:39:  Oh, very good. And very interesting. Thanks. And, uh, so over the last 13 years of period, uh, have you seen, uh, and any other, uh, technologies that were like as disruptive as the AI is becoming recently and how the, uh, marketer sector is changing, uh, with these new emerging technologies?  
**Speaker 3** 00:04:02:  No, not as disruptive. I, I mean, Canta has always been using artificial intelligence for predictive modeling, um, for, um, with embedded within the systems for, uh, digital analytics and things like that, but nothing quite as disruptive as generative ai. I think that is, uh, completely changing the landscape because, um, I think people make the mistake of looking at generative AI is just another machine, but it's so much more than that. Um, you know, especially since it's built on the neural networks that are very similar to how the human brain functions. So the entire idea of being able to generate responses, meaningful responses, deductive reasoning, if, and then unreasoning very sort of almost mimicking, almost copying, uh, the way, uh, you know, human cognitive function is in itself a disruptive idea. So without a doubt, um, this is an inflection point.  
**Speaker 1** 00:05:07:  Yeah. Right. Yeah, thanks. That's very true indeed. And, uh, when you say that you have been already using it for quite some time, can you tell me a little bit more, uh, that, are you developing in-House ai, or you are very actively integrating whatever best is available in the market?  
**Speaker 3** 00:05:24:  Yeah, so kta, I think because it's among the top five, I think they've got deeper pockets as far as sort of partnering with the big AI firms. Um, so integration does take place. Uh, more recently, integration. Before this, we were, you know, partnering with different sort of earlier forms of AI technology, like facial recognition, facial pattern recognition, sentiment analysis from that perspective. Uh, but now obviously with the big ones out there with OpenAI and Microsoft, uh, and, and things like that, uh, of course there are deeper partnerships now, uh, which is leading to the integration of that technology in a lot of the solutions that we have to order. We have, we, we, we have to offer our clients. Yeah.  
**Speaker 1** 00:06:14:  Thanks. Yeah, the, you raised very interesting, your point about sentiment analysis. So I'm wondering if you can tell me a little bit more that, uh, the sentiment analysis that you are talking about. Is it like text-based sentiment analysis or facial recognition and understanding the facial impression and then trying to do some sentiment there?  
**Speaker 3** 00:06:34:  So, we've always had, uh, and this was way before generative AI became a thing, but we've always had facial coding technology. So to be able to gauge people's reactions when they're looking at an ad mm-hmm, \<affirmative\>, and to see what excites them, what makes them angry, what makes them sad, what makes them happy, uh, you know, in different parts of the ad is something that we've always, uh, had. So that's not new. I think the way it's advancing now is the combination of that technology along with the interpretation of that and the implications. That's where generative AI comes in, because before that, we had, we looked at the information and we interpreted ourselves. Now we still do that, but now, obviously with Gen ai, I think, um, it'll just enhance it further, you know, with it, being able to also read the data and, uh, give us our own conclu give, give us its conclusions, you know, especially about detailed things that might be missed by the, by the human.  
**Speaker 3** 00:07:39:  Um, so that's what it is. Now as far as sentiment analysis is concerned, like I said, like, you know, as of now, um, from a qual perspective, I'm not quite sure what is happening at kta, um, because I'm not too close with the qual team, but I can just guess what's happening, right? As far as sentiment analysis concerned, um, to be able to, uh, not only read people's emotions implicitly without them having to say anything, uh, but to also be able to look at text-based analysis where you've got, uh, a range of data, let's say, from millions of conversations online on a specific topic. And to be able to read the emotion into that, the, the, the themes that are emerging and the kind of, uh, positions that are being taken. And that's something AI can do at scale and very rapidly. So it's, I, you know, I don't think I'm saying anything new here, but then if Canata doesn't take this route, then uh, we wouldn't be in the top five. Yeah. So I have a feeling that it is going to happen, and it is happening right now. There's probably somebody behind the scenes who's trying to figure out how to, how to put it all together.  
**Speaker 1** 00:08:57:  Right. Thanks. And, uh, under your research responsibility umbrella, let's say that, are you a hundred percent focused on quantitative research or you also collaborate with Qual team, or you help and do you work on common project and  
**Speaker 3** 00:09:12:  Yes, that's right. So we do collaborate with the Qual team. Um, what we do is we, you know, uh, as, as a, as a director, what I do is I make sure that in certain projects that require depth, I, uh, bring in a qualitative element, and I'm very much involved in shaping the discussion guide, uh, to ultimately when the quality team comes and gives me their report on, um, the findings, integrating those and, and guiding the responses to those reports, but also integrating that into the QU data. So that is something, uh, I do now on the subject of sentiment analysis, I also wanna, um, uh, talk about, um, entity recognition, right? Um, to be able to, uh, recognize groups of, let's say, people or certain organizations when it comes to, um, uh, and I think there's a lot of that that's being done with our digital analytics as well. Uh, we do have a platform at the background that sort of brings it all together. Uh, when it looks at conversations, let's say we wanna understand what is the future of alcohol in, uh, in a country, and we want to sort of, uh, understand the, the, the, the longitudinal trends and the predictive through a predictive lens, then what we do is we, we capture that information, and then obviously there's an AI model in the background that is able to slot things into certain spaces for further analysis. Anyway, um, that was on sentiment analysis. Yeah.  
**Speaker 1** 00:10:49:  Very good. Yeah, thanks a lot. And, uh, if we have to talk about the integration of, uh, different AI technologies in your world, uh, can you gimme some examples that, uh, or what are the specific AI tools that you picked up from the market in the last couple of years, and what kind of impact they, they had on your research, uh, work?  
**Speaker 3** 00:11:11:  So we do have a, we don't pick up AI tools from the market. So what we, we actually use, um, a partnership, uh, to, to obviously gain access to the technology. We've got our own team that is working with the partner in order to, uh, uh, you know, create solutions, just like what you're trying to do, like product solutions, right? So we've always had a certain way of concept testing. We've always had a certain way of, um, add testing. We've, we've, we've got something called Kaya, and I'll go through that in details. But for concept testing, the advantage that we have is that we've been doing concept testing for donkeys years. We've got a, a database of millions and millions of tests that have been done over time. Yeah. Um, and we've got massive norms and databases. We've been able to, um, uh, to collect, uh, in a way the sentiment, uh, and the, the responses to each of these concepts when they were evaluated at the time.  
**Speaker 3** 00:12:21:  And that forms this almost like this reservoir, this repertoire of, uh, knowledge that goes into, let's say a concept, uh, the concept AI that we have. So now, without having to actually, um, go and collect a new sample of people using all that knowledge, we can actually predict what a concept, our concept will fare in the market. Similarly, with ad testing, right? We've got, uh, our product is called Link ai, and with it uses the same repository of sort of all the ads that Canta has tested over time. So we, it's, it's very much grounded in human responses, and it's using all those human responses to sort of predict how an ad will perform in the market without even, uh, uh, you know, recruiting or respondents \<inaudible\>. And then we've got something called Kaya. Kaya is, again, a bespoke tool, uh, for each client that essentially takes all the information and the consumer information they have, uh, and that we've done for them.  
**Speaker 3** 00:13:29:  And we've got some clients who've been with us for, you know, decades, and there's a lot of information there. Uh, and it's almost like, uh, their own, uh, chat bot tool where you can, you can type in whatever question you want about whatever brand within your portfolio, and it gives you the latest information, or it'll give you trended information, it'll give you charts, it'll give you analysis, interpretations, implications. Mm-Hmm. \<affirmative\>, we've got that too. Um, and for more information, you can just have a look at the kta website. I'm sure it's all, all over there as well. Um, what else do we have? Um, there was something else that was, you know, a recently launched product I'm trying to remember. Uh, but anyway, you get the idea, the, the, these products that, uh, have been created for this purpose. Yes. Yeah. Right.  
**Speaker 1** 00:14:18:  No, thanks, Laurie. That's very interesting. And, uh, just a quick follow up with your predictive nature of your research, when you're trying to understand how the ad could have, uh, and, uh, how, how the ad was received in the past, uh, and how, uh, the new ad might behave, uh, in, in the future. And you're using your past experience, but the past experience, uh, is based on different kind of human interactions that had very different experiences, but the new ad is going to be launched in a new environment where the human insight or the human understanding could be very different. So how do you ensure the reliability of your predictive modeling that, uh, I, I'm trying to understand the success rate that, how good it is it at predicting the future and predicting the, uh, future vibe.  
**Speaker 3** 00:15:10:  And that's a good question. Now, what we, we, we wanna make a very clear to our clients that, um, when you're looking at something like a Link AI tool that is based on predictive modeling on human behavior, now let's, whether it's past human behavior as future behavior, human behavior is human behavior. If you look at it over a period of time, there's certain things that make people happy. There's certain things that make people sad. Mm-Hmm. \<affirmative\>, there's certain peoples that make people, uh, make, make, uh, that, that elicit certain emotions that drive a certain level of likability. So yes, adss are unique, but there is a huge commonality factor with, with a lot of common emotions that are being elicited by ads. Having said that, we don't tell our clients that this is your go-to, we don't tell clients that, listen, you know, if you are going to test the massive campaign, and this campaign is really important for you, we don't tell them that.  
**Speaker 3** 00:16:06:  Do link AI instead Mm-Hmm. \<affirmative\>, we actually tell them we rec, we recommend, actually, since this is a new campaign, you need a a new sample. We, we need to make sure we get it right. If there are any outliers or unique aspects of this, we need to be able to call it out for those nuances. And so if there are certain use cases for this, yeah. Now with what we do tend to do with Link is sometimes, like, you know, the companies that we work at, whether it's the Unilevers or the DJOs or the Colgates or whatever it might be, the non, sometimes they have many versions of an ad they've got, they might have five or six creatives to test, and they can't test. It's not a, it's not easy to afford them. And so the first screening stage is when they shortlist the best ad to be tested so that they can guarantee that, you know, there's this chance of success.  
**Speaker 3** 00:16:57:  Mm-Hmm. \<affirmative\>. Um, and that's where they use it. It's not for the big strategic decisions, because that is, we wanna maintain that. Listen, at the end of the day, this is going to be the best bet that you have to give you a prediction. However, there is a margin of error. And that margin of error could be based on something very unique or a certain celebrity you might have used in this ad that is only popular now versus back in the day. And so if you want to capture those nuances and you're gonna be investing, uh, you know, over a millions on adding the ad, then you might wanna spend a few more thousands just to test it with a, with a set of respondents. If not, and if you just wanna shortlist an ad, um, from a group of five or six ads, or even three or four ads, then maybe consider link AI and you'll get a good idea of what to short list.  
**Speaker 3** 00:17:52:  Because even if it's 80% Right, that's good enough to know which ad is better than the other ad. Yeah. Right. And, and that's where all the commonalities come in, right? Because human behavior at the end of the day is, is very, it might, it might make it out to be very complex, but there's a very fixed spectrum of emotions that we've, like, we're talking about thousands and thousands of ads that have been analyzed over time. And you see very common patterns, em emerging no matter how different the ad might be. There's certain things that trigger an emotion, we capture that. So the universal truths and then their differences.  
**Speaker 1** 00:18:27:  Very good. Yeah, very well explained. Thank you. Um, so if now we zoom out a little bit and try to look at the bigger, uh, quantitative research area, uh, what are the potential limitations of, uh, AI insight that one can be concerned about? And do you really want AI to be more helpful?  
**Speaker 3** 00:18:48:  The first thing that comes to mind is data quality. Now, I don't mean like with the technology that we have right now, especially with open ai, uh, potentially, uh, GPT five, um, which is gonna be four or five times more powerful than GT four. I'm not concerned about the accuracy of its ability to do logical reasoning. Mm-Hmm. \<affirmative\>. But what I'm concerned about is the data that's the, the data that's put into it. So if you're gonna have a model and you put rubbish in, you're gonna get rubbish out. You're gonna, you're gonna prompt in a terrible way. You're not gonna get anything meaningful out of it. Yeah. And I think, I think that is the biggest concern, is not so much whether the technology is good or bad. The technology is great. Yeah. It's whether humans can actually, uh, uh, match up to it in terms of what they want from it, and what information they put in.  
**Speaker 3** 00:19:41:  You can put in, uh, you can create a bespoke model using, uh, uh, you know, GPT, um, and you can feed all the wrong information in its learning model, in which case it's gonna just give you rubbish at the end. It's not gonna give you the truth. It's gonna give you a nonsense. So rubbish and rubbish out situation. So that is a concern. And obviously there need to be strict controls on that. Um, I think, um, complexity, so when GPT first became popular, my personal story is I picked it up the moment it was launched, and I started playing around when, and I was, and this was the earlier primitive models, right? And that itself was quite impressive what it was being able to do. Now, what shocked me was I was talking about this to my, my colleagues, and this is the case everywhere, right?  
**Speaker 3** 00:20:30:  This is not only, it's not unique to, uh, some of, you know, my colleagues, it's, it's, it's, it's, it's kind of, it's quite common for a lot of people to have dismissed it. So one and a half years ago, I remember start doing it, and I was like, listen, this is gonna change the game. It, you cannot treat it like a machine. It's basically copying the way you think \<laugh\>. And if it improves in technology, uh, if it improves in, in processing, uh, if it improves in its ability to make those logical linkages, it's gonna change the field. Uh, back then they were like, oh yeah, this is like a science fiction thing today. We see what's happening, you know, there outfits like you that have, you know, obviously taken this opportunity to say, we need to make something with it. And just like you, there's so many people out there cantas on it now.  
**Speaker 3** 00:21:17:  Um, and, and, and so I think the problem is the complexity. So I, the complexity of access, the complexity of how to deal with this thing, uh, now to me, who is, I'm inclined towards it. I am, uh, as far as I'm concerned, I'm an early adopter. And so these things excite me. Uh, and so I found it easy to sort of, to, to work my way around it, to, to, to understand what prompting is, to understand what the capability is, to understand actually the capabilities are endless. Uh, but it's, it's, it's only as good as, as your imagination. And if you can really imagine, you can literally do a lot of things with this thing. Um, and, uh, I think the complexity, so any product that comes in in the market, people are too used to having, uh, information put in front of them.  
**Speaker 3** 00:22:12:  Uh, like almost like a silver SPO on a silver spoon. I think that's the, that's the problem. It's like a lot of people are gonna expect products to be created that literally do the thinking for you, rather than you actually shaping the thinking using it. Um, that is going to be a barrier to usage. So I would recommend that, you know, like, and Microsoft started doing it now, they started creating dumb products, I'd call them, that basically you don't have to do anything. It does it in the background for you anyway, and it just gives you stuff. And I think that's what is you, any product that's created will have to be simplified to that extent, where it, it reduces the level of human interaction and increases the level of auto responses to, uh, to what, what, you know, what framework you want to serve. Uh, if that makes sense.  
**Speaker 1** 00:23:03:  Yeah. No, yeah, you, you very, you raised very interesting, uh, point. So that's a very, uh, you know, very brilliant perspective. Uh, so, but, uh, there, there, there are like a lot of people who will think that, uh, machine needs a supervisor as well. Uh, I totally agree that, uh, if I'm asking dumb question, it'll gimme dumb answers. There is no question about the authenticity of that statement. Uh, but sometime, uh, uh, if I ask if, if, if I'm coming from let's say, healthcare search background or, or different background, I might ask similar question, then how can I ensure that machine is giving me a bit more contextual answer? And if it's not understanding context of my questions or the background, uh, it can still gimme very generic answer. So maybe from machine's perspective, that's like very reasonable answer. Uh, human will look at that, uh, you can straight rule out that, oh, no, that's not useful for me. It's a very helpful for a FinTech person. But yeah, as a healthcare surgeon, this is not giving me any information. So where do you see the role of human in this whole overall chain to build trust in, in a I outputs?  
**Speaker 3** 00:24:21:  I think it's about, it's about, you see, and I think it's be, it's happening to a certain extent now anyway, but I think what AI needs to, what the, any AI needs to be designed, see, because the problem is humans are funny from that perspective. You know, we'll ask a question and then we'll imagine that the machine knows exactly what we're thinking. Now, if we were speaking to another human being, and we, again, like for instance, in the research field, when the client is giving me a brief on a, on a project, if they don't give me a good brief, I will not create a good research design and they will not get a good report. Right? It's as simple as that. If they give me a detailed brief, if they tell me who the stakeholders are, if they tell me what their marketing objectives are, they tell me what their agenda is and how they want to shape, um, the outcome of their brand and their product, where they see it going, then I don't know what questions to ask.  
**Speaker 3** 00:25:13:  Now, it's very simple. Now, when, when a human is dealing with a machine, we're not used to talking to machines. And so we don't give them that detailed information. Fair enough. It's fine. You don't wanna, you don't wanna treat this thing like, you know, you are briefing it, don't have to do it, in which case AI is going to have to meet the human halfway, right? And so you create follow up prompts. So you just ask a question, it gives you an answer, but then it also gives you sub-questions as, did you mean this? Would you like to know more about this follow up questions to elaborate on what the human might have wanted as well? And I think that is the one way to get the human, just to have to click on the, the list of potential directions that they can take the conversation in to elaborate on that.  
**Speaker 3** 00:26:04:  But I think, you know, I, I, I, and that's the problem because a lot of times when I've tried to train some of my team on, um, prompting, um, it's, they ask questions as if the AI is able to read their mind and exactly what they want. And I'm like, \<laugh\> gonna be able to do that. You're not connected to it. You know, it's not being able to read your mind. So it, and that's the problem really, I think. Um, but that's what AI needs to do. And a lot of companies now are doing it. Like, you know, if you look at chat GT four oh, or you look at, um, um, I think Claude does it too. And I think, um, um, what's it called, the Microsoft one, copilot co copilot does it too, but it's, they've got a limited set of sort of follow up.  
**Speaker 3** 00:26:55:  Yeah. Questions. Would you like to know more about this? How, how about this? What is the next predictive thing? Mm-Hmm. \<affirmative\>, I think what I started doing initially was when I put in a prompt and before it had the follow up prompting questions, I used to actually say, after the prompt, I'd say, listen, after you give me the answer, I'd like to also ask, uh, you know, I'd like to also list on the po potential directions that this, this topic can go in, um, and list them down for me too. So it'll gimme the answer, and then it would sort of tell me the different directions it can take.  
**Speaker 1** 00:27:27:  Very good.  
**Speaker 3** 00:27:28:  Um, and so I knew, okay, this is what I need to explore. This is how I need to go further. That needs to be an automated process.  
**Speaker 1** 00:27:35:  Sure. Yeah. I totally agree. Yeah. Uh, very interesting. Thanks. And, uh, if, if we talk like the, if I talk about the AI applications in quant and qual research, like you are mostly in quant, but yeah, you're also dealing with qual. So where do you think, uh, you can see more useful applications of AI technology? I mean, in both side you are having some kind of your project planning, then you're recording data, you are collect after collecting, you are analyzing it in some way. You're interpreting, interpret, doing the interpretation of that data. Uh, the whole year could be very similar, but the type of data is different. Mm. So how do you think AI could be more helpful in, uh, in, in, in both of these areas and which one is more suitable?  
**Speaker 3** 00:28:23:  So if I had to just think of qual for a second, I mean, it's already there, right? We've got transcription services, automated transcription services on the spot transcription, um, which saves a lot of time. You've got analysis with, uh, you know, uh, natural language processing models like GPTs that, uh, can, uh, essentially do a lot of text mining and predictive analytics. And from a, from a, for a large, uh, text, um, or what do you call them, um, transcriptions, pick out exactly what you, you want to talk about in the key questions. You've got summarization as well, tremendous time saving, how it can take, um, a certain topic that you want to explore, summarize the content, uh, at different places in the transcript so that you have one cohesive view and one, one combined view of everything. And of course, with presentation as well, to be able to create, uh, to able to generate PowerPoints, um, a lot, you know, in that same process or to, or to generate PowerPoints that puts all the insights down.  
**Speaker 3** 00:29:31:  So all you have to do at the end is just have a quick look, see if it makes sense, make a few modifications here or there. Um, and that's it, you know? And so, you know, if you look at the, that project lifecycle, this is what I would think of doing transcription analysis, summarization presentation. How does it sort of integrate into the entire process to take something that would normally take about, um, two or three weeks? Um, just one week? Yes. Now, because the discussion guide can be created, uh, using generative ai, maybe just need some iterations at the end to, to make it perfect, um, you know, limited human interaction, uh, and time saving. The only time that it would take would be to do the field. Like, you know, let's say if you were having a focus group interview or you're going have, uh, an IDI, those are gonna take the amount of time that they do, but everything else, top and tail has shrunk. Yeah.  
**Speaker 1** 00:30:26:  Tha thank you very much. Do you mind telling the name of the tools that you're using for the various steps of these projects? Or, or even if you're not directly using it, but somebody else in your team? So,  
**Speaker 3** 00:30:36:  So we are tied up with, with, uh, with Microsoft. Mm-Hmm. \<affirmative\>. So, so what we use is we do use copilot. Uh, but that's only because, listen, the other issue is the challenges, your data privacy issues, right? Because we're dealing with such confidential data from clients. And so now because we've got that partnership with Microsoft, we know that, uh, I mean we, it's a guarantee that the, that the data is held privately. It's not open source, it's not going out anywhere. It's within the, within the, the, the KTA cloud. Um, and so, you know, and even then we take, we, we take precautions, right? We, we mask the brand names, for instance. Yeah. We don't, we don't put it out there yet. And like, I think it's because we're still, a lot of people are still because it's highly sensitive. Uh, and so just if, even though we, you know, there's an agreement and it's all private and it's all secretive, uh, and it's all, you know, privacy is insured, um, uh, you know, we still tend to mask the brand names, um, so that there is no indication at all that this belongs to a certain client.  
**Speaker 1** 00:31:50:  Okay. Yeah. Very good. Um, when you mentioned about, like, uh, you, uh, about the qualitative research, about the transcription summary presentation, data analysis, and, uh, at the end, you want to get like automated, uh, PowerPoint presentation as well. Do you already have that tool that can be, uh, where you can feed your data and, and it can spit out some PowerPoint presentation depending on the different kind of audience, or that was a wish list  
**Speaker 3** 00:32:17:  For Qual? I'm not sure. So I can't, I can't speak for Qual. I'm pretty sure they're working on it. Um, but for quant, like for instance, for the concept stuff and the link stuff, and we've got, we've got automated stuff. So, um, we've got something called Cantor Marketplace, uh, which has a whole range of solutions, and it creates a dashboard and that that dashboard has all the data beautifully, uh, laid out, and then you just download it and it creates a presentation for you.  
**Speaker 1** 00:32:46:  Uh, the dashboard, create a presentation. Can you like talk to dashboard that, okay, I'm going to talk to my research software engineers and gimme a bit more technical information. Or if you say that I'm going to talk to the management board and gimme high level information, can you have this kind of feature in your dashboard or not yet?  
**Speaker 3** 00:33:02:  Uh, uh, it's not yet. No. Mm-Hmm. \<affirmative\> that doesn't exist yet.  
**Speaker 1** 00:33:07:  Right? Thanks. Sorry, just one last question about, uh, uh, if there is like no, uh, hurdle from technical or finance point of view, uh, what, what, what would be your wishlist do you want to see from AI to solve a research at scale, uh, both for quantitative and qualitative? What are the things that you really want to see if there is no bound, but,  
**Speaker 3** 00:33:32:  Well, I'd like, I'd like more interdisciplinary collaboration. Mm-Hmm. \<affirmative\>, right? So when, you know, because we've got so many domains in Canta, and we've got so many experts and so many, um, brilliant people from different fields, um, it would be good to, to to have, uh, an AI general platform that makes it easier to tap into everybody's brains. Like, you know, if you wanna, you want to collaborate with somebody who's in shopper, you wanna collaborate with somebody who's a creative expert, you wanna collaborate with somebody who's in media and with consulting to be able to, you know, if you've got a, an AI platform that facilitates that seamlessly, that makes it also easy to build on each other's expertise and work to create something brilliant for the client, that is something that would be really good. Mm-Hmm. \<affirmative\>. Um, I think, uh, more, it's, it's, it's also about sort of, you know, whether we are actually, and I keep thinking like, like I said, I'm always, I always feel that, um, the technology that is just being launched, it takes a little bit of time for big companies to sort of get to that.  
**Speaker 3** 00:34:46:  Right? Now, I'm not saying cantas behind Cantas actually, uh, among the companies that's innovating quite fast, but I always feel a little disappointed. I'd like, you know, uh, you know, what can we do, um, to, to, to create more enhanced insights? What can we do to, to upgrade the tech so that we can sort of integrate it into everything that we do from a process perspective to a brainstorming perspective, to a meeting, uh, facilitation perspective, uh, to just, because again, I'm not looking at AI as a replacement to humans. I'm looking at it as a way to augment humans.  
**Speaker 1** 00:35:21:  Sure, sure.  
**Speaker 3** 00:35:22:  Just like it's augmented me, right? Over the last one and a half years, I have, I, I, I'm, I'm proud to say that my performance has been augmented by ai. It's not replaced me, but it's made me a better version of myself. Yeah. So, so that's how I look at it. Uh, personalized research, adaptive AI methodology methodologies, right? So when you're speaking to people, um, you want to, and you're interviewing somebody and the interview is going a certain way, but they bring up a different topic to be able to, on the spot change the questions based on that conversation so that you can, you can make the questionnaire more adaptive, uh, and actually make it more meaningful. Yes. Uh, from an insights perspective. Those are some of the things that I think would be interesting.  
**Speaker 1** 00:36:06:  Yes. I think those are the things that we also see the rise of a small language model or personal language model. Right now we are highly dominated by large language models, but slowly, when people want a bit more personalized experience, we are heading into that direction as well. Yeah.  
**Speaker 3** 00:36:21:  Okay. Very  
**Speaker 1** 00:36:23:  Good. Uh, yeah, that's, I think, uh, uh, everything from, from my side. Uh, thank you very much for your time. Do you have any question for, for me?  
**Speaker 3** 00:36:33:  So you, you mentioned, uh, you are gonna be launching, uh, products shortly. Yes. Um, and, uh, is this, uh, are you looking to launch in specific markets or is it just all over the, all over the world at the same time? Something that people just download from, from your website?  
**Speaker 1** 00:36:50:  Initially we are targeting, uh, the uk, uh, and mostly this sector. And, uh, since it's a market research tool, so, uh, currently we are not saying that this is more specific to healthcare or FinTech currently we are open, it's a minimal viable product. Uh, we are trying to do a little bit better than other tools existing in the market. Uh, we are not just providing transcript and a summary. That's definitely the part of it. But, uh, we also want to help with the, uh, things like product planning. You're starting from product, uh, what kind of things you want to plan. And once your product planning is ongoing, uh, you want to recruit participant, we will have a, a very smooth integration with the other like prolific center, uh, Athena, those kind of tools that are providing you, uh, participant. And then we pro provide, we will provide our own video recording tool.  
**Speaker 1** 00:37:42:  Uh, so you can also integrate it with the Google Meets Zoom or Microsoft team. But if you want, you can use our own video recording tool, uh, that give you the possibility of even better resolution in case, uh, better facial information is a necessary for you to run some kind of, uh, you know, facial sentiment analysis that, uh, as well. And then, uh, you will have your, once you're recording your data, then uh, we are developing a bit more automated data analysis framework. Uh, you will, uh, we are trying to get as much as possible quantitative information from qualitative, uh, research as well, so that, uh, you can turn it into some kind of numbers and plots and figures. And, uh, then at the end, uh, depending on the target audience, it can give you personalized, uh, uh, you know, presentations. So  
**Speaker 3** 00:38:34:  That thing will happen slowly. But yeah, we will launch the MVP in a couple of months. Okay. Very good. Going in. So this is, this is gonna be, so you're saying that right now in the uk you don't have an end-to-end solution. Yeah. And you wanna be the first solution? Yeah. Yeah. Okay. Well, good luck. You know, I mean, I think it's a great, uh, it's a great initiative. Uh, I'm always supportive of ai, so I hope, uh, I hope you succeed. Thank you very much, RO Yeah, we will keep you posted. Thank you. Appreciate it. Have a nice, thanks. Thank you. Thank you. Bye. See.

# Lois Harmer

Research Manager @ YouGov | Qualitative Market Researcher

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:01:  Products to improve market research and the, uh, qualitative research related experience for, for various kind of researcher, product manager, and then marketing and sales people as well. Uh, yeah. Uh, so yeah, we can start with the, you can explain, uh, a little bit about your job and then, uh, you can tell me, uh, your day-to-day responsibilities, and then we can start, uh, discussing more details.  
**Speaker 1** 00:00:28:  Yeah, absolutely. So my name's Lois. I'm a research manager in the qualitative team at Yugo based in London. So within my team there's about 25 of us, um, five based in the us about 18 of us in the uk. Um, and then five more people based in India. In terms of my day-to-Day responsibilities, I solely work on qualitative research at the moment, and I manage projects end to end. So with our team, we don't specialize in kind of any specific kind of sector research. So the research that we do is across a variety of clients, really like digital media and technology, um, charities and the public sector, um, governments and regulators, um, banks, um, a whole host of clients. Um, and my role within that is managing the projects end to end, starting with research design and developing the proposals to win the work. And then setting up research, overseeing recruitment. I often delegate recruitment of projects and then right through to the field work itself. So interviews, focus groups and that kind of thing, moderating those and then analyzing the findings and writing up the reports for the clients.  
**Speaker 0** 00:01:52:  Oh, very good. Thank you very much. I'm very interested to hear that you're basically covering almost everything \<laugh\> that, that I wanted to know. Uh, so are, are you leading a team of these 25 people? Uh, are they like reporting to you as well? Once they're going through their products?  
**Speaker 1** 00:02:08:  A couple of the, um, more junior researchers report to me, so mm-Hmm. \<affirmative\>, I manage a couple of research executives. Um, so I would say I have a mid to senior level role within, um, my team. Um, I am in the position where I'm able to bring products into the business Mm-Hmm. \<affirmative\> if I see a use for it. So I'm often on the lookout for different products that might be helpful. And we have trialed a couple of AI products this year based on connections I've, um, made on LinkedIn.  
**Speaker 0** 00:02:42:  Oh, very good. Thank you. And it's like, you are very, uh, interesting, uh, career trajectory. So o over the years, uh, what is your perspective about the evolving technologies? Like these days we are hearing more about ai, so, uh, do you see, uh, can, can you tell me a bit more about, uh, uh, your experience that how time to time a new, uh, technology or new trends are, uh, coming to market search and disrupting the field in a way? So,  
**Speaker 1** 00:03:12:  Um, yeah, my, my perspective on AI is a little bit mixed. I definitely see it as a very useful tool for a researcher in kind of streamlining the analysis process and things like that. But I don't necessarily see it as a replacement for kind of good old fashioned human, um, analysis. I see it as something that can supplement that and something that whilst the tools are getting kind of more fine tuned and things like that, I think it's still something as well that requires human oversight. Um, yeah. But in terms of my exposure to different tools, um, I've predominantly used, um, analysis tools. So one is called Survey Mind, um, and another is called ybl. And we predominantly use ybl within the team recently for ybl. That's been part of us developing our AI products within the team where essentially when and the other side of the business, so one of the quantitative survey teams runs a survey.  
**Speaker 1** 00:04:25:  We will write a particular kind of survey question with an open-ended response, and then we use all of these open-ended responses. We start kind of sell that to the client and we upload this to yabo and it splits it into kind of sentiment and themes, and then we sell access to the platform to the client along with some like analysis and reporting on the findings from the open-ended responses. So we sell that at the moment as a kind of quick version of qualitative research if they don't necessarily have the budgets for full blown qualitative research. So we do that and then we also use, we've started to use the Apple to analyze transcripts.  
**Speaker 0** 00:05:12:  Okay. Yeah. Thanks. And, uh, so what was it just for the surveys or you're using Jabber for any other kind of application as well?  
**Speaker 1** 00:05:24:  Yeah, so there's the surveys where we collect open-ended responses, and we put those into ybl to identify themes, but we also use ybl to analyze transcripts from focus groups and interviews as well. We've started to do that.  
**Speaker 0** 00:05:41:  And, uh, uh, what kind of information it's, uh, throwing back at, uh, at you, uh, once you're feeding these open-ended question or, uh, no, sorry. Yeah, before that, uh, uh, when you say that the, you are using it all of also for transcription, is it only the audio transcription or you feed it to video, uh, recordings of your meetings and then it'll give you something and what kind of output it's giving you?  
**Speaker 1** 00:06:07:  Yeah, so we can upload either a video or the, um, kind of MP free audio Mm-Hmm, \<affirmative\>. Um, and then from that it produces a kind of summary, um, within this platform on the, on the webpage. So it's not an app, it's a webpage, um, of different themes. So it produces, um, thematic analysis and also kind of summarizes the topics and content included within the transcript. Yeah. So that's for when we upload something that's Yeah. A recording of a focus group or an in-depth interview. Um, in terms of when we upload open-ended responses from a survey in Seattle that is its own kind of webpage within, um, I'm trying to think of the right word. Would it be, no, I couldn't do that actually. Basically, it has its own webpage, um, and you go by question and then it will show you the main themes and the sub themes and the prevalence of that in the data as well. You'll be able to click on the, the theme and then it will show comments that correlate with that. So it'll show a survey response that correlates with that. But then there's also a separate section called Chat with Jen, which allows you to ask the AI thing Jen, questions about the data.  
**Speaker 0** 00:07:46:  Yeah, very interesting. And, uh, when you're talking to it, uh, and, uh, can you ask it to replace the information that it has already provided as well? Or, or chatting means? Uh, trying to know more what's not covered there.  
**Speaker 1** 00:08:10:  You can build on questions. So say for example, if you ask the bot how prevalent a certain theme is within the data, um, it will give a response and then you can kind of say, what about X something else? And it kind of builds on it, and it doesn't give you information that isn't in the data. So, for example, it doesn't pull information from online to kind of contextualize the responses. If it's not in the data, it just doesn't say anything about it, if that makes sense.  
**Speaker 0** 00:08:48:  Yeah, sure. Yeah. That's the, yeah, that's totally sensible because at the end of the day, as a researcher, you are probably interested in what you can can get out of your data and not, not like random suggestions coming from its pre-training information from the web.  
**Speaker 1** 00:09:01:  Absolutely. Yeah. That element was important. And also using a tool that isn't using your data to learn. Sure. Because then you get into difficult scenarios regarding privacy. Privacy, um, especially 'cause as a business, I mean, you gov is huge and we have to jump through quite a lot of hoops in order to get a new product approved.  
**Speaker 0** 00:09:22:  Right. And okay. Yeah, that, that, that was very interesting point about you are not comfortable with the, the AI tool getting training on your data. I totally understand and agree with that. Uh, but I'm also wondering that, uh, if we follow another loophole there, that if the AI tool is, uh, learning your data, learning from your data, uh, just to get optimized with your style and, uh, to get more familiar with the field of that product, but this data is strictly under your jurisdiction, like nobody else has access to the training, whatever training this model is getting from your data will be used only for your products and no one else has access to that training. Will you be okay then, uh, for that AI tool to get better by getting training from your data  
**Speaker 1** 00:10:17:  In, in practice? Um, well, no, rather in principle, yes, but I guess for me it's how well the brand or business offering that product is able to give that rationale Yeah. For me to then escalate it when it comes to getting the product approved. Yeah. But on face value, that sounds fine. Mm-Hmm. \<affirmative\>, um, yeah.  
**Speaker 0** 00:10:43:  Right. Yeah. Thank you.  
**Speaker 1** 00:10:44:  You can see it as being beneficial.  
**Speaker 0** 00:10:47:  Yeah. Thanks. Yeah. Okay. So yeah, if the statement is as clear as possible and you can also trust it, then you can let that model to get training to give you better service as long as no one else has access and your securities. Exactly.  
**Speaker 1** 00:11:00:  And yeah, it would be helpful in the sense that I found that when we've used AI products before, if it is a kind of specialist subject mm, it, it is difficult sometimes to get the insights that you need. So the more fine tuned the product can become to a specific topic is only a good thing, um, in my view.  
**Speaker 0** 00:11:24:  Right. Yeah, I totally agree. And, uh, about the fields, about the tools that you have used for different kind of fields, because you're working with like a plenty of fields, have you come across, uh, uh, some examples where you felt that the, uh, discover the tool that you're using or in general, other AI tools available in the market are giving you much better results for a particular field and very weird results for some other field?  
**Speaker 1** 00:11:56:  Um, kind of, it's less to do with kind of sectors, but more to do with, um, the what you are feeding it, if that makes sense. So for example, with the tool that I mentioned, survey Mind, Mm-Hmm. \<affirmative\>, that was really effective at analyzing focus group transcripts, but Yabo doesn't do that very well, so you yabo analyzes transcripts where there's only two speakers. Oh. So an in-depth interview quite effectively. But the other tool that I was using was better for both. It could, it could handle analyzing, um, kind of recordings with multiple speakers or just two speakers. Fine. It was a bit more versatile in that sense. In terms of, um, kind of topic, I haven't observed huge differences, but sometimes when you are taking on more technical projects, so for example, I was working on a project where I was speaking to people that worked in marketing positions about the different platforms and applications and things they used. It didn't handle \<laugh\>, the kind of technical elements very well in terms of the official kind of names of brands and apps. Um, and also it can be a little bit difficult in the health space as well.  
**Speaker 0** 00:13:28:  Right. And thanks. And, uh, like yeah, you raised a very interesting point that Yeah. Uh, like some of them are very suitable to one type of qualitative research. And, uh, what, uh, what was the name of the third one that you suggested? That it was good for a bit of both.  
**Speaker 1** 00:13:48:  That was Survey Mind, but unfortunately, um, because we already had Yabo onboarded with my company. Okay. It was difficult to get the rationale to onboard Survey Mind. I used it at an old business earlier this year, but I couldn't get it onboarded once I moved to U gov.  
**Speaker 0** 00:14:08:  Right. Uh, thanks. And, uh, what about the participants? Uh, how do you think, uh, the, uh, the AI technologies could help you potentially, uh, with the, uh, reaching or contacting to, uh, more suitable participants for that particular research? Depending on your, you know, topic or, or the field. Have you seen any example of the use case so far?  
**Speaker 1** 00:14:38:  Um, what an AI tool supporting with recruitment?  
**Speaker 0** 00:14:41:  Yeah, yeah.  
**Speaker 1** 00:14:43:  Oh, I've not really heard about that. Um, or seen that in practice. I would be a little bit skeptical, um, in the sense of, is this tool just making people up? Um, are these actually real people? Because there's some methodologies that we have on the qualitative side, so online communities or text-based focus groups where we don't actually see the participant. So in that scenario, it would feel a little bit like we haven't done the due done, the due DI can't say the word. Yeah. Due diligence. Due diligence when it comes to recruiting. Um, but also I think we're in quite a unique position at you Govin, that we have this huge panel, so we largely recruit from that anyway. So I can't really see a use case for AI supporting with recruitment,  
**Speaker 0** 00:15:41:  Uh, use panels. That means you have your own users and cus uh, people that you can talk to.  
**Speaker 1** 00:15:46:  Yeah, there's millions of people on the, the U of panel. So we very rarely have to go anywhere else for recruitment. We occasionally use, um, recruitment agencies. Um, but that's, we only really do that when we're looking to recruit really specialist audiences. So for example, like senior lawyers Mm-Hmm. \<affirmative\>, um, or people in a very, very specific location for face-to-face research. Mm-Hmm.  
**Speaker 0** 00:16:15:  \<affirmative\>. Right. Thanks. And, uh, let's say that once you have recorded the data and then you're analyzing or doing the interpretation of that, uh, so, uh, uh, how do you do that? Can you, uh, walk me through your, that process that after the data recording? Yeah. What are your next steps and for you see any application of AI in the following steps to reduce the admin task and quickly give you the insights?  
**Speaker 1** 00:16:45:  Yeah, so we often use, um, an AI platform. So ybl in the early stages of analysis, we often have a brainstorm with the client after the field work. So that will allow, by using that and feeding it the transcripts, it allows us to have a kind of top line view of what the research is telling us to go in and discuss with the client before we then go in and do more manual research. So we would upload all of the transcripts, um, see what's coming out of it, but then separately look at the transcripts ourselves and kind of think about whether there's any gaps or if there's more detail and richness that can be brought out of the, the themes that are already being presented by the AI tools, essentially. So it's very much like a summarizing and a summarizing tool for us and a starting off point. Yeah. Um, except for in instances where I mentioned that we do the kind of quick version of qualitative research with the survey responses, and then we do rely on it quite heavily.  
**Speaker 0** 00:17:57:  Right. So for all the sentiment analysis and transcription and thematic analysis, it's always \<inaudible\>. Yeah. Right. And, uh, w when you mentioned about, uh, communicating with your clients the result of your analysis, uh, what is the, the user format of those, uh, discussions? Uh, do you just send them a PPT or PDF file or you prepare report? How do you communicate, uh, what are the common formats, how you communicate your results with them?  
**Speaker 1** 00:18:27:  Um, the PowerPoint sometimes, um, with our charity clients, it's a word report. The, the charity sector just appear to prefer a word report, um, often because they then go on to publish it. But, um, for the majority of our clients, we report findings and deep, like kind of present them to them in PowerPoint format. Mm-Hmm.  
**Speaker 0** 00:18:52:  \<affirmative\>. And, uh, after these discussions, have you ever come across into a situation where they wanted you to, uh, revisit, uh, some kind of questions or, uh, ask you to do, uh, a little bit more follow up of that research or ask that, oh, well, well we couldn't get the answer of this thing. And, uh, what are your approach then? Will you go back to your, uh, analysis summary and take help, uh, from AI to see if there is any hidden pattern or you will go back and talk to the participant again?  
**Speaker 1** 00:19:28:  Um, no, it's often trying to dig down into the data more. Um, in the first instance, I think it's unlikely we would revisit, um, speaking to a participant as normally the sample size itself has been fairly exhaustive anyway. So yeah, what I would often do is use YBL to search for specific words if a client's interested in a very specific topic and doesn't feel that we've teased it out or chat with Jen, so chat with the bot and ask them about the data as well. One good thing about that chat tool is that, um, when the bot kind of replies to you, you can click on the response and then it will show the relevant comments. So you have the direct quotes from the data. Yeah. So yeah, I do, I do sometimes use it as a tool when there's been follow ups from the client.  
**Speaker 0** 00:20:22:  Right. Yeah. Thanks. And, uh, in terms of some fear of, uh, ai, uh, either in terms of potential bias when it's giving you the information or some ethical consideration or if there is anything else they can do, uh, would you like to tell me, uh, some of the, uh, things that we should be really concerned about when we are using ai? So you have a few things to watch for.  
**Speaker 1** 00:20:51:  I think it's just a concern that the tool will make inferences that aren't necessarily there.  
**Speaker 0** 00:21:00:  Oh, okay. Right. Yeah. Like making up information by itself. That's not in the data, you mean?  
**Speaker 1** 00:21:05:  Yeah. So drawing from kind of broader context rather than from the data, I would say that's my, my main concern when it comes to, um, tools and then where the data's actually going and how it's stored and how long for, so one big selling point with the tool that we use at the moment, yabo, is that, um, the data isn't held for very long.  
**Speaker 0** 00:21:32:  Would are they offering you a plan where they can ask you for more money and keep your data for few more years as long as you need?  
**Speaker 1** 00:21:42:  Um, it's, it's, in terms of the plan we have for yabo, I'm not sure we can keep it for as long as we want, essentially. But it's more that the data that is stored on kind of, it's hard to explain. We're essentially in control of, um, how long the data is on the platform.  
**Speaker 0** 00:22:08:  Right.  
**Speaker 1** 00:22:09:  But I think it's more to do with what they're doing with the data on their side, if that makes sense in terms of storing it.  
**Speaker 0** 00:22:17:  Right. Yeah, I understand. Thanks. And, uh, in terms of, uh, let's say there is no technical problem in idealistic scenario. Uh, and, uh, what are, what, what will be your wishlist from an AI tool to help you to reach those insights quicker? To reduce your admin work and help you to see some hidden patterns? Uh, yeah. Uh, what, what are your wishlist from an, uh, very idealistic AI tool? The things that are, that are not even available yet, or something that you really want to see?  
**Speaker 1** 00:22:53:  Being able to summarize outputs from a variety of different methods. So as I said then with Yaba at the moment, it doesn't handle transcripts where there's multiple speakers very well. Mm-hmm \<affirmative\>. But it would be great even to be able to explore a transcript from an online community where there's lots of different responses to certain tasks and be able to analyze that way as well. Like at the moment, um, we're not able to do that. Um, I would also love to have the ability for it to kind of summarize a project, so not just maybe one transcript in one go, but summarizing all of the transcripts together, um, to identify any nuances between different interviews, for example, to be able to do that quite effectively would be good. Um, a kind of clean editing process as well. So I said that human oversight is quite important.  
**Speaker 1** 00:23:57:  Often trans, when it transcribes the data and then analyzes it, there's obviously spelling mistakes sometimes because it's ai being able to edit those effectively and also edit the themes that come out as well. Sometimes it doesn't pick up sentiment very well either. It thinks that a certain word is kind of inherently negative when actually within the context, the way a human would understand it's not negative. Um, oh, and also because we, with ybl, I talked about two different things in terms of the summary, but also the um, the kind of access that we have when we upload the survey responses. We do give clients access so they can go through and look at the themes and then look at the quotes as well. So being able to have a scenario where clients can actually comment on something if they find it interesting and flagging that way would be good. So essentially being able to, um, like tag certain bits of text both on the, on the researcher side and also on the client side that are particularly interesting too. So just a very interactive experience on the researcher side.  
**Speaker 0** 00:25:19:  Right. Yeah, thanks. That's very interesting. And like when you talk about tag, you are like in qualitative research method, you have your own, uh, you know, definitions of coding and tagging, but when you are talking about some tag to give access to clients to certain parts of your product, so they can look at the, you know, the overview and comment there. Mm-Hmm. \<affirmative\>. So you meant a little bit different tagging just to help them to look at the right piece of, uh, project. Right.  
**Speaker 1** 00:25:48:  Yeah. But also if they see anything as well that they think is interesting that they wanna bring to the researchers mention being able to do that as well.  
**Speaker 0** 00:25:57:  That's very interesting.  
**Speaker 1** 00:25:58:  Often we're looking as we go along, so it would be good. So we use a platform called Recollective for, um, our online communities. And when we're doing analysis in the platform and replying to people each day, if we highlight a piece of text, we can then code it and then write a comment as to why we've coded it. Um, so it allows that kind of ongoing analysis as well. So that would be good, um, within a platform like, well, like an AI tool with the transcription. Yeah.  
**Speaker 0** 00:26:31:  And, uh, if I zoom out a little bit from your search work and we look at your product management area, uh, so from that point of view, when you're managing a, some researchers and everybody's working on a different project, and then each project has multiple interviews, and those interview could be one-to-one or focus group or service. So with us all complexity, uh, where do you think AI could help you to be a better manager and uh, to efficiently talk to researcher and to get those insights quicker and in more cleaner way?  
**Speaker 1** 00:27:09:  That's a good point. Even just a clean kind of user interface ability to have kind of lots of different folders manage missions that way. Probably being able to kind of tag people in the team as well to allow for communication. Say for example, if I did code something, I could, for example, tag someone in it or, um, or a scenario where people are, I'm trying to think of the right word. Say for example, if you went on a specific folder, you could quite clearly see the members of the team that were on that project. And also even if you were going to an individual transcript, you would know who moderated that interview. That would be quite effective as well, in terms of keeping track.  
**Speaker 0** 00:28:02:  Oh, okay. Yeah. Right. Yeah, I understand now. Yeah. Thanks. Uh, and when you say that either will some possibility to also like add tags to product, like to connect different people to those products, so that in your view, uh, from management's perspective, you will see it like very clean, uh, correlation between project and interview and people and who did the last moderation on something.  
**Speaker 1** 00:28:30:  Yeah, exactly that.  
**Speaker 0** 00:28:32:  Okay. Very good. Yeah. Thanks. And, uh, is there anything else that you would like to add in this, uh, this Yeah, your, your whole work? If I didn't cover before that, where else AI could still help you?  
**Speaker 1** 00:28:56:  I think not in recruitment in the sense of finding participants, but, and we, and we do have people that schedule in, but maybe in terms of identifying availability and being able to communicate to prospective participants might be helpful because  
**Speaker 0** 00:29:16:  I like the better way for planning, uh, and finding their optimum time slots. And uh, is, is that what you're talking  
**Speaker 1** 00:29:24:  Exactly that? Yeah, because we, we do run field work across different time zones and things like that. So that would be quite it. Then kind of removing the more manual labor from scheduling. Sometimes mistakes are made across time zones, for example. Mm-Hmm. \<affirmative\>, I'm working on a project at the moment where one of the moderators is based in on the US side, but the, the participants are in the uk. Um, so scheduling can sometimes, well, there's sometimes human error in scheduling. Yeah. So that might be helpful not in finding the people, but making sure that everything goes smoothly.  
**Speaker 0** 00:30:05:  Yeah, yeah. Basically, yeah. We are, uh, I mean the tools that we are developing, we also basically want to make sure that, uh, where can you use ai, uh, to reduce the admin work, to reduce the repetitive task and to eventually empower the people so that, uh, as a researcher you can focus on what you're good at, you can focus on talking to people, you can focus on your product planning and trying to get, uh, more insights from the information that you receive at the end and not just, uh, you know, uh, admin related work. And when you say scheduling, that's like a sending emails, like manual scheduling, like appointments, yeah. Agreeing on a common time slot and Okay. Right. Thanks.  
**Speaker 1** 00:30:49:  Yeah, I can see a word in which a good tool could kind of bring together documents that we save in several different places. \<laugh\>.  
**Speaker 0** 00:31:02:  Yeah. Because as a manager you are also probably spending a lot of time when you have to find the right set of document for the right project. And the information is basically scattered, you mean?  
**Speaker 1** 00:31:12:  Yeah, essentially.  
**Speaker 0** 00:31:15:  And, uh, sorry, just last question. How do you guys manage it now? Uh,  
**Speaker 1** 00:31:20:  Yeah. So at the moment we have an Excel master sheet, but even that isn't particularly user friendly because, because it's in a secure folder, because it has personally identifiable information, it can't be a shared document, which means that if someone else is in it, we just can't do anything. Um, correct. But then it's also not linked to our calendars. So the administrator that's scheduling people in can, has the details of the participants in one document, but then has to look in our calendars in a separate place. Mm-Hmm. And then there's time zones to consider. So there's just a lot of moving parts and it'd be quite nice for it to be in a more in one centralized place.  
**Speaker 0** 00:32:09:  Yeah. And even if everything is in centralized place as a, as your job title, you will be, uh, interested in very different kind of information. Like as a researcher, you would be interested in more technical information and maybe one researcher if there is collaborating with another one on the same project, you want to basically have, uh, access to almost everything related to that product. But as a manager, maybe you don't want to get into too much detail and you want to like, uh, take a step back and look at the high level information.  
**Speaker 1** 00:32:40:  Yeah. And, and at the moment as well, I'm a bit at the mercy of people updating these documents. So on a project I'm on at the moment, um, recruitment's going a bit slower than I'd like. We are wanting to come out field next Wednesday, but we're only about halfway through at the moment. Um, and it's quite a hard to recruit audience. Um, and yeah, I don't really know what's going on unless in terms of numbers and whether I need to speak to the client and understand whether we need to change the project milestones in terms of when we deliver the report. Unless my research administrator updates the master sheet. Mm-Hmm. \<affirmative\>, I don't have a lot of oversight in that sense.  
**Speaker 0** 00:33:29:  Right.  
**Speaker 1** 00:33:30:  Or at least the ideal amount that I would, when I'm managing a project, I have to like message them to ask separate things. Sure.  
**Speaker 0** 00:33:39:  Yeah. Thanks. Okay. Sorry, just one last question. Like you already mentioned that when you're talking to your clients, you are taking your PowerPoint presentation or slides in general and, uh, with your other researcher, uh, you explore the output of a project, uh, probably together. And, uh, who else is the, you who, who, who, who's your other stakeholder in this whole, uh, post-analysis stage of a product? Uh, do you have to communicate, uh, uh, the summary or the finding of this project to somebody hiring the management chain at Hugo as well? And how do you communicate? Uh, I believe that the way of, uh, communicating could be very different depending on the person you're talking to, right?  
**Speaker 1** 00:34:26:  Yeah. Um, not hugely. Um, as project manager, I sometimes have someone else within my team who quality assures whatever I get out. Um, so my job is to delegate the analysis and reporting. I do some of it myself, and I also manage the story that we're telling to the clients overall and how we're framing, um, the reporting of our findings in terms of, yeah, the report itself, um, I would have someone quality assuring it, but it's someone usually that I work quite closely with that's slightly senior to me. So it doesn't necessarily, I don't have to report it to them essentially, or debrief it, present it to them. They will just look over it.  
**Speaker 0** 00:35:21:  Yep. Right. Thank you. And so just one note, I'm I'm sorry for, uh, the delay that we are running a little bit over. It's okay. Uh, so other than, uh, you know, survey Mind, Jabal and uh, uh, recollective, is there any other tool that you have explored, uh, for qualitative research that was providing some kind of, uh, AI enhancement?  
**Speaker 1** 00:35:50:  No, it is just those, yeah.  
**Speaker 0** 00:35:53:  Right. Okay. Oh, hang  
**Speaker 1** 00:35:55:  On. Actually, not me personally, but elsewhere in the team, let me just check my email.  
**Speaker 0** 00:36:03:  Thank you.  
**Speaker 1** 00:36:03:  So I have demoed a, um, a tool called Discover ai, if you've heard of that.  
**Speaker 0** 00:36:12:  No, not yet.  
**Speaker 1** 00:36:14:  Um, so that tool, um, it, it is less something that we would access. It's more, say for example, at the beginning of a project if we wanted to understand on a more broader cultural sense something. So for example, I worked on a project near Christmas about gifting and people's relationship with gifting and how they approach it. So if we want like a jumping off point for a project before we start research design and do qualitative research ourselves, discover AI was a tool where they go through kind of, it's a bit more semiotics, but they go through things like social media posts, blog and things like that to kind of understand, um, the cultural landscape of a topic and then they produce a report for you. So essentially I would be the client for them. Um, but that's a tool that we looked at and also a Microsoft one.  
**Speaker 0** 00:37:22:  Mm-Hmm. \<affirmative\>.  
**Speaker 1** 00:37:24:  Oh yeah. We trialed Microsoft 3 6 5 co-pilot.  
**Speaker 0** 00:37:28:  Yeah. And how was your experience?  
**Speaker 1** 00:37:32:  Um, so I didn't personally try all this. My, um, my colleague did, um, let me see what she wrote in the email. Um,  
**Speaker 1** 00:38:10:  Uh, actually it hasn't really summarized what she found from Microsoft Co-pilot. She didn't find it particularly helpful. Okay. Um, for our work. It didn't really seem to save a lot of time. Um, but yeah, some of the things would be interesting alongside the analysis is translation in sort of the languages that would be quite helpful. Right. Um, also the ability to write a kind of case study or a pen portrait kind of summarizing the person that you interviewed. Mm-Hmm. \<affirmative\>. Um, and for requests to be quite quick as well. So one of the criticisms of the platform that I was trying to onboard called Survey Mind was the, the request take about 20 minutes, which felt too long.  
**Speaker 0** 00:39:02:  So request mean, whatever, you feed it into it and it'll take, uh, 20 minute to give you some input Yeah.  
**Speaker 1** 00:39:08:  To produce the summary and the analysis.  
**Speaker 0** 00:39:10:  Okay. Right. Okay. Uh, thank you very much Louis. I really enjoyed talking to you and I'm sorry for keeping you a bit longer. Uh,  
**Speaker 1** 00:39:19:  It's okay. \<laugh\>, I do it all the time to other people, so  
**Speaker 0** 00:39:23:  \<laugh\>, \<laugh\>, do, do you have any question for me?  
**Speaker 1** 00:39:27:  Um, I don't think so. I'd be interested to, for you to keep in touch in terms of Yeah. Development of what you're doing at the moment in the way of products. Um, yeah. I also, I can't remember what the incentive was when I first got the message, um, but I just wondered how that would be delivered.  
**Speaker 0** 00:39:46:  Oh. Uh, you'll get it soon through the email. Uh, I will order an Amazon, uh, voucher and I will yeah, send it over to you. Uh, you will get it through your email, uh, very soon.  
**Speaker 1** 00:39:59:  Okay. That sounds good to me. And yeah, let me know if you have any further questions, um, about anything that I mentioned today as well.  
**Speaker 0** 00:40:06:  Thank you so much. Yeah, definitely. We will keep you in the loop and we are planning to launch our first product like MVP in September sometime. Yeah. Uh, thank you very much again.  
**Speaker 1** 00:40:17:  Great. Sounds good. Um, enjoy the rest of your day.  
**Speaker 0** 00:40:20:  Thanks, you too. Bye-Bye. Bye. Bye.

# 

# Roshni Namole

Business Analyst | Data Analyst | Data Scientist

## Meeting summary: To be filled 

## Transcript:

**Speaker 2** 00:00:07:  Hello?  
**Speaker 0** 00:00:08:  Hello? Hi, Roshni, can you hear me well?  
**Speaker 2** 00:00:11:  Yeah. Am I audible?  
**Speaker 0** 00:00:14:  Yeah, I can hear you very well, and yeah, I can see you. Thank you very much. Um, okay. Yeah, thanks a lot for your time. And, uh, my name is Ri um, I'm a product manager in the research and insight at, uh, banks. It's a startup. Uh, we are developing AI products for market research. So, uh, yeah, sorry. So Dave is not connecting. He's our CEO of the company who was a talking to you. And, uh, so today I will be alone with the, another participant called ada. It's, uh, it's a tool work by our, uh, company that will be recording the meeting and giving the transcript of the meeting at the end. Are you okay with that?  
**Speaker 2** 00:00:56:  Yeah. Can I turn off the video if you want? That would be  
**Speaker 0** 00:01:01:  No, and anything is fine as, as you feel comfortable.  
**Speaker 2** 00:01:05:  Okay. It won't be recording the video, right. Because I won't be comfortable with that.  
**Speaker 0** 00:01:09:  Okay. It'll be, yeah, it'll be recording the video. You can turn it off if you want.  
**Speaker 2** 00:01:14:  Thank you so much.  
**Speaker 0** 00:01:16:  Okay. No, no worries. Uh, right. Uh, okay. So, yeah, uh, uh, can, can, can we start a little bit, uh, uh, about your background and, uh, you can explain, uh, what kind of, uh, work you do and, uh, what are your day-to-day responsibilities? And then I can start more questions about, uh, uh, to know more about the use of AI at various steps, and if we can get some, uh, some of your sessions, how that could be used for market research field.  
**Speaker 2** 00:01:50:  Yeah, sure. So, hello, I'm sni and I'm pursuing Masters in Business Analytics from the University of Nottingham. And I have worked as a research assistant in the university. Uh, and the project was related to the supply chain mapping and traceability. And I'm also working as a data science intern at Zytech. So, yeah, that's pretty much about me.  
**Speaker 0** 00:02:17:  Oh, thanks a lot. Uh, so yeah, I've looked at you. You have very strong foundation, not just in data analytics, but also the business analysis side as well. Uh, can you give like, uh, some specific, uh, examples where you have effectively translated the data insights into, uh, some more actionable, uh, business recommendations?  
**Speaker 2** 00:02:42:  Yeah, so I have done this, I have done a lot of different courseworks related to customer side. And one such project is customer segmentation, where I had to deal with complex data sets, combine those data sets to the pre-processing, data, cleaning, and then performing data analysis on that. And, uh, I've also used, there is this method called PCA for, uh, segregating the data and seeing which features are contributing most to the data. And after that, I decided to go with the customer segmentation and formed five to six segments and, uh, like decided the strategies to, um, like tailored marketing strategies to target those customer segments. So that was my, uh, project that I did.  
**Speaker 0** 00:03:36:  Very interesting. Thanks. And, uh, do, do you experience of, uh, of working, uh, in this data analysis or business analysis side, but, uh, uh, within market research domain? Or, or, yeah. Could you please explain a little bit more about how you were involved in market research as a, as a data analyst?  
**Speaker 2** 00:04:00:  Sure. So the project that I did in supply chain mapping and traceability, so it was, uh, building the database. And for that I had to do a lot of market research for the artificial turf. And, uh, the first thing that I could do was to target the main key players of the, uh, of the industry or the key players, uh, related to the artificial turf. The first thing that I did was to go through each websites and see which other key players and how they are, uh, like going through the articles which showcase the top five players or the top six or 10, uh, top players in the artificial health industry. And, uh, I, after that, I checked each website related to those particular supplier. For example, my, uh, my, my topic was related to artificial turf, so I decided to see which are the companies.  
**Speaker 2** 00:04:58:  So for example, there was this, uh, one particular company, which is very famous in China. So I went to, to that web of their website, checked if they provide more of the information, but, uh, the supply related information was not present on that particular website. So there I decided to go and research more on different papers that are talking about that company, who are the suppliers and who are supplying the chemicals that I was looking into. And, uh, in the process, I also utilized the AI because I really wanted to see what is the company has and there are these specific brands that the company has. So I'm not re I, I can't recollect one company. So there, yeah, there was like this, uh, one particular famous company and it has a lot of different brands, specific brands. So to know which brands that company has, I have used ai, and, uh, it has given me those brands, and then I just cross-checked it on the internet, if that's true, if they have those particular brands. And, uh, where is their headquarters. It was a part of building the database and conducting a thorough market research. And, you know, after that I could, uh, conduct the data analysis with the data collected.  
**Speaker 0** 00:06:17:  Yeah, very, very good. Thanks a lot. Yeah, that's very interesting. So soon my, uh, sorry, I forgot to mention before that my background is also from data science computational modeling and simulations. So please feel free to dig into more technical detail if, uh, you feel like at any point I would very happy to have that conversation as well. Sure. And, uh, so w when you talk about AI and, uh, that you have, uh, checked their websites. Yeah. Have you, uh, did, did you manually go to check their website one by one and read the information of your choice? Or have you, uh, written a piece of code or, uh, you used some AI tool to scan those website to find useful words that you were looking for?  
**Speaker 2** 00:07:04:  So there's web, uh, web scraping, uh, and I didn't do that because my, uh, the artificial turf is not that like very great industry. It has very less contribution, but the chemical industries involved, uh, as a supplier that has a great GDP. And for that I needed web scraping. So what I did, uh, I scraped out all the different companies, uh, using the words like chemicals. And, uh, there was this, uh, nylon fibers and, uh, different chemicals like pp, which is polypropylene polyethylene. These are the suppliers. So I traced down these particular suppliers and, uh, made a different database for these particular, like the suppliers of ppp, the suppliers of nylon, the suppliers of, uh, polyethylene, et cetera. And then, uh, after that I also used Bloomberg, which is also a website where you can find most of the relations to the companies and their suppliers related to a specific domain.  
**Speaker 2** 00:08:10:  So for the, uh, there I went with the chemical industries, shortlisted it, and then I traced it down from the suppliers, from tier one suppliers till they were like tier seven suppliers. And then reaching out to see who their customers are. And there I found that the artificial tough industries, which I had made a different database, some of the companies were present in those customers, and that's how I traced down the whole path. Yeah. And within that, I also used a lot of different AI tools, mostly Chad G. But I think that information was not that, uh, latest information because some of the companies have merged, some of the companies have, like, there is Dew Point, which is like very famous company has partnered with Dow Company, and these are the major key players in chemical industry as well as in artificial turf. And, uh, the information related to those companies were not that latest. So I had to go through different websites and different annual reports to check if they're still in partnership.  
**Speaker 0** 00:09:16:  Yeah. Okay. Very, very good. Thanks. And whenever you used, uh, ai, uh, have you tried to manually crosscheck the authenticity of the information that you are getting there?  
**Speaker 2** 00:09:27:  Yeah, most of the time when it's related to figures, mostly I have found that AI is very well when, when I have to explain my code, what I do is I do my analysis and then I just take the code or the results and paste it in the charge g pity and ask charge g pity to put it into proper or professional word or in a report way. And it writes very well, giving all the results, but doing the same, um, no, or asking about figures or the year, et cetera. At that point, I think it gives me, you know, a bit of, uh, um, not the updated information. So for that I need to cross crosscheck it. Otherwise it just, uh, gives me, uh, proper reporting, report writing, uh, format. I think it just writes it very well with showcasing proper words and giving my results a good professional way.  
**Speaker 0** 00:10:22:  Yeah. Very good, thanks. And, uh, if we talk about a little bit broader overview of AI in market research. Yeah. So what are your thoughts on the current application of AI for various kind of, uh, uh, tasks in the market research? And, uh, can, can you list like some of the tools? I mean, you have mentioned Chad, GPT, but if there is anything else, what, what is like the total number of tools that you have used and, uh, how effective they were?  
**Speaker 2** 00:10:51:  Yeah, so I've been using different, uh, tools like I have for data science, I'm using Black Box, and it is mostly to check if the code is correct or if I'm making any mistake, because there are sometimes very, uh, silly mistakes that I do while coding. So for that, I, uh, mostly go with the black box and it gives me correct code. And, uh, the other, um, there is also hyper ai. I'm not, uh, yeah, sure. But yeah, that is one more which gives me good references. So, uh, I got to know it from my friend, so whenever I put a topic or I want to know or research something related to a topic, I put that into that particular ai and it gives me amazing references relevant to the topic which Chad G doesn't support because Chad G end up giving me, uh, the links that are not valid or I just can't open those links.  
**Speaker 0** 00:11:52:  So, so what, uh, at this hyper ai, is it still giving you the links as to support your, uh, I mean, support the data that is passing you or, uh, just writing everything there and they're saying that this is the source? Do you links to crosscheck?  
**Speaker 2** 00:12:11:  Yeah, it gives the links. It just give, uh, suggests you the papers that I can look around related to my topic. For example, I am looking for customer segmentation related research papers, so I'll just ask hyper AI to give me the, um, information related to that. So it'll provide less information compared to Chad G, but it'll be authentic because it'll be providing all the different links or the references of those papers that it, it has fetched the data or the information from Yeah.  
**Speaker 0** 00:12:41:  Uh, yeah, that, that's very interesting. Thanks. And, uh, if we have like, integrate more AI technology into qualitative research, have you thought about that or have you any experience of using AI in a qualitative term as well? And uh, and in terms of integration of AI tools, particularly for that, uh, side of research, do you see any potential challenges or pitfalls?  
**Speaker 2** 00:13:11:  So can you elaborate a bit more on the, like what kind of qualitative research?  
**Speaker 0** 00:13:15:  So, you know, like for market research, people can have like either qualitative research where they can have one-to-one discussion with people. They can, uh, try to analyze surveys, they can organize a focus group discussions, or they can have a more quantitative discussions where they will do data analysis, that they will read information about the customer experience, uh, from the websites or from the cookies. Like all the information website are recording. So in which area were you mostly involved? Yes, mostly quantitative or qualitative or a mix of both. And how do you think the AI integration in those research area could be challenging? Yeah,  
**Speaker 2** 00:13:56:  Okay. So I have, uh, worked with mix of both, but I would say Chad, GPD or different ais that I've used so far were good in the quantitative, uh, part of the things, but they were not that good in the qualitative part because most of the qualitative part needed human judgment. And I don't think the AI was giving me accurate information or accurate judgments or references related to that. So I've mostly used it for the quantitative part because I think, as I told you before, that, uh, if I'm putting a code or the results, it'll be very good in ex explaining those results rather than giving me, um, you know, a suggestion or something different rather than a qualitative way. So, yeah.  
**Speaker 0** 00:14:41:  Right. Thanks. And, uh, when, when you said that you have done some data analysis for market research, so, uh, could you please walk me through the whole project that, uh, how did you come up with the idea to do that project? Or, or how did you decide that what way you want to go to record your data? And, uh, how did you analyze it and uh, how did you do the data interpretation at the end to finally share your observation?  
**Speaker 2** 00:15:12:  Yeah, sure. So, uh, my task was collecting the data. So it was starting from the research questions, what kind of data I'm looking. So the first question was what kind of suppliers or the companies that I am concerned with? And for that, I just Googled it, like what are the top artificial companies in, I was in charge to look at the California region, but then I had to go through the global value chain and then trace it down to the California from global value chain to USA to California. So then I just went with the global players, and this was my first question, to make a list of all the global yeah, uh, players involved in artificial turf manufacturer or suppliers. And after that, uh, I went and, uh, shortlisted the USA ones and the California ones. And there were different factors while making these, uh, questions like what goes into the artificial term because the materials are also important.  
**Speaker 2** 00:16:17:  So then I traced down or started making databases for different suppliers involved in those particular materials. For example, as I said, about the probably properly. So that is one of the material which is harmful, and I was in charge of, so I looked at those suppliers, again, the global players, the players in the USA and then California. So again, same, I built the database, and most of the time I have relied on the information that is present on the website of those particular companies rather than the articles or, uh, ais because I believe the information from the websites of those particular companies is more authentic and up to date rather than the research papers or the different ais that have been used. And one such example is, uh, knowing about the partnership, which I've shared already, and the year of, uh, sorry, the headquarters of different companies.  
**Speaker 2** 00:17:18:  So I, I was also looking at the headquarters because for one company there was like, CC Grass is like the most important company, and it had, uh, almost one 60 different, uh, presence in different countries or regions. So I, I was just focusing on the country, so, uh, sorry, the main company. Yes. So I was, uh, then I decided to focus on the headquarter of that particular company. And for that I just used Chad to just give me the headquarters of this particular companies. And some of the information was not accurate, but I had to recheck it again. Yeah. And, uh, I also used various, uh, because it was mostly about building the network from the suppliers to the customers and the companies. So, uh, for that I used the network analysis, and for that I also needed to use AI for most of my tasks. And I was just putting out in words that I need this particular connection, or I want this particular flow of the diagram. So yeah.  
**Speaker 0** 00:18:23:  Uh, uh, \<inaudible\>, thank you very much. And this diagram and the, the end result for data visualization. Yeah. Uh, have you created, uh, those vis visualization by yourself or you, you got it from ai?  
**Speaker 2** 00:18:38:  So, uh, I mostly try to plot it by myself, but uh, whenever I get the results, not according to, uh, like not according or appealing to the graphical representation, I decide to make it better with the help of ai. So with the ai, I could just, you know, use different palettes or different format or a different chart type, and, uh, AI comes up with different suggestions to plot one particular diagram in different ways, so then I can just see how those different diagrams are and which are appealing or, you know, easy to understand. Yeah. And then I could select the one  
**Speaker 0** 00:19:16:  Very, very interesting. Uh, and were, were you getting those visual visualization, I mean, before going to ai, uh, were you getting it with the Tableau or Power bi, or what were the tools that you were using?  
**Speaker 2** 00:19:28:  So, yeah, so mostly I've used, uh, Python, but I've also used SQL. Yeah. So for, uh, for SQL, I've mostly, uh, used for the error because there are very silly errors when you're doing the subquery, et cetera, and that, so for that, I have used a AI to know what kind of error is there. And even with the help of the, uh, for, sorry, for, uh, while using Python, most of the visualization I've done using Python as well as Tableau. So I have asked Chad G, and sometimes I just ask Chad G like, I want this particular graph with this particular features. So it just tells me or gives me the step how I can put the rows and columns are there in the Tableau, right? So how I'm supposed to put which feature over there and how I can make a donut chart, et cetera, on Tableau, as well as in Python, I mostly use it to modify the code, but sometimes I just ask, uh, the AI to just give me this particular, uh, chart or this particular thing I'm more interested into, and I just need a visual representation of this information, and it just gives me the Python code related to that.  
**Speaker 0** 00:20:43:  Right. Yeah. Thanks. So when, when you say that you have used AI to get rid of errors in SQL or to get better charts and descriptions, so if I understood correctly, it's, is it always charge g PT or there is any other AI tool that you've been also experimenting?  
**Speaker 2** 00:20:57:  So it's mostly charge G pt, but I'm also using, as I said, uh, black box. It is mostly, uh, for the codes itself. Yeah. And, uh, there's one more, which is a different ai, I think it is, uh, WRTN ai. I don't know how I got to know about this, but it is a good ai, it is in Korean, but it translates into English as well. Okay. So, yeah. But it is also a good AI and it provides, uh, references as well as, uh, authentic information when in comparison to t  
**Speaker 0** 00:21:32:  So just like hyper ai, uh, with more references.  
**Speaker 2** 00:21:36:  Yeah. And, uh, in hyper ai there is a word limit. Uh, I think it is same with the copilot as well, Microsoft copilot. Yeah. Also have word limit. So, but this, uh, this, uh, WRWR just a second, I just crosscheck, it's wtn ai, this particular AI doesn't have that kind of word limit. Even charge G doesn't have that much of word limit, so it becomes easy to put the right code or the report and just to make them into professionally, uh, coherent.  
**Speaker 0** 00:22:11:  Yeah. Thank you. Yeah, that's very interesting. And, uh, uh, yeah, you have already explained a little bit about your view on qualitative data. So I'm, I'm just, uh, cross-checking now that, have you experimented with any AI powered tool for qualitative data analysis, or not yet?  
**Speaker 2** 00:22:32:  Uh, I have, and, uh, I think I, I, I could not understand what exactly, I don't know, after the update of the chat g pt, for example, the authenticity or the working of the AI has been not that well. And, uh, for qualitative, I have used it, but I was not happy with the result and, uh, I decided to write it by myself. Yeah.  
**Speaker 0** 00:23:01:  Yeah. Uh, sorry, can, can I ask a few more question there just as a follow up that, yeah. So, so you were not happy, but let's say in the very idealistic scenario where you have all the technology working, give, trying to give you whatever you want, there is no other, uh, you know, resistance from any other point of view. So in that idealistic scenario, what do you want that AI to do for qualitative data analysis and how would you trust it? What are your deciding factors there? How can it make you happy?  
**Speaker 2** 00:23:36:  Yeah, so, uh, as it is performing very well in quantitative, if I'm giving that particular result or that particular code, it is giving me a proper result. And I know, because I know the result that there is 80, like 98% of the customers are female, and that is in my result, I can see through the visualization that that is in my result. And if the chat, if it is not giving me that, so I, I would know that okay, it is not authentic. And in case of qualitative data as well, I would be, you know, cross-checking it. And I want that authenticity or that credibility of the, uh, information that charge G is providing. I hope that answers your question.  
**Speaker 0** 00:24:20:  No, yeah, that, that, that, that answered it. Thanks. And, uh, what kind of output you want to see usually at the end of a quant qualitative data analysis. I mean, I understand for quantitative, you mentioned the different kind of, you know, figures, plots, and you can play with the data representation with ai. Yeah. But qualitative, uh, people usually go for automated transcript. Uh, and then everybody has their different opinion that what could be more important information from qualitative analysis. So what would be your, uh, favorite set of information that you would like to see at the end of, uh, qualitative data analysis?  
**Speaker 2** 00:25:00:  Uh, I was, I would want to see the results in a more humanized way as possible, because it is very, because the correlative data is like the non numerical data, right? Yeah. Yeah. And there, we, we need some human touch to that to understand the, for example, the individuals or the social reality, et cetera. So that, so that if the information is a bit humanized, I think that would be a very good thing about the qualitative part.  
**Speaker 0** 00:25:33:  Right. Thanks. So yeah, just to cross check, if I understood correctly, uh, so humanized means, uh, in whatever context you are having that qualitative analysis, so it shouldn't give you like a random words, but a bit more relatable information to to, to that particular field.  
**Speaker 2** 00:25:55:  Yeah.  
**Speaker 0** 00:25:56:  I I is this your definition of humanized dve or  
**Speaker 2** 00:25:59:  Yeah, it is, it should be relevant to the part that I'm talking about, because most of the time it just gives you the information and that is quite irrelevant to the part we are no wanting the information. Mm-Hmm. \<affirmative\>. So, so that, uh, is the thing. So I'm looking more for relevance to the topic, relevance to the, uh, data rather than just giving me overview of things.  
**Speaker 0** 00:26:28:  Yeah. Very, very, very good. Yeah. That's, uh, very interesting. Uh, so how do, do, do you think, how would you approach using AI to, and to enhance that personalization of a research experience? So basically we are creating more humanized way. I mean, this is what we want to see at the end. Yeah. So what, what, what do you think, what, what can we do there to improve it more? Uh, I think you've already mentioned one way was, uh, to use something like hyper API, uh, hyper AI or WT and AI where you can get some references.  
**Speaker 2** 00:27:06:  Yeah.  
**Speaker 0** 00:27:06:  And I think that can add one layer of trust, but is there anything else that we are missing to make it a bit more humanized?  
**Speaker 2** 00:27:14:  Yeah. So sometimes what happen is whenever I'm as telling or writing something and I wanted to, um, make it relevant as well as a bit, you know, personalized related, for example, most of the cover letters that I try and I write it, and then I want, um, the charge GPT to go through it and make it a bit, you know, personalized, even uploading my cv, it doesn't give me a very personalized one or the tailored CV or the tailored information related to my specific, you know, search. And that is, uh, I think that is not good because I have to, then I don't think I can use ity for making the colleges right. So even after providing my, um, resume, it doesn't make it to the tailored one. It still ends up giving you very basic or general information or just highlighting the general things about the company.  
**Speaker 2** 00:28:12:  So I think that is something which is very, um, difficult for a student like me. And, uh, uh, the other thing is about the links. So, uh, whenever I am asking, so most of the time I am, uh, giving the papers and, uh, I'm asking the AI to put it in a Harvard style, et cetera, and it ends up writing wrong information or the year, and I'm looking at that particular same paper, which has, for example, the year on that particular paper is 2023, but I don't know why the AI just ends up giving me 2021\. And it is okay, it is very, uh, incorrect information just right in front of my site. I believe it should be more accurate, right. And relevant, and it should give more tailored responses to the individual who operating it.  
**Speaker 0** 00:29:08:  Right. Yeah. Thanks. Yeah, very interesting. And so in, in, in very idealistic scenario, let's say the, this is like one of the last question now. Uh, how do you think AI will change the role of, uh, uh, you know, the researchers in market research in the next few years? How, how, how, how do you think, I mean, we have seen a lot of examples of the use of chat GPT and the automated transcripts. Is there anything else that you, where you would like to see AI to be taking more front seat and, uh, reducing, uh, the repetitive task and quickly helping the researcher to get the work done? Or is there anything else that we are currently missing but you would like to see?  
**Speaker 2** 00:29:55:  Um, okay, so I think, um, I really need to think about this. Just give me some time.  
**Speaker 0** 00:30:04:  No, no worries. Yeah, take your time.  
**Speaker 2** 00:30:09:  Yeah. So, uh, I think it would be good that the AI would come up with the questions. Like, for example, when I started my research for the artificial turf, I had to frame the questions and see, okay, these are the questions and this is the flow that I can, you know, go through. There should be a structure. So if the AI can give you a structure to follow about how you can conduct a market, it, it gives you, but in a very general way.  
**Speaker 0** 00:30:39:  Yeah.  
**Speaker 2** 00:30:40:  Yeah. So that general way doesn't apply to whatever you are searching at least while you're researching. And so it, it would be good if it gives you a proper structure and how you can, uh, start with one particular question and then go, go into another question and then so on. So that structure would be a good thing. And, um, I think the updated and relevant information, there is no updated information available. As I said earlier, that the website information was supposed to be more authentic rather than the AI information. And that would be a good thing so that it provides you the updated information about the website rather than just providing the information.  
**Speaker 0** 00:31:28:  Yeah. Interesting. Yeah. Thanks. Thanks a lot. Yeah, that's very good. Uh, I, I really, yeah, appreciate it. All the information that you're giving. Uh, just, uh, one, one of the last question about your data visualization tool. Yeah. Like, do you think that, uh, I mean, once you're using Tableau or any other data visualization tool, do you think that the existing, uh, AI tools like Chat, GPT, hyper AI or WT and AI can give you same level of visualization, uh, functionalities as you can find in W or Power bi just with within natural language processing and and feeding some information? Do you think we can get like same kind of sophistication? Have you tried that?  
**Speaker 2** 00:32:16:  Yeah, and I think no, because for example, power BI and Tableau are more specific to the visualization and the AI doesn't end up giving you that particular, uh, you know, good results. They might end up giving you suggestions and the suggestions are really good, but not the proper kind of visualization that we see. And it is not just the visualization, right? It is with all the different, uh, options that we get. For example, in Tableau, we get different options to try and drag and drop different features and to see different results, right. So I think that would be totally different for the Theis as well as for the specific softwares that are for just visualization.  
**Speaker 0** 00:33:04:  Right. Okay. Uh, thank you very much. Yeah. That was more or less, uh, all of my question. And now I will just, uh, remove Ida from the call in case, uh, you want to discuss anything that you don't want to talk on record. So I will remove it now. Okay.

# 

# 

# Jane Bujakowski

Senior Consumer Player Insights Manager at King 

## Meeting summary: To be filled 

## Transcript:

**Speaker 1** 00:01:01:  Hello.  
**Speaker 2** 00:01:05:  Sorry.  
**Speaker 1** 00:01:06:  Okay, no worries.  
**Speaker 2** 00:01:17:  Sorry, my phone was connected to my headphones and I couldn't hear anything.  
**Speaker 1** 00:01:22:  Can you hear me now?  
**Speaker 2** 00:01:25:  Yes, I can. Thank you.  
**Speaker 1** 00:01:28:  So it seems like you, you, your video and voice is like really, uh, not synced, like when you're talking.  
**Speaker 2** 00:01:35:  Not sure I can think about that. \<laugh\>.  
**Speaker 1** 00:01:38:  So do you want to reconnect again?  
**Speaker 2** 00:01:41:  I can try, yeah.  
**Speaker 1** 00:01:42:  Yeah, please. Thanks. Hello? Oh, yeah, this is much better. Okay. Hi. Thank you very much for connecting. Uh, I'm Gure, I'm a product manager in the research and insights at, uh, at a startup called bs. Uh, we are developing a AI product for market research and, uh, CO Dave, he will not be joining today. Uh, he has another meeting and, uh, we have another participant chair called ada. Uh, it's a tool developed by our company that will be recording the meeting and it'll give transcript at the end. So it is just for the note taking purpose. Are you fine with that? Yeah,  
**Speaker 2** 00:02:35:  That's fine. Thank you.  
**Speaker 1** 00:02:36:  Okay. Right. Thanks a lot. So, yeah, uh, we can start with the, you, you can explain a little bit more about your job, that what kind of job are you doing now, and, uh, what are your day-to-day responsibilities. And, uh, then I can start asking, uh, uh, more tailored questions about the next steps, please.  
**Speaker 2** 00:02:55:  Yeah, sure. So, um, I've just started a new job. Um, so, so, uh, it's a bit CBC, um, but, um, I work in Insights team. Um, uh, it's for gaming, um, uh, and basically we employ other research agencies to conduct the research in terms of writing the questionnaires and finding the people, um, and giving us a report. And then it's up to us to sort of action that, um, and, uh, go out to the stakeholders within the business and answer their questions essentially.  
**Speaker 1** 00:03:39:  Right. Thanks. And, uh, in, in, in your current job, but yeah, firstly, congratulations on your new job. Thank you. And, and this job or, or the recent one, uh, have you been using, uh, AI tools to, to for, for your  
**Speaker 2** 00:03:55:  Work? Um, not hugely actually in my, so my last job was in a research agency, um, and they were, they created an AI sector of the, of the business. Um, so that was just sort of getting going as I, as I left. Um, so they did sort of, um, some AI stuff for projects in terms of, uh, social media, um, understanding what people were talking about for different brands and that sort of thing. Um, and I know in my current job there is a desire to use more, but I don't think it's used that much currently.  
**Speaker 1** 00:04:37:  And, and for that social media listening, uh, were they already like, uh, getting data from different kind of website or the, the website that are storing cookies information and uh, and you don't have to talk to anybody to get that information? Or how, how were you recording the data?  
**Speaker 2** 00:04:55:  Um, so it wasn't me that was, was doing it, so I couldn't say specifically, but I know we spoke to people. It was, um, via tools, I think.  
**Speaker 1** 00:05:04:  Right, okay. Thanks. And, uh, and, and at your current job, uh, uh, are, what, what's the nature of, uh, the work? Is it like mostly qualitative research, uh, job or you will be doing quantitative part as well? Yeah,  
**Speaker 2** 00:05:19:  Quant,  
**Speaker 1** 00:05:21:  Uh, sorry, can you repeat again?  
**Speaker 2** 00:05:23:  Quant, it was mainly quant.  
**Speaker 1** 00:05:27:  Quantitative You mean  
**Speaker 2** 00:05:28:  Quantitative? Yeah,  
**Speaker 1** 00:05:30:  Sorry. Okay. Yeah, again, I I your voice is coming after, oh, sorry. Started to delay. Uh, no worries. Uh, okay. So for, okay, I'm just trying to understand. I can, the previous job, uh, uh, your company, they started to explore ai Mm-Hmm, \<affirmative\>, but were not, uh, directly involved their use meant, no. And can, can you explain a little bit more about the nature of your job there? What was your responsibilities there and, uh,  
**Speaker 2** 00:06:01:  Yeah, sure. So, um, in the agency I was sort of responsible for running projects for clients. So, um, we have a project that will be trying to answer a specific question, for example. Um, and I would have a small team on that project. We would write a questionnaire, then we would script that, so put it into an online survey, um, and then we would find the right people to answer it using panel suppliers. Uh, then we would process the data, um, taking it from raw data into, um, Excel or of the software analysis tools that we used. Um, and then we would create a report and present it back. That was kind of the lifecycle of a, of a project there. And we would do that 6, 7, 8 projects at a time.  
**Speaker 1** 00:06:53:  Oh wow. And, uh, how many people were like working on a single project?  
**Speaker 2** 00:06:59:  Uh, a team was usually two or three,  
**Speaker 1** 00:07:01:  Two or three people on the same project. And you were basically managing the whole seven eight project at the same time. Yeah,  
**Speaker 1** 00:07:09:  Yeah. That seems very complex and very interesting. So how, how, how are you managing that? If you have to walk me through the, your whole product journey. Like in the beginning you're just thinking about a product, then you're planning and then you're deciding that what kind of data you have to collect, what kind of people you have to talk to and uh, how to provide the resources, uh, to your team. So, mm-Hmm, \<affirmative\>. So were you using like a different tools for all of these different steps or how much of this was automated? And can, can you please walk me through your  
**Speaker 2** 00:07:40:  Yeah, nothing is, nothing's automated. Um, so in terms of sort of designing the questions that we asked, that's all manual from the team. Um, we then, uh, as I say, work with panel suppliers who find the right people, um, for us to, uh, speak to. So say if we only wanted to speak to people between the ages of 2030, then they would find those people, um, and then we would sort of manage the field work and check that we're getting the right people and the right number. Um, and then we would get the data back from them in sort of a raw format. Um, and as I say, we turned that into sort of tables, so it was the right, uh, sort of codes and different cross breaks that had different populations in. Um, and then the creation of the report was manual as well.  
**Speaker 1** 00:08:34:  Hmm, right. Interesting. And, uh, the nature of those project was also mostly quantitative or that there, there was a bit of qualitative research involved as well.  
**Speaker 2** 00:08:44:  Sometimes they were mixed, but my role was quantitative.  
**Speaker 1** 00:08:48:  Okay. Okay. So yeah, the, for the overall project completion, there were parts of qualitative as well, but you were mostly focusing on quantitative side.  
**Speaker 2** 00:08:58:  Yes, yes. Some were solely quantitative, but some had quality as well.  
**Speaker 1** 00:09:03:  Right. Thanks. Yeah, that's very interesting. And if for let, let's see if we have to do out to the broader picture. Uh, where do you see AI could have helped you? Uh, I, I mean also product management side plus also quantitative research side, like hands-on session. Do you see that, uh, there are some areas where you could have benefited from ai?  
**Speaker 2** 00:09:27:  Yeah, definitely. I think in the sort of the questionnaire development stage, it would be very useful, um, useful. Um, so if I give you an example, if we were working with a supermarket, it's probably a bad example 'cause everyone knows the supermarkets, but, um, and we wanted to know what their main competitors were or what people were talking about when they are talking about that supermarket, understanding sort of what's out there already would be very helpful in terms of the language that people are using. Um, so this is something that we, where we did use the AI capabilities at our company to find the sort of what we call the brand list, so the other brands to ask about as well as the, the main one that we are talking about. Um, so that was, that was really useful in, in all honesty, I'm not a hundred percent on AI's full capabilities. I'm sure there's probably roles for it at every stage. Um, there is also, you obviously can do like automated reporting. Um, so, uh, sort of if I want a bar chart of this data, um, then having that automated, um, I know that is possible in some forms now, but it's quite a manual process to get it set up. Um, so if there was a way to make that easier, then I think that would be quite a good role for AI as well.  
**Speaker 1** 00:10:56:  So, sorry, if I understood correctly, is uh, like data analysis part, you mean the understanding your data or transcriptions or that there was writing on my side?  
**Speaker 2** 00:11:09:  Yeah, actually you just reminded me of something, but I actually more meant the actual creation of like a, like we do write reports, so the visuals of it, um, more meant the creation of that. Um, but what you just said there is manage me, we quite often have like what we call open end questions, so, uh, people can type in whatever they want. So, um, automatic coding and sort of sentiment analysis of that, um, is something that's really useful.  
**Speaker 1** 00:11:35:  Yeah, right. Interesting. And, uh, at the end of your data analysis, uh, what are the usual form of representation of data? How do you, uh, are, are there like some common ways you always present your data? Uh, like in PPT presentation, but also including tables and spreadsheets, or how do you use the  
**Speaker 2** 00:11:58:  It's pretty much mostly PowerPoint.  
**Speaker 1** 00:12:01:  Mostly PowerPoint, yeah. And, uh, okay. And, uh, when you say that, uh, a lot of people are using it, but you're not a hundred percent into into it, is it like a, uh, an any trust issue with ai? What is, uh, holding you or, or, or this is something you would start exploring more?  
**Speaker 2** 00:12:20:  It's probably, um, a lack of knowledge of what's actually available. Yeah. Maybe, um, like I don't really know what can be done.  
**Speaker 1** 00:12:35:  Yeah,  
**Speaker 2** 00:12:37:  That makes sense.  
**Speaker 1** 00:12:39:  No, sure, sure. Yeah, that, that, that's totally sensible because not everybody is trying to, uh, jump onto the latest technology available. It's like, unless we are very clear that how is it going to benefit me, and sometime we can explore it by ourself or sometime your company can force you that we are moving to this tool and  
**Speaker 2** 00:12:58:  Yeah.  
**Speaker 1** 00:12:59:  And do you feel that kind of, uh, need or, uh, uh, you know, suggestions coming from your company's point of view that we should use more AI or, or your bit more independent on your side that you can manage in whatever way you feel?  
**Speaker 2** 00:13:13:  Okay. I think from, from, obviously as I said, I, I've only just joined my current company, but um, there does seem to be desire to use more ai Mm-Hmm. \<affirmative\>, um, I think to, you know, make things more efficient. And, um, there's lots of data in lots of different places in my current company. Um, so you know, a way to consolidate that, that sort of stuff I think is what, um, is looked for, uh, but not there yet.  
**Speaker 1** 00:13:42:  Right. Yeah. Thanks. And, uh, so, uh, okay, for the moment, when you're not using AI a lot for different steps of your, your product, can you tell me the names of some of the tool, like from the beginning, from product planning to data recording, data collection and data analysis, towards the end, when you are sharing these, what are the different tools that you're using for different stages?  
**Speaker 2** 00:14:09:  Yeah, sure. So I'll, I'll go back to my previous job because I think that's probably more useful. Um, but uh, some of it's not fancy at all. Um, so like when we're writing our questionnaire, we are just doing that in a Word document. Um, then when we are, uh, scripting it, so putting it online, it's into, um, in software called the Cipher. Um, then, uh, when we are doing the data processing at the backend, it's software called Merlin or Merlin Co. Um, and then, uh, we use Excel and uh, key research and then PowerPoint.  
**Speaker 1** 00:14:50:  Okay. Yep. PowerPoint at the end. And, uh, the information that goes into PowerPoint, I believe that for same product you would be talking to different kind of, uh, uh, people at the end of the product, maybe sometime discussing it with your team where you can have more technical discussion. Mm-Hmm, \<affirmative\>. And then at some point you're also talking to more management people where you will be talking to high level language. So yes. Uh, or am are there only these two cases are there, or, or can you explain me a little bit more that, uh, how many different kind of, uh, expertise were available on the target side where you have to, uh, you know, rephrase your slides or PPT, uh, every time to talk about the same project?  
**Speaker 2** 00:15:34:  Yeah, um, probably at a high level, what you said is, is correct. It, we will usually find out the audience before we are creating the report and gear it to that specific audience. Um, so quite often we'll be doing one report, but it will only be suitable for one audience, and that might be a research audience. Um, but then for us it's then not applicable or youthful to take that to the wider team, like product teams or marketing teams because it's too research specific. Um, so I would say the, the two sort of high level buckets that you said there, where you're speaking a bit more technically or where you're speaking a bit more high level and it needs to be condensed and, um, a bit more succinct, I would say. Yeah, those are the two main things.  
**Speaker 1** 00:16:31:  And, uh, do you think, uh, some sort of automation or AI could help you there with the data representation? Sorry for getting it right way for data visualization for target specific audience?  
**Speaker 2** 00:16:47:  Yeah, I mean definitely if there was sort of a, an option for this is this is all the data that we've got for like the more technical, um, details team and then thinking, simplify that and just pull out X, Y, Z or simplify the language that has been used. If that's, if that's a possibility, then I think that would be, that would be useful.  
**Speaker 1** 00:17:11:  And, and, and how do you, can, can you tell me the names of some tools that you use to, uh, for data visualization of, for quantitative data, let's say when you have to that  
**Speaker 2** 00:17:21:  Used? I need to be honest, it's just, it's just PowerPoint and we just create charts or visuals or images or whatever within, within PowerPoint. We don't use tools to do it.  
**Speaker 1** 00:17:32:  So, uh, okay. So, so the data format before that is like Excel sheets. Yeah. And, uh, you manually, uh, get figures and plots in PowerPoint from just Excel sheet.  
**Speaker 2** 00:17:45:  Yeah,  
**Speaker 1** 00:17:46:  That's it. Uh, is, is there any other type of data format that you might have received and, uh, you needed to get like, some meaningful trend from that data other than Excel sheets?  
**Speaker 2** 00:17:58:  Um, not really. We tend, we tended to put everything in Excel or the other research software that I mentioned, Q Research, um, that was sort of done, like that's its own separate package like program, but then you'd export it to Excel. So everything sort of in the end came from Excel forward to then populate our PowerPoint.  
**Speaker 1** 00:18:23:  Right. Yeah. Thanks. So do you think that, uh, the nature of your job or the tools you were using in your previous job would continue in, in, in your current job as well? Or you have to like learn new kind of tools now and structurally change?  
**Speaker 2** 00:18:38:  Um, no, I think it'll be fairly, I think it'll be fairly similar to be honest. I think, um, yeah, from what I've seen, PowerPoint is, uh, one of the main things we do also sometimes use Google Slides, um, yeah, which I'm not a huge fan of because I think they're quite inefficient and fiddly. Um, but, uh, yeah, so that, so that's why we most of the time use PowerPoint. But if there was a way to make Google Slides better, \<laugh\>, I'm sure that would be good.  
**Speaker 1** 00:19:09:  So, so, so, so why do you think that's inefficient? Because a lot of people like it so you can collaborate, uh, at the same time in Google Slides.  
**Speaker 2** 00:19:17:  Yeah, that's definitely a positive, um, of it, but you could do that in like SharePoint or other, like, I don't have to be on Google Slides to be able to do that. Um, it's things like you can't copy and paste the charts from like one side to another. You have to recreate them. Um, the capabilities on Google Slides are not as, um, strong as in PowerPoint themselves. It's harder to actually work with. It takes, it just takes more time. It's more fiddly, um, to sort of align things or make them look nice. Um, and yeah, just generally I don't think it's as good a program as working in PowerPoint for what we, what we need to do.  
**Speaker 1** 00:20:00:  Right. Yeah. Thanks. And, uh, so you have briefly mentioned, uh, sentiment analysis about, uh, qualitative analysis. Like, uh, you want to get some kind of information from ai. Uh, uh, would you like to tell me a little bit more that, uh, what is your, let's say in idealistic scenario when there is no problem with technology, uh, what would be your wishlist, uh, for AI to what, what kind of information you want to get out of qualitative analysis with the help of ai?  
**Speaker 2** 00:20:30:  So most of the time what we're doing with that is grouping it into themes. So if we ask for example, someone why they like a certain product or an idea that we've shown them, what ultimately what I want to know is have half the people said the same thing or is there 20 different things that all get equal mentions? So being able to understand what the theme is and the size of that theme within the, within the number of participants that we have, those are the, those are the two main things. Um, and sort of understanding whether people's, 'cause this is something I think is currently missing. I can't think of an example off the top of my head, but the context and the tone can sometimes be positive and sometimes be negative. And I think that sometimes is missed. Um, with, with current, with current tools or whatever that we've been, that we've been using, um, complexities of the English language I guess. But, um, yeah, that, that's ultimately what we want to know. We want to know the themes of what people have said and how we would then categorize them as if, as if we'd ask them that on a list for them to choose from. We then want to be able to take everything that they have said and turn it into that list essentially.  
**Speaker 1** 00:21:45:  Mm-Hmm, \<affirmative\>, right. Yeah. Thanks. Very interesting. And, uh, in your work, uh, are you happy with the, uh, AI is just speaking in English language or do you often come across the need of, uh, translation to different languages?  
**Speaker 2** 00:22:03:  I mean, I think, um, I only speak English, so I don't need anything else \<laugh\>. But in terms of that, um, we do do research in other markets. So for that example, um, that I was just saying, if I've asked an open-ended question, obviously when we get the data back on sort of coded questions, then it's fine because all the codes match up. But if it's a verbatim response, then I have no idea what has been said. Um, so, uh, in previous projects we have like outsourced it to agencies, translation agencies for them to then translate it and for them we would then look through it. Um, so, or yeah, automatic translation, um, of those would be useful.  
**Speaker 1** 00:22:50:  Right, thanks. And, uh, let's say that AI is giving you, uh, this information about thematic analysis or it's giving you sentiment analysis and trying to, uh, give you as clear message as possible, uh, where human will take more time, uh, will you trust it or if not, then uh, what would be your deciding factor to trust it?  
**Speaker 2** 00:23:14:  Trust the translations, you mean?  
**Speaker 1** 00:23:17:  Well, not just the translation, like the key insight that you're getting, like the sentiment analysis or thematic analysis, like the output of the qualitative analysis. Um, what would it take for you?  
**Speaker 2** 00:23:29:  I think, yeah, I think I would, I think I would trust it, um, of my, my hesitation would be more on translations. 'cause I don't think automatic translations are that good currently. Um, but uh, in terms of the sentiment analysis, I can't think why. I wouldn't trust that. I probably would trust that if it was, um, grouping into, into themes, I feel like there's only so much that can go wrong there.  
**Speaker 1** 00:23:59:  Right. Okay. Yeah. Thanks. And, uh, what do you think about, uh, ethical complications? Do you, uh, are, are you concerned with the, any ethical, uh, complications that might come with the, the rise of AI for your data analysis and the recording? Or is there any obvious issue that could be a deciding factor for you that can make you a user or can make you hate AI forever?  
**Speaker 2** 00:24:32:  \<laugh\>? Um, yeah, I guess, and actually just, sorry, just going back on what I just said, that the issue I think of where you need the human context is on that, is on the, on the tone and interpretation. So what I, basically, what I was saying before probably would be an issue of trust in terms of are people saying whatever they said in a positive light or in a, in a negative light. Um, and I think that there's, there's only so much that AI can do with that currently. Um, and I think it, it misses the, the nuance of what people have said when you sort of, uh, bring it back to the basics of this word means x because sometimes it doesn't, sometimes it changes depending on the context or the tone in which people have said it. Um, so I think that's what would make me skeptical would be missing out on the, on the nuance of, of what is actually being said.  
**Speaker 1** 00:25:47:  Mm-Hmm, \<affirmative\>. Right. Yeah. Thanks. And, uh, in terms of tools, uh, AI related tools, uh, can you tell me how, what, what have you used so far when you, when you say ai, uh, what comes to your mind and what's, uh, your experience of, uh, just the name of some tools?  
**Speaker 2** 00:26:08:  So the o the only one that we have used for that specific example was the Q research, um, software that I mentioned. They have like an automatic, uh, coding. Um, and then there is something that we have used very specifically, um, for communities called the leader. They have sentiment analysis in that. Um, other than that, for work purposes, I don't think I've used anything. I mean track DBT obviously, but not specifically to, for any of the, the project stages that I've sort of spoken about really.  
**Speaker 1** 00:26:46:  Yeah. So when, when you talk about, uh, coding, automatic coding in Q research, you are still talking about coding in qualitative research and not the programming languages, right?  
**Speaker 2** 00:26:57:  Sorry? Yes. Yeah, coding, we, we say coding sort of up into themes.  
**Speaker 1** 00:27:02:  Sure, sure. Yeah. And, uh, about the sentiment analysis, the name of the tool is leaders,  
**Speaker 2** 00:27:09:  Our Leader, um, but it, that's not, it's, that's not the function of it. It's  
**Speaker 1** 00:27:15:  Oh, yeah. That, that's a separate tool, I think a leader. Yeah.  
**Speaker 2** 00:27:18:  Yeah. Um, it has that option within it, but that's very much not the main function of a leader.  
**Speaker 1** 00:27:25:  And what, what are the other function that, uh, you like in a leader other than,  
**Speaker 2** 00:27:29:  Oh, the main, the main function of it is that it's sort of a, um, a community platform. So you can build communities of people and do research with them. That's the main function of it. Um, but yeah, within that there is this option of doing sentiment analysis basically.  
**Speaker 1** 00:27:47:  Oh, so is it like when you want to continue some project and, uh, those are your participant and, uh, you can build your community there, but how do you, how do you find those people then?  
**Speaker 2** 00:27:57:  So it, it depends really. You find them sort of outside, usually using whoever your client is. They might have like a, a customer database and you would contact them and ask them if they want to be part of that community. Um, but it kind of depends on who your client is as to how you find them.  
**Speaker 1** 00:28:15:  Yeah. And have you ever come across a product where your clients are not giving you the list of customer data? They just give you a list of questions that they want answers to, and do you have find participant on your own?  
**Speaker 2** 00:28:27:  I personally haven't had that. No. I'm sure, sure it happens, but I haven't had that \<laugh\>.  
**Speaker 1** 00:28:32:  Okay. Right. Yeah. Thanks. And just, uh, last question now, uh, thanks a lot for covering, uh, uh, a lot of details. Uh, just if you have to give like a brief answer that, uh, uh, for your product management plus research in qualitative and quantitative sides, uh, what, what, what are the features that are missing now, but you want AI to really, uh, give you those features that you can start using soon where you feel like you are often spending way more time or it is either it's a admin task or it's a data analysis or anything like your top priorities that you want Yeah. To, to solve.  
**Speaker 2** 00:29:16:  Um, probably, probably where it would be most useful is, and again, thinking back to my previous job, you work on lots of different clients and lots of different industries and you don't know them all in depth. So you have to do what I was sort of talking about at the beginning in terms of the language that people are using, what they're talking about, what other brands they're using. Um, so sort of the desk research part of it. Um, creating a, like a, an information sheet for things that you might need to know about that brand, that industry that's sector, um, that would be very useful. Um, the data visualization side of things. I think those are the two most manual parts currently. Um, so those, I would guess would be the priorities.  
**Speaker 1** 00:30:12:  Right. Okay. Great. Thanks. And, uh, about, uh, the collaboration point of view, uh, when you talk to people, uh, you take care of your team, uh, you have like seven, eight product, uh, going mm-Hmm. At the same time. And, uh, uh, in the project management, from the project management point of view, do you see any direct application of AI there where you can save your time, uh, to easily convey your messes or, uh, you know how Yeah,  
**Speaker 2** 00:30:50:  Creating timelines and then the timelines always change. So instead of having to go in and update manually, sort of when things will be ready or the new timeline, uh, being able to say, I've shifted this back X days and it updating, um, and managing the resource of people. So understanding, uh, on a given day, this person would have three different things to do where they can only do one. And sort of flagging that would be would, from a people point of view, that would be good.  
**Speaker 1** 00:31:25:  And, and Right. Yeah, that's very interesting. And currently for project management, uh, can, could you please remind me, uh, what's the tool that you're using  
**Speaker 2** 00:31:35:  For, for what aspects? Sorry?  
**Speaker 1** 00:31:37:  Oh, for product management, how do you manage your product? What kind, how do you manage the progress of product? Like from the planning and then your, your team is working.  
**Speaker 2** 00:31:47:  Yeah, there's no tool \<laugh\>,  
**Speaker 1** 00:31:48:  There's no tool, no  
**Speaker 2** 00:31:50:  \<laugh\>, no, it's just, it's just, it's just manual. Um, yeah.  
**Speaker 1** 00:31:55:  Okay. So it's uh, like a lot of information that's scattered in people's personal space and then you have to ask them to provide you that.  
**Speaker 2** 00:32:03:  Yeah.  
**Speaker 1** 00:32:04:  Right. And if there is a tool that can, uh, that, that, that can give you possibility to manage different stages of the project at a single place, would you be interested in the Yeah. Exploring that or that's like, uh, uh, too much information at same place and uh, you will still not use it?  
**Speaker 2** 00:32:25:  Um, no. I mean, I think if it simplifies it and helps manage resource, I think is the main thing. Um, so understanding that and three weeks time this person has a big clash of projects, that would be, that would probably be the main thing.  
**Speaker 1** 00:32:39:  Right. Okay. Great. Thank you very much. Uh, is there anything else that you would like to highlight, uh, uh, in terms of, uh, increasing the use of AI in market research that I might not have asked before? Any other discuss?  
**Speaker 2** 00:32:55:  I don't think so. I think I've probably covered everything. I, I, I genuinely think for me and for others is it's sort of an, there's an awareness, knowledge gap, um, of what actually is available. Like some of these things probably are available. Um, so other than the things that I think would be useful that I said that, that would be the main thing for me.  
**Speaker 1** 00:33:17:  Main right. Yeah. Okay, great. Yeah, that's very interesting. Uh, yeah, sometimes people just need to know that what's out there that can help them and Exactly.  
**Speaker 2** 00:33:27:  Yeah.  
**Speaker 1** 00:33:28:  Yeah. Only then they can tell that, how it could be better. Yeah,  
**Speaker 2** 00:33:31:  Exactly.  
**Speaker 1** 00:33:32:  Okay, great. And thank you very much for your time, Jane, do you have any other question for me?  
**Speaker 2** 00:33:39:  No.  
**Speaker 1** 00:33:39:  No. Okay, great. Thanks again and have a nice rest of the day.  
**Speaker 2** 00:33:44:  No worries. Thank you.  
**Speaker 1** 00:33:45:  Bye.

# 

# Patrick Hourihan 

International Insights Director | OTT research expert | Research programme strategy | Voice of the Consumer | NBCUniversal Media 

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:00:  \<inaudible\> insights@uhbeings.com. Uh, we are developing, uh, uh, ai, uh, products for, for market research.  
**Speaker 1** 00:00:10:  Great.  
**Speaker 0** 00:00:11:  And, uh, here we have another participant called Data, which is our note taker work by our company. So, do you mind if I keep it here to record our meeting?  
**Speaker 1** 00:00:22:  Yeah, that's fine. Do you need me on camera in this or can I turn off the camera?  
**Speaker 0** 00:00:27:  Well, that would be great if you can keep it,  
**Speaker 1** 00:00:30:  Keep it on.  
**Speaker 0** 00:00:31:  Yeah. Yeah. If you can keep it on  
**Speaker 1** 00:00:34:  Is the, um, is, how's the data gonna be used? Just off the back of it, just so that I understand, like, is there, I, I'd like to think any video and audio is, is confidential and proprietary.  
**Speaker 0** 00:00:46:  It'll be strictly for our internal use only by the product team. We are basically just three, uh, CEO of the company, Dave, and then one more product manager. It's basically just by the, uh, used by the product team. It'll be used only for our internal discussions. Nothing, uh, public.  
**Speaker 1** 00:01:03:  Okay. Sounds good.  
**Speaker 0** 00:01:05:  Yeah. But if, if you have any problem, yeah. You can, uh, switch off the video, uh, as you prefer. Uh,  
**Speaker 1** 00:01:11:  Sounds good.  
**Speaker 0** 00:01:13:  Okay, great. Thanks. Uh, so, uh, can, can we please start with the, a little bit introduction about your work, uh, think what kind of jobs you do, and if you can also explain a little bit more about your day-to-Day activities and if, uh, you are using ai, uh, for, for any of those jobs.  
**Speaker 1** 00:01:32:  Yeah. Um, my name is Patrick Rahan. I am a senior director in the Global Analytics and Measurement team at NBCU. Um, my, I've worked in research for about 20 years at different media companies, the BBC at Yahoo, and at NBC Universal, as well as some time freelancing and agency site. Um, uh, my role fundamentally, I'm not gonna talk about the details of my role too much 'cause it's, you know, within, within NBCU, but, um, my main focus is understanding streaming behaviors, so not broadcast tv, but really the, um, the emergence in the media landscape of, of, of global streaming platforms and trying to understand the relationships that consumers have with them and with the content that sits in those platforms. Um, that research can take a number of different forms. Um, as with any international broadcaster, to be honest, I don't think this isn't proprietary, but there is data analysis, there is focus groups, there is, uh, quantitative surveys.  
**Speaker 1** 00:02:42:  Um, there are passive listening tools. And most roles in media and, and mine included, will include a combination of analysis across all of those things. Um, I am working on a, um, we are actively thinking about AI at the moment. I think the challenge with ai, if I'm honest, is if we go back two years and we think about, um, pre chat GPT and pre the gen AI explosion, there were lots of tools that already out there that were talking about, um, machine learning and data automation. And it's quite hard to tell which of these tools now are, um, are actually meaningfully AI powered, you know, and how different are they to what went on in the past. And also it's really just, it's, it's, I read a, a piece on LinkedIn the other day, which is really interesting, which basically said, if you are pitching a new company or a new business or a new product and you don't mention ai, then is it gonna feel like a gap to whoever your audience is?  
**Speaker 1** 00:03:59:  Because you shouldn't need the words ai just to make it sound relevant. So I think there's a lot, personally, I think there's a lot of smoke and mirrors around Yeah. What does constitute AI and what doesn't? And ultimately what, as a, as a researcher and as a leader of a research team, what I'm looking for is tools that greatly enhance our ability to create insight and to interpret data and to find solutions to challenges. Um, I guess I think about there's a, there's a few different ways of thinking about it. I mean, you are, you are asking me specifically about AI from a research perspective, but there is also AI that can enhance our business overall, which makes research easier or harder. So, for example, there's lots of, um, one of the areas within content creation and NBC universal obviously creates content like Yep. Um, uh, is about the ability to auto translate content or the ability to lip sync content in a much shorter space of time and a more efficient space of time.  
**Speaker 1** 00:05:07:  And none of that is really about, really about market research, but all of it is about the product that we help to support. So I know today we're not really talking about that, but the, the ways in which we've used AI tend to be, um, using, um, some tools, um, like, I'm not gonna say that they are, but like chat PT or like, um, copilot to, um, enhance survey design or to, um, support enhance, um, uh, discussion guide design or, um, and that would be more, that would be an in-house usage. From an external perspective, it's probably more what we'd like to see are meaningful tools that allow us to, um, manipulate and dissect large, large sets of streaming data faster and more efficiently. I think we're looking for tools that allow us to, um, synthesize data wherever possible, whether that's, um, just fusing data, two different data sets that are relevant to each other, or even fusing more synthetically with other data sets that may, you know, that you are looking to do some predictive modeling of off of. And I think it's quite hard in that whole area about data interpretation to understand, you know, you've gotta do a full product analysis to really decide whether you wanna buy some of those things. Um,  
**Speaker 0** 00:06:48:  Yeah,  
**Speaker 1** 00:06:50:  Sorry, it was a slight ramble for a while.  
**Speaker 0** 00:06:53:  No, no, I, I really appreciate that. Thank you very much for covering a lot of topics. I totally agree with you in terms of, uh, the use of, uh, AI that if people are appropriately using it or not. My background is from data science and machine learning and computational modeling. And, uh, I, I totally agree that a lot of things that are pure automation are sold as the AI now. It's not really ai, uh, but yeah. Uh, the need that you mentioned about synthesize, we are, uh, that's also one of our goal to develop a synthesize tool that will help with the data analysis and, uh, using AI to drive more insights from your data. So could you please tell me a little bit more, when you say data, are you talking about qualitative data or quantitative data? Uh, and, and how much ai, uh, are currently trying to use in data analysis?  
**Speaker 1** 00:07:43:  Not that much. Mm-Hmm. \<affirmative\>, um, it is the, it's probably 80 20 in terms of the data that we use towards quantitative data. Uh, either be about viewership or about subscription or about, um, or maybe third party data. Um, and qual data, yeah, it's probably 20% or even less, maybe 10% of the time. Um, it could be open text verbatims, but we do commission qualitative research, IE focus groups or debts or, um, expert interviews and those kind of things. Um, the reason that, I mean, at the moment we, if our business tools of choice, um, are like, like, like most companies, you know, it's tends to be Microsoft or another, another company there we're be implementing some of their AI improvement into what we do on a day-to-day basis, just because that's an enterprise deal. But we're not using too many specific tools. We're more at the exploratory stage. Mm-Hmm. \<affirmative\>, um, probably for the reasons that I've already discussed, which are, it's difficult to tell what's a meaningful data synthesis tool versus what isn't until we do, um, some broader assessments basically.  
**Speaker 0** 00:09:07:  Right. Thanks. And, uh, if you want to know a bit more about qualitative research, uh, and are you using, uh, any, uh, services by any research agency that you will tell them, uh, do this kind of stuff, or you will have to organize, uh, and plan those research project by yourself and then recruit the participant and talk to customers? How do you carry that?  
**Speaker 1** 00:09:32:  Yeah, we, we don't, um, we tend to commission qual agency to run the research, run the field work off off the back of an RFP, but as part of that pitch process, you know, there will be AI related tools that will be part of the methodology that will be chosen. And, you know, it's usually a competitive pitch, so they will have an influence in terms of, um, what to do. And the, our regular agencies that we work with on some of that stuff have, have actively built in. So it's almost secondary AI usage because if the agencies we are using have adopted certain tools that are helping them become more efficient, and that's obviously helping us too.  
**Speaker 0** 00:10:14:  Yeah. Great. And, uh, in terms of, uh, data analysis, so they are probably doing all the data analysis and, uh, can I ask what kind of information they hand it back to you and in what formats?  
**Speaker 1** 00:10:27:  Uh, PowerPoint, um, usually it's, you know, transcripts and, um, and video footage we can always get hold of, but because we are, we usually ask for full service qualitative support. It would be, um, PowerPoint and maybe Word document summary. Mm-Hmm. \<affirmative\>. So, you know, there could be, um, if we get all of the videos and Vox pops and everything, then that obviously that's ripe for, um, ai, um, sort of metadata exploration, I guess in terms of, we were looking for certain sound bites within a, within a, um, hour and a half focus group that could be helpful, but that isn't something that we've done so far.  
**Speaker 0** 00:11:15:  Yeah. And, uh, do they also provide you some information about these sentiment analysis, like getting the tone of the discussion or those kind of things?  
**Speaker 1** 00:11:27:  Uh, they do. I mean, that would be in the final analysis, but that would be based on human, um, human analysis rather than a, an AI system telling them that it would be, um, yeah, that would be, we, we wouldn't get a sentiment analysis alongside it. It would be just something that would be included.  
**Speaker 0** 00:11:51:  Mm-Hmm. \<affirmative\>, uh, how do they decide about the preferred methods for data collection? That if they have to run a survey or if they have to do the video recording, do you usually tell them or you tell them the higher level requirement and then they are free to choose whatever way they feel confident in then just Yeah, how,  
**Speaker 1** 00:12:13:  Yeah. A high level requirement usually, but, you know, preferred methodology, we are, you know, still very well versed across all research methodologies and we'd usually guide towards what we would expect to see within the RFP and then, um, after that, um, you know, the agency can choose, but we expect them to choose the methodology. It works best for the project, obviously, and that will be part of the competitive process. So, you know, if you, um, if you, uh, yeah, if you, if if you pitch the wrong approach or maybe they're not, not, not the optimal approach, then you know, you're less likely to be chosen.  
**Speaker 0** 00:12:57:  Mm-Hmm, \<affirmative\>. Alright. Thanks. And, uh, so once you're getting the data and they're handing over you the summary and, uh, the PowerPoint, so do you usually tell them that, uh, this should be their target audience that you want to talk to management board or you want to talk to other qualitative researcher under your group? Do they give you, uh, optimized summaries or slides for different target groups, or it's always just a one generic result?  
**Speaker 1** 00:13:27:  I mean, it depends on the project, so absolutely. We do have sort of persona build outs from qual research, but, um, we can talk about if you, if you want to talk about Quant, we can talk about quant research because the outputs are, are different and probably more detailed than what we, we tend to get a strategic insight output from qual and stick to, and we don't get, uh, we, if we request it, we'll get sort of additional persona analysis. But it's, it's been more the traditional outputs up to now basically, which is a narrative around trying to answer the research objectives that we've put in place in the RFP.  
**Speaker 0** 00:14:11:  Right. Thanks. No, I totally understand that there is not like a hundred percent clearly very well defined boundary between two research and maybe in the same project you might need to do a bit of both quantitative or qualitative. Right. And then how do you, uh, would you like to comment on the use of AI for quantitative research as well? How much are you using or you are exploring or Yeah, what would,  
**Speaker 1** 00:14:33:  So, so again, if, if it's a commission project, we'd usually get some kind of insights output or dashboard and or dashboard, um, in some business intelligence tool. But also, um, we would typically get, um, uh, a full data set which either we can, um, analyze and put into our data warehouse or we can, um, analyze it in Excel or, or Power Query, um, using Power Query if it's a big file. But the, we are not at the stage of really combining that data with anything else. It usually stays within the confines of that project. Mm-Hmm,  
**Speaker 0** 00:15:20:  \<affirmative\> and, uh,  
**Speaker 1** 00:15:25:  Sorry, actually, so that would be commissioned quant research, but the, but the bulk of what we do is non-commissioned, which would be just data that we have access to Mm-Hmm. By either first, second, or third party. Either our data, our partner's data for our content that sits on their platform or third party altogether. And that's the opportunity area around ai. And that's where we have large data warehouse capacity to hold different, um, different sets of, um, viewership data really to help us understand how content is working and what are the, um, metrics of success and that kind of thing. Um, and because those data sets are quite disparate and can, can be quite different, that's where there is always more of an opportunity to create efficiencies or for, or for us to really minimize the time it takes to data clean, to add relevant metadata and to distill that into something that a researcher can just take away and go, okay, I'm gonna analyze this now.  
**Speaker 0** 00:16:32:  Right. Thanks. And, uh, for data visualization in general, uh, could you please comment on the, the, the ways, different ways to represent that data? Like how, uh, I mean they gave you summaries and then, uh, for quantitative data they can get, they can present it in some different forms. So what are the mostly used forms to, for data visualization? Like a graphs, charts, uh, or summaries or, uh, or, or detailed dashboards, or how do they hand it over to you?  
**Speaker 1** 00:17:04:  So is it, are you talking specifically about a commission piece of research while we've worked with a research agency? Or are you talking about any data analysis that could be done internally?  
**Speaker 0** 00:17:14:  I think you can talk about like both. Yeah.  
**Speaker 1** 00:17:18:  And so commissions, can you just repeat your question briefly?  
**Speaker 0** 00:17:22:  Sorry. Uh, yeah, so I mean, for data visualization, uh, what are the most common ways, uh, to look at the data? They giving you some link to your dashboard where you will see graphs, charts and summaries or, or, or, or mostly in just words like transcripts. So what kind of visualization information you get in both projects.  
**Speaker 1** 00:17:45:  Yep. Uh, commission research primarily two ways. PowerPoint and, um, business intelligence dashboards. So business intelligence dashboards to be able to track data over time and to be able to slice and dice both for researchers and stakeholders. But in terms of PowerPoint, it's like, usually if we commission, we commission with a very specific objective in mind and we wanna see the answers to that objective, uh, narrated through an insight story that's usually, basically usually still comes in PowerPoint and will probably still be coming in PowerPoint in 10 years time. So, um, 'cause it's a very useful visual medium, um, about the, um, yeah, you know, sometimes yes, we still use lots of different types of charts. Yes, we use word clouds. Um, you know, we try and, um, increase the visual aesthetic within PowerPoint wherever possible. I use imagery, et cetera. In, um, dashboards, it's really built around usable tools.  
**Speaker 1** 00:18:51:  So we don't want, we want something, not, not all of the dashboards will be used by researchers. They'll just be used by data literate, um, stakeholders and employees, you know, not necessarily in the research team. So you have to build them for people that they're actually gonna be able to utilize. So simple, clean, visual effective, um, other priorities about how those dashboards would work? Um, internally, um, if it's a report, if it's like an insight report, it would again, usually be in PowerPoint. But again, there, there's basically two main outputs now. Um, well, sorry, three, um, word is used rarely, you know, sometimes, but rarely as a, as an analysis output. But it would be either a business intelligence tool like Tableau, power bi, or PowerPoint or Excel. And Excel would be the least visual, but the fastest route to the data. So it might be a, a lower value request where we're basically providing data. Business intelligence tools tend to be for longer periods of time. Mm-Hmm. \<affirmative\> PowerPoint tends to be for, um, bespoke report outputs, storytelling, and, um, you know, a deeper understanding of a particular topic.  
**Speaker 0** 00:20:15:  Right. Yeah. Thanks Laurie. That sounds very interesting. And, uh, particularly for your role, uh, are you using any AI tools to when, when you have meeting with people to get automated transcription or summaries or notes or to perform some kind of analysis, do you use any AI tool there for your job? Yeah,  
**Speaker 1** 00:20:39:  I mean the, they, they're not research specific I'd say. So just because of the, um, because of the types of tools that we have baked into the company at large, those things, like in meetings we have, like your tool here, ada, we have transcription, capturing technology within meetings, um, obviously recording within meetings, there is some synthesis of meetings that happens, but all of these, I'm not saying are applying directly to, to market research or to an audience research project. Um, so less so that it feels like we're, I think the way our position in the company, in way in the international research team would be, we are just getting to a point with AI tools that some of them, some of them feel meaningfully useful and should create greater processes, but we are, um, taking baby steps into that world right now, basically. Yeah.  
**Speaker 0** 00:21:45:  Right. Thanks. And, uh, these transcriptions of summaries provided by those tools, uh, how likely are you gonna trust it? Are they usually reliable or you wish that, uh, this could have been better?  
**Speaker 1** 00:22:00:  I mean, that's a good point. Um, I haven't used them too much because I, I don't, I feel like I, my my process isn't always about reading a transcript after a meeting. My process is usually about, um, writing down key points within a meeting. I'm just a, I'm just a writer, so, um, I know I might not capture every moment at the meeting, but I like to think I'd distill down the key points. Um, yeah, I'd, I'd, I'd use it for a meeting that I had a transcription for where I wanted to go back over some old notes. Mm-Hmm. \<affirmative\>, uh, yeah.  
**Speaker 0** 00:22:32:  Right. Thanks. And from ethical consideration point of view, uh, do you see any ethical challenge that that could arise because of the increased users of AI in either in market research or in general, that, that you would like to highlight?  
**Speaker 1** 00:22:47:  A hundred percent. Um, you know, incorrect data, because models are based on what's out there and what is out there can be in incorrect data. Um, you know, the, the new apps chat, GPT copilot have usually got health warnings within them to say, um, you know, essentially data may not be accurate or insight may not be accurate. Um, please check, but that's a bit like teas and C'S in a legal document. It's like when you're signing up to buy something, quite a lot of the time you'll skip past the T's and C's in the legal document. You'll just sign up to the product that you want to get. So the tricky thing is that we are, um, AI is now out of the box essentially. And, um, people will, a, a proportion of users will just believe whatever they get and use that. And that is, and as a researcher, as working in a team of researchers, that's not how collectively we work, but we don't reflect all of the users of, of, um, these new tools. So the danger is amongst, you know, certain sectors of users that their sort of threshold for validation's much lower. Um, you know, this is, this is all stuff, you know, and I'm sure you are aware of too. It's what it's, um, so yeah, the big and scary arguments I think are quite big and quite scary, but just simply the, the job of a researcher is, um, trying to get to that grain of truth. And so any, any product that might take you away from the grain of truth is a concern basically. Mm-Hmm.  
**Speaker 0** 00:24:43:  \<affirmative\>, right. Yeah, totally agree. Yeah, that's very valid, uh, concern. Uh, about, uh, so now, uh, I'm going back to the data analysis part again. Um, when we talk about qualitative data analysis or quantitative and select, you are getting the end result for qualitative, uh, do you have like in-house expertise to run qualitative research or quantitative research, uh, in your company as well where you don't have to ask the other companies?  
**Speaker 1** 00:25:12:  Yes. Um, quant, yes. Qual maybe in the US but not internationally.  
**Speaker 0** 00:25:18:  And, uh, do you see an opportunity of, uh, increasing the use of AI in quantitative analysis as well, or providing some sort of automation where the manual work could be decreased or, or if you want to give some examples how it could be achieved?  
**Speaker 1** 00:25:35:  I think like the, the opportunities in quant analysis are, um, around data processing, data tagging. So we, we, the metadata involved in media entertainment is extremely valuable, but often doesn't come as within your data. And it is challenging for everyone, I think. So improvements in metadata and fusion of metadata would be hugely useful because some of that work still put takes place manually to this day. Um, I think it's less interesting to think about a AI product that gives you the answer to the question, because I still think that you can't skip past human distillation of a human contextual understanding of whatever the business challenge is, um, unless you very precisely guide your tool through every step that you wanted to do, and it still may make, make mistakes. So I feel a bit like AI's there to do the heavy lifting, to get the data in the shape that you need it to be in, and then for, um, uh, research specialists to come in and, you know, ultimately do the last five or 10% if AI can, can reduce the amount of human interact, human effort, um, to get the data into the shape you want it in.  
**Speaker 1** 00:27:02:  I still think that's, yeah, I mean it's, yeah, it feels like, um, quite an old problem, but it's a very real problem still, you know,  
**Speaker 0** 00:27:11:  And, uh, when you talk about the manual understanding of text or, uh, or the sentiment in general, are we talk, are we talking about, uh, coding and tagging that's like manually, uh, interfaced with the, the information that you're getting  
**Speaker 1** 00:27:26:  Sometimes? Yep. So it'll either be manual or it'll be, you know, using lookups or some other way of making that manual process a bit faster, but ultimately it's still a form of manual process. Mm-Hmm. \<affirmative\>. So yeah, it's not, um,  
**Speaker 0** 00:27:42:  Right.  
**Speaker 1** 00:27:43:  Industry is not as advanced as you'd like it to be.  
**Speaker 0** 00:27:46:  Thanks. Yeah, that's very interesting. And, uh, now can you, would you like to tell me your wishlist, for example, uh, from AI that to what are the low hanging fruits in your research journey, either in qualitative or quantitative? I mean, you, we have covered basically a bit of everything. Mm-Hmm. \<affirmative\>. But if you have to gimme like a couple of, uh, low hanging fruits that this is what you want to see, uh, happening and that can increase the productivity, not replacing human, but uh, empowering the human factor. Yeah. So what, what, what, what would be those ideas or list?  
**Speaker 1** 00:28:19:  Yeah, I just gave you the first one. I think low hanging, lowest hanging fruit would be how can, um, incomplete data become more complete quicker so that the point of insight creation can be earlier in the process and you save that time. Basically. If we can turn around content insights faster, I think that would be relevant. Um, and also that, you know, that would just make it a more time efficient process. I think that one other interesting area is, um, we, our content is consumed in every country in the world, but every country in the world is culturally very different to each other. Some of them are much close other, and that's a big challenge, which is if we can get to a consistent way of assessing across markets around, but within the context of the culture of that country. Like that's, you know, there's a lot of work that basically has to be done at a country level, at a local level, 'cause of the nuances in that market. I imagine if there could be a, um, AI tool that allows us to create more of a level playing field where mm-Hmm, \<affirmative\>, you know, we can model out how something might look across multiple markets, but taking into account all the factors that exist in that market like that doesn't feel like that exists. Um, and the other one is  
**Speaker 1** 00:29:55:  Age old problem about, um, box office and theatrical windows. Um, so the way that the whole of the movie industry works is from hitting the cinema through a number of different windows and, um, the windows where consumers can opt in to buy that content or watch that content or rent that content or, you know, get a DVD of that content. And, um, I don't think that the film industry has, you know, I think there's more to, to more to learn for the film industry there, basically overall,  
**Speaker 0** 00:30:35:  Right.  
**Speaker 1** 00:30:36:  To make that a cleaner process and to maybe to model out in a more advanced way.  
**Speaker 0** 00:30:41:  Mm-Hmm. \<affirmative\>. Alright. Thank you very much, Patrick. It was really interesting, uh, chatting with you. I learned a lot. Uh, do you have any questions for me?  
**Speaker 1** 00:30:53:  No, I don't think so.  
**Speaker 0** 00:30:54:  Okay, great. Uh, thanks again and, uh, have a nice rest of the day. Cool.  
**Speaker 1** 00:31:00:  No, sorry, I did have one question, \<laugh\>,  
**Speaker 0** 00:31:02:  Sorry.  
**Speaker 1** 00:31:02:  Which is, do I, um, do I get sent the Amazon voucher on, um, LinkedIn or do you send it to my email or what?  
**Speaker 0** 00:31:10:  Uh, I, I, i, I, I will follow up about that soon, uh, today. Yeah. Okay.  
**Speaker 1** 00:31:16:  Appreciate that.  
**Speaker 0** 00:31:17:  Thank you very much. A.

# Georgios Papadopoulos 

Insights & Analytics @Reckitt | Research Aficionado | Consumer Advocate

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:01:  Thank you very much for joining. Thanks a lot for your time. Um, my name is gri. I'm, uh, a product manager, uh, in research and insights. Mm-Hmm. \<affirmative\>. And, uh, yeah. Uh, we are from this company, a startup company called bes. And, uh, we have two other participant here. Uh, Dino is our ux, a UI designer, and yeah, Ida, uh, that's our, uh, note taker, uh, developed by our company. Mm-Hmm. Do you mind if Mm-Hmm. Uh, it stay? Can you, can it record? Are you okay with that?  
**Speaker 1** 00:00:34:  I'm fine with recording.  
**Speaker 0** 00:00:36:  Thank you very much. Uh, so yeah, basically, uh, we are trying to understand, uh, the research, uh, workflow in market research, uh, in particular about qualitative research and, uh, trying to understand that how ai, uh, can change and, uh, or improve various steps in that direction. So we will just ask you a couple of questions, uh, uh, along that line. So maybe, uh, do you want to start by, uh, introducing yourself and telling us a little bit more about your day job or what kind of work you do? Uh, yeah.  
**Speaker 1** 00:01:12:  Yeah. So my name is George. I currently work for a company called Racket. Mm-Hmm. \<affirmative\>. Um, uh, even though I'm currently a gardening lead, because I'm, I'm moving on, I'm moving to a competitor, uh, to a similar role. I'm the insights lead at the moment for the UK business, um, for the health business. And I'm looking more into tra what we call traditional insight. So including qualitative, but also quantitative data and many, many more. Um, and I feed largely into the strategy planning. Um, but equally, I work on innovation. I work on comms, uh, all the way from big idea stage to, um, you know, development, execution and, and measurement, post execution. So I've got a very multifaceted role when it gets to insight, uh, and representing the consumer. Um, with qualitative being a chunk of, of what I do now, I manage a budget, so therefore I work with research agencies, uh, on many, many occasions. And, and we have, you know, you know, not as much transactional, but rather more partnership, uh, type of relationship. Mm-Hmm. \<affirmative\> when we work towards, uh, the completion of a brief, the delivery of a project, et cetera. And, and I've got, um, I've worked in the industry for 13 years now, or 14\. Um, and I've worked on both client side and, and research side. So I was ages ago. I used to be Nielsen basis, which was more on the quantitative front.  
**Speaker 0** 00:02:38:  Right. And thank you very much. Yeah, that sounds very interesting. Uh, so when you, uh, start a new project, uh, let's say start with qualitative research, when you're planning something, can you walk us through your journey? Like how do you plan it, and then how do you recruit the participant? How do you collect data and how do you analyze it?  
**Speaker 1** 00:03:02:  Yeah, so as I said, largely things are, you know, very rarely do something qualitative on my own, mostly due to to time capacity, right? Which means I typically work with a research agency. So, uh, quite a lot falls down to what the methodology is going to be because, you know, the type of qualitative methodology changes drastically, um, how I approach the work that I do. Right. But if I could put it very loosely to cover everything, and I'm happy to go into more detail, um, things typically start with a brief. Um, so I'm embedded into the different marketing teams I work with Mm-Hmm. \<affirmative\>, and we kind of coauthor the brief. So we write down the things that we believe we need to learn, um, you know, the knowledge gaps, what's the actual market, market research objective, the business objective we we're trying to achieve, which could be, for example, the dry penetration or to, uh, steal from a competitor here and there, or what, what have you.  
**Speaker 1** 00:03:56:  Um, and then what I typically do is I send the brief to at least one research agency. Um, I, I think I'm typically really good at pinpointing the right partner. Um, and if, if the cost is not too significant, so say if the cost is under 50,000 pounds mm-hmm, \<affirmative\>, I might just reach to one agency. So I don't even reach to, to more than one to pitch. Um, then there is a tiny bit of a back and forth typically, uh, with a proposal, with a recommended approach, um, et cetera. And once I have a proposal, I then typically move to a kickoff call, uh, where I will have someone from the marketing team, which would be my internal, say, stakeholder with myself and the research agency. So I try to connect the two, we, you know, we, we try to bring the life, the brief to life into this particular group and, uh, this particular call.  
**Speaker 1** 00:04:52:  And then, then pretty much that's it. Then it's typically the agency who does the rest of the work in terms of doing the right recruitment, uh, running the, the qualitative groups or the consumer connects on the online groups, whatever that might be. And then there is a debrief coming up now through this latter bit of the process. I work with them in terms of reviewing, um, the screening, uh, the, the screeners or the screening criteria, and then the discussion guides if there is one or the tasks that we ask consumers to do. And then when, when I see the, the report, I typically feedback to the report, and I try to make sure it's, it's all the right quality, it answers the key questions. Uh, there's nothing that contradicts the internal business story that we have, et cetera. So that is in a very natural, obviously, some of those pieces of research could take up to four weeks, five weeks sometimes. Mm-Hmm. \<affirmative\>, um, you know, so I try to condense everything into a minute.  
**Speaker 0** 00:05:52:  Yeah. Right. Yeah. Thank you very much. And, uh, in this whole work, even, even though like most of the work is, you know, you are delegating to the agencies. Yes. Uh, can you gimme some examples of, uh, uh, at which steps, uh, AI could, uh, facilitate your whole journey? How, how it can help you? Or have you been already using AI at different steps? Or are you aware that your agency's using?  
**Speaker 1** 00:06:18:  So, whilst I cannot tell you about agencies that I've used or, or be using, because quite a lot of those are contractual to record, what I can say is, um, where I find AI can or does help already Mm-Hmm. \<affirmative\>. So, and, and whilst I cannot pinpoint agencies that do that, I can talk to you about the capabilities already. So I think the first bit is, so translating the brief into the right discussion guide is definitely one and the right tasks. So there is at least one agency that I'm aware of where you input the brief, um, as you've written it, um, you know, internally, and then it spits out the discussion guide. So it gives you a questionnaire effectively. And then what you have to do as a researcher is to, to go through and kind of, uh, maybe make amends, but largely it's doing quite a lot of the work of, say, a junior analyst who would put together the first draft of a discussion guide.  
**Speaker 1** 00:07:19:  So that's definitely something that I've seen. Um, another thing that I can think of is when it gets to qualitative insights relating to, to consumer connects and videos. So, you know, when we say qualitative research, obviously sometimes it's consumer connect, so one-on-one. So what we do sometimes is moderated consumer conversation. So I typically pay a moderator to run them, and either me or some of my colleagues, we are in the background and we are listening in. Um, but once, say you run a number of those, say you run 15 of these on a particular topic, then there is another AI tool which, uh, kind of analyzes the transcript and it's pulling out, uh, the key themes. So on this question, consumers discussed 1, 2, 3, 4\. So it's almost writing a report. It's not writing a report per se, but it's pulling out the key themes that it thinks it's e listening from analyzing the consumer content.  
**Speaker 1** 00:08:18:  Uh, so that's definitely another one that I've thought about. Um, another one that I'm very fond of, um, is not related a hundred percent or qualitative, but it is qualitative, qualitative in nature, in what sense? So, you know, in a quantitative survey, uh, there are sometimes open-ended questions, right? Which are more qualitative in nature. So you ask consumers to type, say what they like, what they didn't like to explain an idea, et cetera. And then there is an AI algorithm that when the consumer types something, the algorithm is prompting on what they typed. For example, the question is, what did you like about this innovation? Mm-Hmm. \<affirmative\>. And then they write, I don't know, uh, the taste, just making this up. And then the AI would ask, okay, but why do you like the taste? Because X, Y, Z, and then why do you think X, Y, Z? So it's drilling down to get into something more rich and concrete than inside. So that's another one that I've seen and I'm very fond of. Um, these are some, some first thoughts that I can think top of mind.  
**Speaker 0** 00:09:22:  Right. Th th Thank you. Yeah, I totally understand that. Uh, you cannot name the agencies or the contractual bond, uh, but about these AI tools, uh, do you remember the name of these AI tools?  
**Speaker 1** 00:09:33:  No, I wouldn't be able to share.  
**Speaker 0** 00:09:35:  Oh, even the AI tools. Okay. Right. Uh,  
**Speaker 1** 00:09:40:  But, but, but if you Google them, I'm pretty sure you'll find them straight away.  
**Speaker 0** 00:09:44:  Okay. Right. Yeah, no, uh, they're like a lot of, uh, different kind of tools available in the market. Uh, but what we are trying to build here is, uh, uh, trying to, uh, use AI for various steps of the market research and trying to give to customers to us through the single window platform. So they don't have to like, go to a different platform for each of the steps. So they can just do whole of their, uh, process at a single place.  
**Speaker 1** 00:10:11:  Gotcha.  
**Speaker 0** 00:10:12:  Uh, so when they're, when the agencies are collecting data, so you tell them that, uh, what kind of data collection method they have to use, or do they decide by themselves? Like simple interviews, surveys, or video coding? How, how, how, uh, you think that AI can also help you with your data collection journey?  
**Speaker 1** 00:10:33:  Mm. So when you say data collection, I understand the methodology. So what sort of methodology we would use? So videos or, yeah. \<inaudible\> journey or focus group or whatever. But typically that's a decision that I take based on my experience and what I think is more relevant. So that's layer one, um, more relevant to the question or what I think is more appropriate. But then there's another layer to that that's really cost related, right? So if I don't have a lot of budget and it's not a high priority, I might set them to one of the agencies that do video captures, right? Like the likes of, I don't know, watch me think, for example. Um, or if we want to run consumer Connects, um, and I say, look, we don't really have budget for this project, but I have a little bit of credit with discuss io.  
**Speaker 1** 00:11:20:  So you can run the consumer connects on your own to validate or to get some input on X, Y, Z topic. Now when it's something bigger, so if I want foundational insight, for example, then I might decide to go on a consumer community type of thing that stretches in the course of, I don't know, four days, for example. And there's different tasks. Mm-Hmm, \<affirmative\>. And that's one that I can think where I haven't seen such a tool, but potentially an AI tool could help with to structure that journey in a consumer community. And the different tasks that people run, excuse me, run every day. And also on that, there's typically a moderator who monitors a few times every day what the responses were. And then they make prompts. So, why did you say that? Why did you like that? Oh, that's fantastic. Tell me more. You know, those kind of things, which an AI tool should be able to do as well.  
**Speaker 1** 00:12:10:  Um, but obviously the consumer community, because it's so rich in terms of what it produces, so diverse with the different tasks, I haven't come across an AI tool that could run analysis on it in the sense there's so many different tasks and different questions and the very different nature. So I dunno if an AI could put things together into a coherent story. Um, and then the other bit is obviously the more traditional, um, qualitative focus groups, for example. Mm-Hmm. \<affirmative\>, uh, sometimes we do those online, sometimes we do those offline. Um, but I find in order to, to get people to talk and to open up and you see their facial gestures and the way they move as well. 'cause all of that is part of the insight that you gather. I think that they're a moderator, so someone who's physically there is imperative. And I also find that these people typically build the story as they go through the process to say they've got four groups to run after the group. One, they already have formulated a little bit of the story, and then they try to see recurrences as they go through the next groups.  
**Speaker 0** 00:13:19:  Yeah. Right. Yeah. Thank you very much. And, uh, so you, you, you mentioned that, uh, you get all, uh, agencies are collecting data then, uh, you're getting just reports and then you are using it to talk to, um, other stakeholders like \<crosstalk\> or different kind of people who are probably from management letter. Yes.  
**Speaker 1** 00:13:42:  And, uh,  
**Speaker 0** 00:13:45:  They're giving you the final reports, or still you have to do some data analysis by yourself above that qualitative analysis?  
**Speaker 1** 00:13:52:  I typically get the first draft, yeah. And then I review the first draft until it reaches the point where I'm happy with. And then typically the agencies, um, deliver the debrief so they come and present on their own, um, the report that them and myself have worked together towards building Mm-Hmm. \<affirmative\>, once that is done, sometimes there might be some additional story to be told internally. Uh, so we might need to create a different summary and executive summary of some sort as we play up some of the things that we've learned. Um, but that's where I don't use the agency anymore. I work with a team in terms of what are the arguments and the story that we want to bring internally and to escalate further. Um, so, and I, I typically don't have to do further analysis on my own. If there are questions which sometimes come up during a debrief, for example, they go home, they review what they've learned, uh, they might have to listen to some of the transcript again, et cetera, and then they get back to us with another slide or so, or with an email of some sort. So there are follow up sometimes.  
**Speaker 0** 00:14:57:  Yeah. And, and if you have to do data analysis by yourself, uh, can you tell me the name of some, uh, tools that you might use for qualitative analysis?  
**Speaker 1** 00:15:06:  Well, do you say quantitative or qualitative? Uh,  
**Speaker 0** 00:15:09:  Well, you can go both.  
**Speaker 1** 00:15:11:  Well, there's, there's a lot that I do in DY in terms of quantitative. Mm-Hmm. \<affirmative\>. So, but I use, you know, the platforms that everyone is using. Right. So say to Luna is one, um, the Ipsos platform is another one. Yeah. Um, these two are the ones I use quite a long, along with Zai, which obviously, you know, it's got semi-automated research tools. So you don't really do much, you just click a few buttons and then you send something live and then you get the data from that. Um, but these three, I think are the ones that everyone is using. Then there's others. There is, um, another one that's called apio, for example. There's plenty that are, you know, quite similar and they do the same thing. So for me, it's a matter of, um, buying at scale. So you get cost efficiencies and just use the one that you feel more comfortable using, because there's a different way that the different, uh, of these platforms visualize data, for example.  
**Speaker 1** 00:16:08:  So it's, there is a matter of personal preference on how you want to be seeing things, but in terms of value extracted, all of these are the same in my view. Uh, so you, I can work with any of these. And then there's discuss io, which I really like. Mm-Hmm. \<affirmative\>, uh, which I do, I guess you probably know them. They do consumer connects. Um, they do quite a lot on the AI front if you want to, to visit their page and see what they're doing. And they're piloting quite a lot of things. Um, and then there is, watch me think, which is more on the video type of research. So you kind of formulate the questions. And then there are consumers, so the questions get sent to consumer, and for every question they record the 30 or 62nd video of themselves responding to the question. So then right now, how it works, and at least all I know about how it works is there is an analyst at the back end who's looking at the videos, and then they're trying to put together one slide, I don't know, a question of some sort. Um, but there is a DRY element to it. So you don't need to use a moderator. Uh, they have a, a platform and you can, can just use the platform on your own. I tend not to, but I'm sure other cmis do that.  
**Speaker 0** 00:17:19:  And when you say that, uh, they're automatically analyzing this, some of these short videos, are they just giving you, uh, video transcription summary or presentation of research data, or they also run sentiment analysis?  
**Speaker 1** 00:17:33:  Yes. They, I dunno about wordsmithing. Most of these agents, they do have some sort of a sentiment analysis. Uh, but they, in these particular cases, they typically pay for a moderator. So someone, sorry, for an analyst, not a moderator, someone is actually listening to all of these, and then they write themselves a summary in a PowerPoint format of some sort.  
**Speaker 0** 00:17:55:  Uh, really? So, so it's not fully automated or AI  
**Speaker 1** 00:17:58:  Enhanced? No, no, no, no. I didn't say it's automated. Oh, okay. I said it's do y So what, what sometimes you can do is you can load your own question. So either you send a brief and then you pay for, they call it consulting time. So you pay them a fee for them to pick up your brief, write the questions, field the study, do the analysis, or you can do it on your own. So you log in, you type your questions, you choose the type of sample and all of these things, and then you just get the videos back, basically. And I think there are some tools embedded that do, for example, sentiment analysis and things like that. Um, but I haven't used that in age, so sorry, I don't remember really well,  
**Speaker 0** 00:18:41:  Uh, no, no worries. And so, so these were the, like, uh, tools existing in the market and what were they doing? But, uh, hypothetically, uh, what are your thoughts? What, what, what are your thoughts on AI's role in this whole data collection and data analysis of qualitative research or even quantitative research? What do you think? What's missing there, uh, that, uh, that, that, that you would love to see?  
**Speaker 1** 00:19:08:  Sure. So there's two things, right? So the first, in terms of how I view it, um, it's not there to replace anyone. It's there to facilitate things to make them easier. Uh, I don't think, for example, that the analyst, there's no need for an analyst because the AI is going to run the analysis. I think the AI should facilitate the analysis that the analyst is providing. I'm very adamant about it. And that is coming from the reliability front. 'cause from everything I've seen so far, I haven't seen an AI tool that works a hundred percent reliably. So when you run a sentiment analysis, it's working at the back of words that it's hearing. I can give you an example. We recently run a campaign on from the brand neuro fan, which is called See My Pain. So the role of the campaign, it's a purpose campaign, right?  
**Speaker 1** 00:20:00:  So what we're trying to, to do is to showcase that. So the insight is that women, during their interactions with healthcare practitioners, they have a more negative outcome because more often, unfortunately than not, uh, the pain they feel is dismissed as more emotional. And it is what I'm not, I'm not going to get into the detail, but I'll get to the meat of it, right? So, so the meat of it is that when you run a social media analysis, what the algorithm is looking at is, is words. So if women are talking in relation to the campaign, which is called See, my Pain About Pain, it's negative, right? Because it's pain. So it has to be negative. However, they might be talking about the campaign in a positive fashion because we're driving awareness of a topic that's important, but what the analysis is showing is that it's negative.  
**Speaker 1** 00:20:48:  Mm-hmm. Right? Yeah. And then if you look at the volume and the content of conversation over time for the neuron brand, you know, suddenly you start seeing it a little bit as more negative, but it isn't negative because people are more negative towards the brand. It's more negative because the AI thinks that because suddenly more words around pain are being used or discomfort, then there must be something negative going on with the brand. So what I'm trying to say is that the ai, at least from what I've seen to the day, can't quite replicate the human analysis and the human experience, right? Because there's a lot of, a lot of faults still going on, but it's there to facilitate it for sure. As the, for example, the, the capabilities that I described in the beginning, um, about, for example, the AI being able to write a survey or write a discussion guide at the back of the brief, um, then you'd still need to get in to say, okay, is that correct?  
**Speaker 1** 00:21:45:  Now is the flow right? Are there all of the questions in, is the formulation of its question correct? So it's there to facilitate. So for me, that's definitely one thing. As a CMI, for example, I'm pretty sure there are projects that I run and I pay a consulting fee to someone. I'm not talking about the huge big projects where I'd need focus groups, for example, but the most small ones, right? Yeah. The ones that are not the most important ones necessarily, but I pay a consulting fee, which could be 2, 3, 4, 5,000 pounds to an agency, uh, to spend time on my project. But if I had the AI tool that could write me the discussion guide, for example, and could at the end pull some key insights for me, which meant that I only need limited incremental time on the project to run analysis, to write maybe a few slides, I don't know, something like that, then I would definitely consider it.  
**Speaker 0** 00:22:36:  Right. Very, very interesting. Thank you. And, uh, so at the end of the data analysis, when it comes to data visualization part, uh, can you tell me what are the, like, uh, what are the most common ways to pre represent your data? Do people tend to read just the summary, uh, of the conversation, the list of questions, or they want to see something like charts, graphs? Or, or, or is there any other way?  
**Speaker 1** 00:23:03:  Let me, let me take a step back. So firstly, there's a bunch of, of online tools and dashboards where you run the project in the dashboard, and the data is presented in the dashboard in any form. What I've found again and again in my career is that it's really difficult to get people to use these dashboards and to get used to them. And at the end of the day, I'm always asked for an offline report in this role, in my previous roles, in my, you know, so it's really difficult to get a marketing team to log in and, or, you know, to send a link to them and they click on the link and then they see the data in the dashboard, et cetera. The, the problem is one, obviously it's inertia just because they're used to, to PowerPoint. PowerPoint is the dominant format, by the way.  
**Speaker 1** 00:23:44:  It's a hundred percent PowerPoint in every company I've worked in so far. Um, so the inertia is definitely one. But the other bit is the fact that even if, say you're a little bit younger, you're used to dashboards and stuff like that, when you need to push things up the chain, they definitely need PowerPoint point presentations. Uh, because the research that might create, I'm making this up, 70 slides. When it gets presented to the management, there will be one slide or half a slide, sometimes in, in a deck of 20 slides that take a lot of different things into account, right? Yeah. So the research rarely makes it to the management. So at some point they will need PowerPoint format to pull maybe, uh, verbatim from here to pull a chart from there to pull, I don't know, something else from somewhere else, the table and then put them in the wrong text.  
**Speaker 1** 00:24:33:  So PowerPoint format is imperative in my view. Uh, and, and maybe to give you a frustration that I tend to have is if the online dashboard is really slow or get stuck when I want to download the very same charts that I see in a PowerPoint format, that is typically something that annoys me quite a lot. Yeah. Um, and I've seen glitches and difficulties in all sorts of dashboards that are, that I'm using at the moment. Um, so yeah, so in terms of visualization, I'd say PowerPoint is imperative. And then depending on your level in the hierarchy, uh, visualization is really important. So when you deliver a story to the, to the marketing team, say to the marketing manager or the, the senior brand manager or whatever, typically a deck which visualizes data in charge in a nice visualization way, I don't know, with images, et cetera, to bring the story to life tends to work more often than not really well. Uh, but when it gets to further up the chain, something simpler tends to be needed. Mm-Hmm. \<affirmative\>, uh, but I find that people just put that on their own together.  
**Speaker 0** 00:25:41:  Right. Yeah, that's a very interesting point indeed. Thanks. And, uh, like, like AI is, uh, a use is increasing now day by day. Uh, from your perspective, do you think any ethical consideration that people need to take care of with, uh, when, when these use of AI is increasing and we should be careful?  
**Speaker 1** 00:26:04:  I think the reliability of it is largely the one I can thing. I'm pretty sure, you know, GTPR, those kind of things. I'm pretty sure someone is going to look into those. 'cause obviously, you know, working with research, you work with a lot of people, data. Mm-Hmm. \<affirmative\>. So, you know, there's a lot of, you know, contracts that we sign, um, to protect the personal data. So to give you an example, if you've taken a video or a photo of someone talking about the product, obviously that needs to, that cannot get distributed or get out of the organization. You can't use it in an advertisement or so, so any AI tool, I think you'd have to make sure that is, is bulletproof when it gets to, to, to data protection and, and storage of data and all of these things. I think definitely that one. Um, so that's definitely one.  
**Speaker 1** 00:26:54:  Um, and, and there is another bit, which, which I came across really recently, um, because, so I'm using an emotional AI type of tool at the moment. Um, which, what, what the, what the agent, it's an AI powered research. It's not an AI research per se, but what it does is that, uh, you have consumer conversations like one-on-ones, someone is running those, and then the transcripts are fed into a machine. And then the machine, the AI machine is, is finding out, is trying to figure out emotional levers attached to specific words or phrases. So it's an AI powered, but human led type of research. But it's what you put in is the content and the data, let's call it, that the AI machine is going to work with. So I wanted to put the transcript from consumer conversations that I had from a different research agency.  
**Speaker 1** 00:27:45:  And then the obvious question from the other research agency was, I need to make sure that this data that I'm providing you are going to be used for this particular project only, and they're not going to be used for the AI machine to improve itself in order to better work on someone else's project. And I thought, well, that's, that's such a a no brainer if you think about it, because if I'm paying you and I run the research with you, a project with you, I I want the value that I'm paying for me because I don't want your AI to learn from me and then go work with someone else and work better. You see? Yeah. So for me, I dunno if that's ethical, but there's definitely a competition thing going on there that I've, I've came across, I've come across.  
**Speaker 0** 00:28:29:  Right. Thanks. Yeah, no, that's very interesting point indeed. Uh, and how in, in your whole, uh, job, like, uh, how would you define that? How much qualitative or how much quantitative research is involved?  
**Speaker 1** 00:28:42:  That changes by role, obviously, right? It changes on the type of role that you do. Um, I, I wanna say 50 50\. I'll just kind of make that, make that up. Um, but it depends on the role. There are some roles that are more, and some, and some companies that just have this culture on, or more qualitative. Uh, there are companies that are more into validation and they do quite a lot. They don't do that much, you know, upfront in insight. Uh, I'll say 50 50 for the sake of the argument between quantitative and qualitative in terms of how much I spend at least.  
**Speaker 0** 00:29:16:  Yep. And, uh, uh, do you use ai, uh, to facilitate your quantitative research? How much it's impacting there?  
**Speaker 1** 00:29:29:  I don't use that much on quantitative yet, to be honest. Mm-Hmm. \<affirmative\>. And the reason is there are, as I said, some really cool tools out there, but I'm very skeptical on the reliability of the output for me to try them yet. But, but definitely I am willing to. So if I had, you know, spare over budget at some point and I want to spend it somewhere, I, I would wholeheartedly give it a go to be honest.  
**Speaker 0** 00:29:57:  Right. Yeah. Thank you.  
**Speaker 1** 00:29:59:  And also, and also frankly, there's AI and AI because everything is called AI these days, but not everything is ai, right? Yes. So everything is getting baptized as ai, but it's not necessarily ai. Yeah. So there are, you know, there are tools that they've been there forever, right? Like, um, a algorithm for example, that picks up consumer verbatims from a quantitative survey and gives you work clouds, you know, now that's called AI these days, obviously it's not an AI tool, it's a basic,  
**Speaker 0** 00:30:25:  It's basic permission.  
**Speaker 1** 00:30:26:  Exactly. Exactly. So it depends what you mean by AI as well, but everything is AI these days.  
**Speaker 0** 00:30:33:  So yeah, like 10 years ago everything was big data. Now it's a yes. Yeah. Even, uh, yes, uh, automation or just, uh, bots are also, uh, dub as AI now. So yeah, I totally agree with you there. Uh, okay. Uh, thank you very much. Uh, it was really interesting, uh, talking to you. I've learned a lot. Do you have any, uh, questions for us? Yeah,  
**Speaker 1** 00:30:56:  So, so what you guys are trying to do is build a new capabilities. So you're talking to people like me to find out what needs and gaps?  
**Speaker 0** 00:31:03:  Uh, yeah, we're just trying to now, uh, improve what we already built. Uh, so it's a startup company. Uh, the idea is that we have our own video cutting tool, just like, you know, meets, uh, Google meets our teams. Uh, so we can record our media, uh, video on our platform. Uh, then our chat bot will give you the video transcription and the summaries, and uh, then, uh, you can talk to your repository. Uh, you can ask questions based on that particular project. And, uh, uh, other than that, we are trying to, uh, improve the data analysis part as well, uh, so that people can just download, like you said, that when you go up to the ladder, uh, in the management role, uh, people don't want to say the dashboards, they want to see like single PowerPoint slide. Yeah. So you can, uh, convert your data analysis according to your target audience, and you can download it quickly. So we are building something like that, and not just data, uh, analysis, but we are building from scratch where you want to plan your project, you're planning your project, uh, right on our website, you're recruiting the participant through our website. Then you have the video recording tool there. So basically all the major steps in qualitative research, uh, can be performed through our website, so you don't waste time on going to do different tools and trying to learn different tools. So everything would be available through single platform.  
**Speaker 1** 00:32:38:  Nice.  
**Speaker 0** 00:32:39:  So yeah, with the, with your heart, we are just trying to, uh, improve. Otherwise, for the last three years we have been working on the development. Oh, wow. And, uh, the, uh, the company officially started this year.  
**Speaker 1** 00:32:50:  Congratulations. What's your, your company called again?  
**Speaker 0** 00:32:53:  Uh, beings, uh, beings. Like human beings.  
**Speaker 1** 00:32:57:  Beings, yeah. Beings. Yeah. Being,  
**Speaker 0** 00:32:58:  Yeah, yeah.  
**Speaker 1** 00:33:01:  Noted. Cool. And I'm getting an, an Amazon voucher today.  
**Speaker 0** 00:33:06:  Uh, yeah, I will follow up that soon.  
**Speaker 1** 00:33:08:  Amazing. Thank you very much. And yeah, let's, let's still keep in touch. Yeah. If you guys need anything further input from me, of course, drop me a note.  
**Speaker 0** 00:33:17:  Yeah, that will be very interesting. Thank you very much. Have a nice rest of the day. Thank  
**Speaker 1** 00:33:21:  You. Have a lovely day. Bye.  
**Speaker 0** 00:33:22:  Bye.

# Vilma Xhakollari 

PhD, Research fellow and Market Research Analyst Reading University 

## Meeting summary: To be filled 

## Transcript:

**Speaker 1** 00:00:41:  Hello?  
**Speaker 2** 00:00:43:  Hello? Can you hear me well?  
**Speaker 1** 00:00:46:  Yes, I can hear you.  
**Speaker 2** 00:00:48:  Uh, so how, how do I pronounce your name? Is it Wilma?  
**Speaker 1** 00:00:52:  Yes, correct.  
**Speaker 2** 00:00:53:  Hi, Wilma. I'm, uh, Gure. I'm a product manager from, uh, this company called Beings. Nice to meet you. Yeah, really nice meeting you. Thank you very much for your time. Uh, so yeah, Dave will not be joining us. Uh, he's the CEO of the company and he got another, uh, meeting, so it'll be only me now. Mm-Hmm. Uh, I have another participant here call Data. It's our note taker. Do you mind if it record the meeting?  
**Speaker 1** 00:01:19:  Yeah, no worries.  
**Speaker 2** 00:01:21:  Thank you very much. So, uh, yeah, maybe you already have a little bit of a overview of the talk that, uh, we are basically trying to understand that how resources are using AI in their whole research journey. So, uh, can you please, uh, tell us, uh, like the overview of your job, and then I can start asking a bit more tailored questions. So like, uh, what, what do you do now or what's your daily job and, uh, what kind of, uh, uh, uh, work you're doing related to market research or qualitative research?  
**Speaker 1** 00:02:01:  Uh, so at the moment I'm a postdoc at the University of Readings, school of Agriculture Policy and Development. Mm-Hmm. \<affirmative\>, I work, um, on consumer behavior studies. Um, usually what I do is I design questionaries. Um, I analyze the data, uh, using different, uh, softwares, but mainly I've been using r um, for quantitative studies. I also conduct, uh, qualitative studies and usually, uh, the analysis. Uh, I mean the, the reason that we do qualitative studies, or at least the reason why I do qualitative studies, is to understand how can I design the quantitative studies. So, usually I've, uh, used like, um, content analysis for analyzing, uh, qualitative studies. And, uh, sometimes I've used envivo in order to produce like a word maps and so on. Mm-Hmm.  
**Speaker 2** 00:03:01:  \<affirmative\>.  
**Speaker 1** 00:03:02:  So, um, I also do like, uh, literature review, um, using different databases to, uh, identify different articles. And I also, um, present, do presentations with students, for example, because I also teach sometimes or follow students with their, um, thesis. Mm. So more or less, yeah. This is what I do in very few works.  
**Speaker 2** 00:03:33:  Very, very good. Thank you very much. Yeah, that sounds very interesting. And, uh, so when you said, uh, are you talking about the art programming language? Yes. Okay. And, uh, so in this whole workflow, uh, have you been using AI at different steps or how are you using, uh, to facilitate your different steps of your research?  
**Speaker 1** 00:03:57:  Yeah. Uh, I have used umif, uh, ai, um, like the, I started using ai, let's say like almost six months, seven months ago. Mm-Hmm. \<affirmative\>, um, or eight. But anyway, uh, let's say autumn time. Right. Um, I have, uh, at first I've used it, um, when I was, uh, facing like a white paper and I wanted to start writing something about, like a research or about an article, for example, just to have an idea on what were like some bullet points, let's say. And then of course I was developing it myself. And then I had, um, some, um, I was, uh, designing a questionnaire in Qualtrics. And what we wanted to do actually was to, um, create, um, like, um, purchase scenario. And, um, unfortunately with, um, Qualtrics, um, function that was not straightforward. So we had to use JavaScript because Qualtrics actually, um, supports JavaScripts, but doesn't give any, uh, support on how to design with JavaScript.  
**Speaker 1** 00:05:11:  So that was a very difficult moment. So I had to, in a way, rely on ai, but I was not satisfied with, um, um, with the outcome because was not, um, I mean, the, the solution it was giving was not, um, addressing my, my problem. Yeah. So I must say I lost time with that \<laugh\> and, um, I didn't know to, you know, to, to have like a, a solution for that. Um, I have all, and so we used other, uh, other techniques in order to do like what we wanted to do actually. Um, I've also used it for coding in r like, um, um, when I had like, um, like when I already had a code, but I wanted to maybe modify it or understand it a little bit better. I've used it. Sometimes I might say it has given me some, some results, like good results, but sometimes has made me lose time.  
**Speaker 1** 00:06:22:  So I think probably the problem is, was here was that it, I might need a more, um, how they say like a, a more specific program, which is specifically for our coding or like coding in general. Um, so I might think that probably it's important to defer this different ai, um, softwares maybe. But as a user, I wouldn't prefer it because I would prefer to have like, um, one app that I can use for different stuff, uh, because apps are, uh, used differently. So, um, so yeah, this was my experience, um, and sometimes I have used it, but not for work, but to get some information about different topics, but nothing related to work.  
**Speaker 2** 00:07:19:  Right. Thank you very much. And can, could you please tell me the names of those AI tools?  
**Speaker 1** 00:07:24:  I used chat GPT mostly, and I've used another one, which actually I forgot to say I've used it. Um, I had like a paper and I wanted to do like a proofreading of the paper, and I've used another one, which was actually really good, but I, I can't remember now the name or something, like, kill Something. Um, I, what was that? I can't remember actually at this moment. And that was good because it was giving you like, um, uh, the, the, the formatting, I mean, the text, or let's say the con the content of the text that you were, that you had was being, um, changed based on the academic or if you need it, like just for students or like, uh, to just for general public. So it had some features that could, you know, change also. Um, I mean, it didn't change the content, but let's say the, the colors of the words that I was using. So, but I don't remem I can't remember now the name of it.  
**Speaker 2** 00:08:34:  Okay. Uh, no worries. And, uh, when you, uh, mentioned what, uh, Qualtrics that you were unable to, they, they were not providing you right. Interface to use JavaScript there. Could you please tell me a bit more about the use case? What exactly Qualtrics were not able to do, and you needed JavaScript to do that?  
**Speaker 1** 00:08:53:  Yeah. Um, so what we wanted to do is like to mimic like a purchase scenario. So, uh, an online purchase scenario. So we wanted people to see, um, like we had a list of products and we wanted people to see, um, how their basket was changing when they were clicking. Not only the basket was changing, but also the total price that they were spending. Um, so, but Qualtrics couldn't manage to have that. I mean, we wanted everything in one page. Yeah. Um, not like in another page, like the total price we wanted in one specific page, but then we didn't, I mean, we didn't have much time to lose. I mean Mm-Hmm. \<affirmative\>, nobody knew I used, uh, Java script before. So that, that's why it was a bit complicated also to achieve it. And I mean, uh, what the problem was that, um, the code that we were obtaining from, from artificial intelligence was okay for the, let's say, as an interface, but then it was not very good for, um, registering the data.  
**Speaker 1** 00:10:04:  So that's where we had the problem, because like, um, I mean, the way was, uh, showing the information was perfect for us, but then was, was not being able, the, the code was not being able to register, uh, the information Yeah. Like the \<inaudible\> that participants were making. Um, so that's why we, we left it, uh, we, we, we didn't find, uh, somebody that also helped us to, to do this. So we just, uh, continued, uh, with our, and with, uh, Qualtrics, sorry. And unfortunately we, we found like a way to, to mimic, uh, purchase, uh, scenario. Not very smart way, of course, with the features that, um, Qualtrics could provide. But then we, we managed do it actually.  
**Speaker 2** 00:10:54:  So, so then you did it everything by yourself, by writing, coding by yourself, and did the outside Qualtrics?  
**Speaker 1** 00:11:01:  Yes.  
**Speaker 2** 00:11:02:  Okay. Yeah, that sounds a lot of work, \<laugh\>.  
**Speaker 1** 00:11:04:  'cause it's, it was actually, um, and also as I mentioned before, sometimes when I've had like, um, uh, there was also a couple of other times when I had like, um, uh, some, um, some coding in r and I wanted to do like some analysis, like for example, a diff like to add, um, because R has different packages. So I wanted to understand better, uh, each, um, uh, function of the, of the package. And I wanted to add it to, to the, to the code. But then while doing, um, I was not understanding, so I was asking, uh, AI to GBT in this case, but was not giving me like, uh, adequate answers because I was comparing. Um, but I've realized that if you have basic, um, coding requirements, it's very good. Yeah. For example, I supported one of the students, um, for their thesis, and they had a very basic, uh, descriptive analysis, let's say. So it was very, very well, uh, very good for them to also to understand, um, uh, the code, because actually it was explaining if you were asking it to explain what each of those parts of the code was, um, meant. Uh, so it was explaining everything very correctly, but just for basic stuff, not, like, if you want, in my opinion, if you want to go deeper into a code or something, or you need a specific, um, like a AI app, or you need, I mean, other, other support  
**Speaker 2** 00:12:47:  After that, after that experience, uh, did, did you ever find any other tool that could, uh, where, where you can use AI for quantitative research as well? Or where you  
**Speaker 1** 00:12:57:  \<crosstalk\>? No, I didn't. At the end, I didn't, because, uh, for this analysis that I was mentioning, then I found like other people that did like similar studies, so they supported me with, uh, correcting my codes and stuff. So I didn't, uh, research any other, uh, ai. So, yeah.  
**Speaker 2** 00:13:20:  Okay. Thanks. And, uh, for qualitative research, how do you usually perform that research? How do you do the project planning? How do you communicate or you manage your project or, or you talk to people and, uh, how do you do that kind? Could you please walk me through that journey for quality?  
**Speaker 1** 00:13:38:  Yeah, yeah. Uh, well, usually in my work, in, in academic, but I think also in, in business or other fields, like first you do a desk research, for example, I had like, um, I'm just giving an example. Gluten-free products. Mm-Hmm. \<affirmative\> for what people prefer mostly about, uh, these products. So there was not much, but from an economic, uh, perspective. So there was not much, um, uh, there was not much, uh, studies about, uh, uh, willingness to pay or preferences for this category of products. So I had to, um, to do like qualitative study. And, uh, in this case, I didn't use any artificial intelligence. I just used like, what the objectives of our study was that trying to understand which were like some of the characteristics that could affect their willingness to pay. So what I did was, like, I identified one product that I wanted to do, uh, the research, which was let's say pasta like, uh, uh, pasta and, uh, snack.  
**Speaker 1** 00:14:42:  So I wanted to compare these two products, which one to choose because we were not sure about which product to choose. So, um, we just, uh, did like, um, um, like give a guide, you know, like an interview guide about the, the, the question about the structure of the focus group, because at that case, we did the focus group. And, um, so we organized, we did the structure, and then we added, uh, for each of these structure, uh, questions. And then we just, I mean, we did like content analysis of, um, of, of, of the focus groups that we had. I think we had two or three focus groups. I don't remember very well with eight to 10 participants for each. So then we understood that, for example, pasta was, uh, the product that probably was also accepted by, um, by non celiac people. And that's why we went for this, uh, product. And then we, based on the, on the results of the interview, we identified like two characteristics for, uh, which we wanted to, uh, do the experiment, and that's how we actually organized it. And we didn't do actually any, uh, we didn't require any support from artificial intelligence.  
**Speaker 2** 00:16:03:  Right. Thank you. And, uh, if you have like, uh, uh, talked to people for a qualitative research interview, have you used any tool that will provide you like meeting transcription or meeting summaries or,  
**Speaker 1** 00:16:19:  Uh, well, I know that m vivo is one, uh, I've used M vivo, but not when I, I mean, not qualitative study with consumers, but what I wanted to actually understand was we, I did, um, a literature review about chip talk script, which is like, uh, I don't know if you are familiar with it, but we use it usually in academia in order to make people, uh, be as true as possible. So what we did, we downloaded the scripts, uh, that, um, different researchers were using. And then, uh, we, through envivo, we created like, um, world maps in order to understand which of the words they were mostly using in this script.  
**Speaker 2** 00:17:06:  Okay.  
**Speaker 1** 00:17:07:  And there it was the possibility to, uh, have like, um, all the words or then choose like 10 most, uh, used words or five, or depending on your, uh, requirements. So we were uploading the scripts, and then, uh, Envivo was giving us the, the, uh, the, the mats.  
**Speaker 2** 00:17:31:  Right. Thanks. And, uh, when, when you're talking about words, were you just trying to pick up any word to understand it's importance in the whole context or context, or you were trying to do some sentiment analysis to understand the tone of your  
**Speaker 1** 00:17:46:  No, actually, we were interested in understanding just which were word the words mostly used, like we didn't ask for, because what, um, the thing is that, uh, usually, uh, chip talk is used in, um, hypothetical choices, dress experiments, um, which is a technique that measures like different, um, um, which means preferences for different characteristics of a person, and also the willingness to pay for, um, for a given product. So what we wanted to understand was like, um, we did like, um, actually an, an analysis of the structure of these, um, of the scripts, uh, based on the original study that actually in a way introduced chip, uh, talk. And then, um, we were measuring, uh, also we measured also the, the, the, the words, but mostly we were interested in understanding which words were mostly used in this chip, uh, talk script. Not any other, uh, requirements we had. We also measured like, um, uh, the word count and the other stuff. But that was another thing. We didn't use NVivo for that.  
**Speaker 2** 00:18:57:  Right. And so when, when you mentioned do you use of NVivo or any other AI tool, uh, were you able to find any tool where you will do most of your work only on that tool without using any external script or any other tool? No. You always have to like go for more than one tool.  
**Speaker 1** 00:19:17:  Yeah. We have to go for, I mean, I think it's a bit different because in academia it's a bit more specific. Everything that you do, you need to explain why you are doing, so it's not just like from a practical point of view, but you also have to understand what are, what are also some theoretical implications of what you're doing. So I, I, so far, I haven't found any tool that actually provides me with all the information that I need when I do an analysis, for example. But I think it's not, I think also if you are a researcher, uh, in, you need to, to study and to think out of the outside of the box, because I think a research in business is different, and research in academia is a bit different. I mean, the same, um, logic, but the, the tools that we use are a bit different. So  
**Speaker 2** 00:20:10:  I, I, I totally agree and understand my, I used to be particle physicist before, so I, this is my recent shift to, to industry. So I totally understand and agree with you. Uh, if I have to ask you, like, so, okay, so, so far you haven't found your dream tool, uh, for qualitative research. Uh, but if I have to ask you your wishlist that, uh, you, uh, plan the project, then you, uh, decide what, how you want to collect the data, then you analyze, then you present. So in this whole year, uh, can you gimme your wishlist that, uh, how do you think AI can give you a new tool or how it could be how, how, how can we, uh, facilitate your research journey so you spend less time and you get more insight?  
**Speaker 1** 00:20:59:  Well, one way would've been like, perfect if I give some keywords, for example, to this tool, and it, uh, gives me like some question templates, you know, like I don't need to, uh, lose time on how to structure, um, the focus group, or let's say, I think at least to give you a first, uh, draft, because I'm sure that probably as a researcher you need to do some changes, but at least the first draft, just by giving some keywords, for example, pasta, uh, gluten-free consumer preference, celiac, non-celiac. So, so maybe by giving just this, um, uh, keywords, it can give me like some, uh, question or like draft of the guide guideline. Yeah. Um, and this is to start, let's say, um, but also like, um, for example, as a researcher in academia, it's also important to have some, uh, reference, uh, because as I mentioned, like we have to explain everything, why we do something and why, so it's, it's very interesting, at least for example, this, um, AI to be connected to, um, uh, for example, Scopus or Web of science databases that contains, um, like several articles from several, um, uh, uh, journals, especially Scopus, which I think has more, uh, journals indexed, um, mm-Hmm, \<affirmative\>.  
**Speaker 1** 00:22:41:  So, because this is also very important because yes, it, it might give me like some ideas, but then I have to go and, uh, search myself about the s and stuff. Um, in terms of analysis, um, I mean, I'm not very, uh, as I mentioned, like I haven't used very specific or very, uh, elaborated, um, analysis for qualitative studies because I rarely have published, uh, I just published it once and I haven't published like, qualitative studies because I, I mean, I'm more interested to have a qualitative study just for designing my, my, uh, quantitative study. But probably if I would, um, publish, I must say that, um, let's say that I know, uh, a little bit before what kind of analysis I would, um, I would, uh, apply. So maybe could have given, could give me some, um, insights of coding or like, for example, how to conduct a qualitative study in how to analyze a qualitative study in R for example. I know that there are some packages actually that they do, but I never used, because I mentioned like I never, and Vivo is one of these tools that, uh, software that actually, yeah, many people use for qualitative studies. I have my doubts because it's a bit like, it tells you what you want it to tell you. So in a way, I mean, I think there are some issues with this software, but yeah, uh, I think this can be, uh, like these two things probably are necessary.  
**Speaker 2** 00:24:27:  So you mean first it, whatever information it's telling you, it can give you like a questionary template to help you prepare the project at Yeah,  
**Speaker 1** 00:24:35:  Like a draft, yes.  
**Speaker 2** 00:24:36:  Yeah. And then do you want some kind of reference to check the authenticity of those possessions that, how, how is it picking those? And thirdly, you want it to be, uh, uh, could, could you please elaborate a bit more about the NVivo? Was it like, uh, that you didn't feel that it's unbiased possession?  
**Speaker 1** 00:24:58:  I, I didn't understand.  
**Speaker 2** 00:25:00:  No, sir. You, you said that, uh, for InVivo you felt like it's giving you what you want. Yeah.  
**Speaker 1** 00:25:06:  Yeah, because it, I mean, I haven't used Envivo like all the time, but in the, in the moment that I've used, I tell Envivo, do this and do that, and it gives me, you know, like kind of information I'm information requiring it to give it to me. For example, different from quantitative study that you see what is, let's say, a correlation between two variables. Right? And you dunno if they're correlated or not. So the, the, the, the program tells you if they're correlated or not. But in Envivo it tells you like, okay, put me, okay. I mean, for example, I, I ask InVivo to give me like, uh, 10 mostly used words, and okay, it doesn't depend on me, but for some other stuff now, I, how, for example, it doesn't tell me like, um, what, with this information that I have, for example, what can be some nice, some important information, uh, output that maybe I'm not understanding that it's interesting to have, you know, like some, um, some suggestions on, on, I, I mentioned, I, I still repeated maybe because I'm not very familiar with qualitative studies, but still it's important also, even if I'm not familiar, it's good to, to have some suggestions and that's will improve my work as well.  
**Speaker 1** 00:26:35:  Mm-Hmm,  
**Speaker 2** 00:26:36:  \<affirmative\>. Right. Thank you. Thank you very much. And, uh, in terms of, uh, let's say da, uh, you, you have your data, then you analyze it. And then in terms of data presentation, uh, what, what, what, what are the kind of styles you usually prefer to see? Uh, like graphs, charts, or different kind of figures? Uh, can you explain if there is something that you really favor?  
**Speaker 1** 00:27:02:  Yeah. Um, well, I tried, when I do presentation, I try not to be boring, \<laugh\>. So I try to avoid having, uh, a lot of words in one, um, in one slide. And I usually use like, um, smart art, for example, like schemes or, uh, pictures or like different ways on how, for example, different concepts are connected to each other. And also another way to, for example, that we use when teaching is our, like, um, videos. And sometimes I realize that when you use, uh, I use mainly, uh, PowerPoint, but sometimes the videos do not work properly in PowerPoint, so it's better you have the link and then just, um, put it in a browser and then show it to the students. So maybe this can be a bit, um, uh, improved. Um, so yeah, I think, yeah, some more interactive, um, presentation are, are important. But I think there are some actually, uh, there are some softwares already that exist that they do some interaction, um, um, presentation, but I haven't used them to be honest. Um, also Canva makes good presentations. Yeah.  
**Speaker 1** 00:28:22:  I've used it actually not for presentation, but I've used it like for poster, uh, or yeah, or like making, like some, for example, when we do the experiments, like a trace experiment, as I mentioned, you, we need to have some pictures to show to the consumers with different alternatives. So it has helped me to mm-hmm, \<affirmative\>, um, to do that. But also I've realized I haven't used it recently. Actually, last time I used it was like maybe eight, nine months ago. But I realized that when I was using it, it's not very flexible in some stuff. So I mean, there are some templates, but not very flexible.  
**Speaker 2** 00:29:08:  So, so are we talking about can, or which tool are you talking about? Which one?  
**Speaker 1** 00:29:13:  Canva. I was talking about Canvas.  
**Speaker 2** 00:29:16:  Okay. Right. And, uh, do you remember the name of any other tool? Uh, other than Canva?  
**Speaker 1** 00:29:25:  No, I haven't. Uh, well, sometimes I use also PowerPoint to do some, you know, representation of, uh, of, uh, pictures or posters. But I don't like very much PowerPoint to do the poster. They, they don't look very nice with the PowerPoint, but with Kava actually it's much better.  
**Speaker 2** 00:29:44:  Okay. Great. Uh, thank you very much. Yeah. That's I think you more or less, uh, all the main things I wanted to hear from you. You're welcome. Uh, yeah, it was really inter uh, nice chatting with you. Uh, do you have any question for, for me, or do you want to know anything? Uh, what \<crosstalk\>  
**Speaker 1** 00:30:01:  Yeah, I'm, I'm curious to know why you are doing this, uh, research, if I may know actually.  
**Speaker 2** 00:30:07:  Oh, sure. Uh, I mean, we, we are a startup. Uh, we are developing a, uh, we are developing AI driven products Mm-Hmm, \<affirmative\> to improve the market research. So basically we have a video recording tool, and uh, then we have, uh, ADA that's here in the meeting. It's our, let's say chat bot that's, uh, that, that will give you automated transcript at the end of the meeting. Mm-Hmm, \<affirmative\>. And, uh, other than that, we are building a tool, a bigger level tool around this video coding functionality that we will, uh, provide, uh, uh, basically end-to-end data analysis platform. If you all the things that you were asking that, uh, uh, you wanted some suggestion in the beginning when you're starting your project, we want to give space where you can start managing your project. You can write your rough ideas, you can ask suggestion to, to our AI tool.  
**Speaker 2** 00:31:03:  It'll give you some templates depending on your industry. Uh, if you're from healthcare sector or FinTech sector, it'll give you, uh, your personalized, uh, templates. And then you can organize the one-to-one meetings with the, with your customers or, or people you want to work with. Uh, or you can organize focus group discussions. And then it'll, uh, automatically record all those meetings. It'll, uh, give you transcriptions, it'll give you summary. And, uh, once it'll start, uh, recording all this information, uh, just like you ask questions to charge GPT, you can ask questions to our tool and it'll give you this information. And then that was mostly interested also in data analysis because, uh, we want to also automate that part as well, uh, specifically starting with qualitative research. But of course, at some point we will be adding, uh, quantitative analysis as well that how much quantitative information we can get from qualitative research. And, uh, yeah. So basically, uh, providing you end-to-end solutions. Uh, that's what I was trying to understand, if you have already encountered something like that, or, or not yet.  
**Speaker 1** 00:32:14:  Not yet. And, uh, why you are also interested, because I imagine this is like a product for the business, usually you want to also introduce it to the universities or  
**Speaker 2** 00:32:26:  We, we can on the longer term. Yes. Mm-Hmm. \<affirmative\>.  
**Speaker 1** 00:32:29:  Okay.  
**Speaker 2** 00:32:30:  Because yeah, uh, you, you, you're more or less exploring similar tools like InVivo or Canva and other tools that you mentioned. So I think, think, uh, academia is also over overused. Yes. It could be very beneficial for, for people. You can save time all the time that you're spending on management and uh, you can focus on what you really want research. Yeah,  
**Speaker 1** 00:32:51:  That's true. Okay.  
**Speaker 2** 00:32:53:  Yeah. So that was more or less the summary.  
**Speaker 1** 00:32:57:  Thank you very much, was interesting.  
**Speaker 2** 00:32:59:  Uh, thanks again, Wilma. It was really nice chatting with you and uh, yeah. Wish you good luck.  
**Speaker 1** 00:33:05:  You too. Bye.  
**Speaker 2** 00:33:07:  Bye.

# Iga Pilewska 

Sr Principal, Research @ Gartner | Business Research Expert

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:59:  Hi.  
**Speaker 1** 00:01:26:  Hello.  
**Speaker 2** 00:01:28:  Hi, how are you doing?  
**Speaker 1** 00:01:31:  Hi. Can you hear me well?  
**Speaker 2** 00:01:33:  Yes, I can. Can you hear me?  
**Speaker 1** 00:01:36:  Yeah, it is a little bit breaking up a little bit. Can you say something again?  
**Speaker 2** 00:01:43:  Yes, let me check. Maybe I can use, um, maybe I can use different audio settings.  
**Speaker 1** 00:01:50:  Right. I, I, I don't hear you at all now.  
**Speaker 2** 00:02:12:  Is it better now?  
**Speaker 1** 00:02:14:  Oh, yeah. It's much better now. Okay. Very good. Thank you very much for joining. Uh, my name is Grith. I'm a product manager, uh, in research and insights@beings.com. It's a startup. Mm-Hmm. \<affirmative\>, uh, working to develop AI products for, uh, market research and customer experience. And, uh, uh, our CEO, uh, Dave, who was communicating with you, he's not connecting today. He got another meeting, so it's only me and, uh, another participant here called ada. It's our, uh, note ticket developed by our company. Uh, it'll just record all the notes. Are you okay with that or?  
**Speaker 2** 00:02:52:  Yes. So just like to check, um, how will the data be used and will my name be assigned to the data, or is it anonymous?  
**Speaker 1** 00:03:00:  Uh, it could be totally anonymous, uh, when it's recording, it's not recording by name. Um, but it's, uh, it'll be used only by our product team, like, uh, by another product manager and c and UX UA designer, basically by four or five people strictly internally by the company.  
**Speaker 2** 00:03:20:  Cool. Sounds good.  
**Speaker 1** 00:03:22:  Very good. Thanks. Uh, so how do I pronounce your name? Uh,  
**Speaker 2** 00:03:27:  It's iga.  
**Speaker 1** 00:03:28:  Iga. Mm-Hmm. \<affirmative\>. Nice to meet you. iga. Uh, could you please, uh, so we can start by basically, uh, knowing a little bit more about your job. Please tell me a bit more about your, uh, job and day-to-Day work, what kind of work you do, and then I can start asking, uh, more questions.  
**Speaker 2** 00:03:46:  Yeah, sure. So, um, I'm a researcher. Uh, my day-to-Day involves writing reports and interviewing clients, identifying the market trends, and then the problems that, um, executive leaders have when it comes to executing their goals and finding solutions that can help them execute either in a cheaper way or faster way or smarter  
**Speaker 1** 00:04:15:  Way. Okay. Very good. Very interesting. And, uh, how much, uh, artificial intelligence do you, uh, use at your job for various product?  
**Speaker 2** 00:04:27:  Um, not very much at the moment. Um, we, I use some internally, but when it comes to external AI tools, not at all.  
**Speaker 1** 00:04:44:  Uh, WW was it because, uh, the rise of AI is basically happening very recently, or you feel like, uh, there is not enough tool available in the market that could help you?  
**Speaker 2** 00:04:58:  Yeah, I think, Hmm. I think the type of the job I'm doing, um, will, probably does not fit into AI tools that exist, that, well, that is one argument. And the argument is, um, obviously we are restricted, uh, what we can use as well, because a lot of the data that we possess is, um, very much protected. So there's limited amount of actions that we can do with the data that we have, which involves tools external to the company systems.  
**Speaker 1** 00:05:42:  Right. Thanks. And, uh, so for any kind of research you are doing, do you already have like quantitative and qualitative researcher in-house by the company? Or you don't, uh, outsource your project to external then?  
**Speaker 2** 00:05:58:  Yeah, so I'll, I'll be the qualitative researcher. Mm-Hmm. \<affirmative\>. Um, so then I suppose there's a little bit potentially less ai, um, being used in this type of research. And we also have qualitative researchers.  
**Speaker 1** 00:06:14:  Mm-Hmm. \<affirmative\>. Okay, great. And, uh, when you talk to people, uh, is it you who will decide what kind of project you want to continue or you get like a work from people higher up in the management letter, uh, and then how do you, you know, do the project planning? And then how do you decide, uh, whom to interview next and, uh, what kind of data you want to collect in those interviews? Uh, how do you take this decision and what kind of tool you use?  
**Speaker 2** 00:06:47:  Um, it will be to some extent my decision. Um, there isn't any, there isn't, I don't think there is. I mean, there, there is a lot of tools which are very basic, uh, I suppose, you know, such as, um, Excel documents, you know, Google Sheets, et cetera. Um, but, um, there isn't much when it comes to more advanced technology that, that I use for these purposes.  
**Speaker 1** 00:07:22:  Right. And, uh, about data, uh, analysis and data visualization, uh, how do you decide that, uh, what kind of tool you want to use for data analysis there? I mean, I, I understand that a lot of, uh, your data is, uh, very restricted, private, but still, I believe that you use some kind of tools strictly when you have confidence in them that only you are the owner. So how do you analyze your data and then, uh, for visualization, if you can tell me a little bit more?  
**Speaker 2** 00:07:56:  Yeah, sure. Um, so is the, maybe let me just double check. Is the main kind of topic that you are, um, covering the use of AI tools in the researcher job, is that the main that you are looking at?  
**Speaker 1** 00:08:14:  Yeah, uh, yeah. I can explain a little bit more about what we are developing. We are developing a artificial intelligence products. Mm-Hmm, \<affirmative\> or for, for people in market, basically empowering qualitative researcher. How do you plan your product? And if you have to collaborate with the other team member, Mm-Hmm. \<affirmative\> and much time you spend there and, uh, then helping you to find a right set of participant for your interviews. Mm-Hmm. \<affirmative\>. And, uh, giving you a template how to, uh, how you can, uh, interview those kind of people, giving you like a preset, uh, of information. Uh, and then once you have the data, then you get, uh, the transcription and summaries and some sentiment analysis of those interviews. And, uh, then on the longer term, we also want to add the possibility to include quantitative analysis tool plus qualitative research analysis tool. Because I think as a, as a searcher, you're not a hundred percent just using qualitative.  
**Speaker 1** 00:09:14:  There is a bit of a spreadsheet or Excel sheets involved as well. And we want to basically automate, uh, this whole research journey. So that's why I was more interested that in this whole, uh, research journey, uh, mm-Hmm. \<affirmative\>, how, can you tell me a little bit more that, uh, what are the tasks that could be automated and, uh, details about your steps? Uh, you can use some examples. Uh, when you, uh, for example, for data collection, do you go for surveys or interviews or video recording or audio recording? And how do you decide that you have to pick this method? And then what do you do after that? So basically just to please walk me through, uh, mm-Hmm, \<affirmative\> those projects.  
**Speaker 2** 00:10:01:  Yeah. There is, um, you know, I am, um, I'm thinking how much I can share. 'cause you know, a lot of the things are protected, obviously. So I can share like everything. Um, I'm, I'm happy, you know, to walk you through my previous job where I was a researcher as well, and tell you more, Derek, 'cause it's no longer protected. But with the current processes, um, I suppose a lot of it is under NDA. So I can, I can tell you what, where I think there is an opportunity, uh, when it comes to, you know, what would be useful for me, but probably I cannot disclose too much when it comes to the process if it makes sense.  
**Speaker 1** 00:10:42:  I totally agree. Yeah, please, uh, uh, whatever information you can share. Uh,  
**Speaker 2** 00:10:48:  So, um, when it comes to my previous job, what we did, um, it was more like, uh, qualitative research in technology, which involved researching the, the demand side of the market, which is the regulation and what is happening, uh, that is basically pushing the technology and, and the technology development in certain direction. And then the supply side market, which is all the vendors and what technologies available there. So definitely we did not have a lot of help when it comes to analyzing the policy papers. That will be very useful. Um, and that's something that, you know, AI can help with. So we would just do it manually. We would use certain tools such as like signal, uh, for news. Um, but a lot of it was very manual desktop research when it comes to what is the upcoming regulation and what will be the forces that will, um, push the market.  
**Speaker 2** 00:11:51:  And then similarly for the supply side, we obviously had subject matter expertise who'd, uh, point us to vendors. We should brief for the construction of quadrants, but when it comes to the smaller, uh, players, we would usually go ourselves. Um, and that also was an imperfect process. Um, we would just look for companies that received awards for like, upcoming startups or were featured in media or someone heard of, but that technique was a little bit, um, ad hoc. And then when it comes to data analysis, I think the transcription is something that already often exists, you know, already in companies. Uh, but definitely pulling the information and the relevant information outside of transcription was something that we did manually as well. Um, so to a certain extent, you know, a lot of it was based on, um, our impression after the call, uh, and there was a lot that was recorded, but I think there was an opportunity to definitely do more of the data, uh, from the transcription. I'll pause here to see if it makes sense.  
**Speaker 1** 00:13:00:  Oh, yeah, that's, that, that's very sensible. And, uh, when you say that, uh, based on our impression, uh, were there always like more than one person interviewing, uh, the other candidates and then you will collectively discuss and share?  
**Speaker 2** 00:13:16:  Yeah, there would always, there, there would always be like two or usually two or three people, um, at the call. So, uh, you would, um, limit any bias by making sure any assessment that comes out of the call is, is based on the consensus and shared agreement. Mm-Hmm. \<affirmative\>. So, yeah. That, that is the answer to your question.  
**Speaker 1** 00:13:41:  Correct. And you mentioned that transcription are always, uh, like already available in one way or another. So did you mean that, uh, you were including some AI based transcription tool or how those transcription were available?  
**Speaker 2** 00:13:56:  Yeah, I think whichever platform you use, uh, you know, whether it's like Microsoft Teams or like another platform, there is already a feature of a transcription there that exists for researchers. So you can, you know, you can use that and that's helpful to some extent, especially if you're looking for like keywords. Uh, I think that is something that often people do, but it's often in a form which is not very friendly to an eye, so it's hard to, you know, do, um, do a lot of coding on the back of it. So that is definitely an opportunity for, for improvement there.  
**Speaker 1** 00:14:36:  Yeah. Uh, so you meant there's a lot of coding, uh, from the tools point of view, or you have to do some coding to get the transcription?  
**Speaker 2** 00:14:45:  Uh, I, I say that you have to, that all the transcription comes in a very blunt, uh, format. Right. Which is, uh, you know, an, an imperfect basically, uh, format. So if you want to get any meaning out of it, it requires some work unless you want to read like verbatim, but, uh, that often just is very time consuming. Right.  
**Speaker 1** 00:15:12:  Right. So, so basically you want to, to see those transcription to give you as realistic meaning as possible, depending on the context, and not just throwing word by word meaning, right?  
**Speaker 2** 00:15:27:  Yeah, I think, um,  
**Speaker 1** 00:15:28:  Or, or did you mean something else?  
**Speaker 2** 00:15:30:  Yeah, I think if, if we could have, you know, key takeaways, summaries, and then, you know, often as you know, because you're just conducting interview, there is very specific things you're looking for. So you can, you know, code, right? Like I to, um, pull out this information for you Mm-Hmm. \<affirmative\>, uh, so it would be very helpful to have it, you know, straight away, right? And like, have to go manually and look for the keywords and, you know, see what was mentioned around the keywords, um, and then do it manually yourself.  
**Speaker 1** 00:16:07:  Right. And thanks. And, uh, how important, uh, it'll be for you to have some kind of a thematic analysis or sentiment analysis at the end of that project?  
**Speaker 2** 00:16:19:  I think quite important. I don't think it's a deal breaker, but I think it could improve the job significantly and just, um, not only improve the quality, but also the time spent.  
**Speaker 1** 00:16:35:  Mm-Hmm. \<affirmative\>. Right. Thanks. And, uh, in terms of, uh, data visualization, in what format you would like to see your data so that you can either, you know, uh, record it for, uh, as a final project or if you would like to share it with your other collaborator or people from management board. So could you please walk me through different kind of formats that you find really important that should be there for visualization purpose?  
**Speaker 2** 00:17:09:  Yeah, with visualization, I think it's a little bit more tricky with qualitative data. Um, just because I don't, I don't see how directly, you know, AI can help. I just, besides generating ideas, um, I think if there's quantitative data, definitely there is more opportunity there. Uh, however, I feel, um, I feel the research world is very much behind when it comes to visualization, and I feel the, um, expectation or like the bar, uh, there is quite low when it comes to what is being expected. So I think it's a little bit harder to create a proposition there that a lot of people will decide it's necessary or it's significantly improving their jobs. But yeah, you know, usually as a researcher I just used, uh, PowerPoint and like Excel graphs, and that would be, that would be sufficient, sufficient and for more abstract visualizations. Um, usually when you try to convey an idea, um, it's, you know, you just look for metaphors, which is I think, a little bit more tricky to use AI for, and not impossible, but a little bit more tricky.  
**Speaker 1** 00:18:27:  Yeah. Uh, thanks. And if you, if you don't have to worry about trickiness or technical, uh, implications, and if I just ask your wishlist Mm-Hmm, \<affirmative\>, uh, so in this whole, uh, journey, uh, what, what, what would you like to see that you think that you're spending a lot of time, which is mostly just admin or thinking by yourself, but if there is a tool that can help, I mean also about the visualization, but basically let's start from the beginning. Project planning to participant inclusion data, uh, like video coding, uh, data analysis or visualization in this whole chain. If you don't have to worry about technical, if you just have to ask, what if this thing is there, it can live, make my life easy. So can you gimme some example there?  
**Speaker 2** 00:19:17:  Yeah, I think definitely tracking the, um, the project development would be very helpful. Like a lot of it happens manually in Excel. Um, and I think visualizing a little bit, like, I think there is a lot of, you know, data as you're conducting interviews, right? That you maybe don't look at on daily basis, uh, you know, such as like the, um, demographics split of maybe the people you interview or like various, um, characteristics and you know, how maybe later you are in the project in comparison, what you assumed and things like that. So that definitely would be helpful to have, uh, all of it just updated rather than like having, uh, to manually go into Excels and, and track it yourself. And also, uh, just have new ideas how to maybe, um, create a shared understanding of like, projects among stakeholders. I think that is maybe key, uh, in the researcher job just because you have a lot of people that are involved and a lot of them may have different ideas when it comes to progress or insights that are coming out of the process. So I think something that would help a lot would be just, you know, visualizations that will put everyone on one page if it makes sense.  
**Speaker 1** 00:20:47:  Mm-Hmm. \<affirmative\>. So everyone on one page means, uh, for example, if you are talking to your colleagues who are at the same mentor and expertise level as you, you can share with them the numbers, plots, figures, all the technical information. So there you will like that kind of isolation. And when you say other people are involved, so then you want to have same result, but in little bit different format, for example, targeting the management board, and then maybe you want to see like, is this what you meant that seeing your result, but in different format for different audience?  
**Speaker 2** 00:21:26:  Yeah. Um, but as well just kind of, I think, um, something that, you know, is some definitely a lot of times is spent on, um, discussing things among, among like individuals and then making sure like, you know, the entire group has, uh, uh, has one understanding of, you know, where the project should continue going, right? Or like, what are the trends or like what we need to change. So, um, having for instance, like automatic key takeaways from each one to one, uh, and then like, you know, resurfacing them in the group so people can see what is happening and providing a bit more visibility of what's going on in the project when you're not in a meeting with people would be very helpful. Um, so that's what I meant, but what you mentioned would be helpful as well, I suppose, which is, you know, different, um, kind of visualizations for senior stakeholders that you need to, uh, update and convince of your vision and a little bit different visualization, uh, for like your peers who usually collaborate with you and, you know, can spot opportunities for improvement or just need to, um, finish, review certain steps that, uh, you need to complete the product.  
**Speaker 1** 00:22:56:  Yeah. Thank you for clarifying that. Yeah, that's a, yeah, the point you mentioned is much more interesting. I didn't think like that before. Uh, so now I'm wondering that you said that, uh, even if you didn't attend a meeting, there should be some key takeaways that if you're coming, you will be, uh, fully up to date quickly, just within, within a minute. And, uh, if your one-to-one is not online, do you still want something to record the, your, your, your chat and give you key takeaways? Even if you're chatting with somebody over coffee or you're meeting it, uh, like on some dinner meeting or lunch meeting outside your office space, you're, do you still want to be able to record some key takeaways and still copy back to project even if it's offline?  
**Speaker 2** 00:23:43:  No, I think that would probably feel a bit too intrusive. Um, just because you probably chat about many things over a coffee and, you know, work is only one thing, so I think people would feel a bit more stressed if that would be the case. Um, so for work meetings, for sure, for like coffee meetings, I think that's, that's too, uh, the risk is you'll lose more than you'll gain.  
**Speaker 1** 00:24:08:  Uh, okay. So then people will be holding up a lot of information when they know that Okay, even in a very casual environment, they're being recorded.  
**Speaker 2** 00:24:17:  Yeah. Mm-Hmm. Um, I think people are a bit worried about being recorded. So yeah, I think, you know, for like work, work meetings, uh, yeah, for sure. And, you know, work settings Mm. That would be very helpful. But for outside might be a bit tricky.  
**Speaker 1** 00:24:37:  Right? Okay. Yeah. Then I think it could be like, uh, we can leave it to the user if they want to use it outside or if it's like a offline meeting with your colleague, but still work environment. Mm-Hmm. \<affirmative\> and you would like to have something that can, uh, still give you key takeaways that you can upload back to the project.  
**Speaker 2** 00:24:56:  Yeah, I think for, uh, for work meetings or anything that happens, you know, uh, through my work laptop, yeah, for sure. That will be, that will be very helpful. And if you can, you know, uh, I often like to think, you know about projects as, um, like at Trello boards, right? If you can just have all like, key takeaways, you know, in one place and then key takeaways from key takeaways, that would, I think, decrease a lot of time spent on just reproducing the data and rewriting stuff.  
**Speaker 1** 00:25:31:  Right. Yeah, that's very interesting. Thanks. And, uh, the next question is about, uh, yeah, along the same line, but more towards like ethical considerations. Uh, what are your thoughts on, um, on the rise of AI and the use of AI in market research and what could be the potential ethical, uh, conflict that can, uh, like scare where the user or, uh, you know, make them more confident to use the tools? So how, how do you see that scenario?  
**Speaker 2** 00:26:05:  Um, so when it comes to potential worries, I definitely, you know, the, the privacy of data is definitely one mm-Hmm. \<affirmative\>. So, uh, I think you never, you never know how the data will be used when you use an AI tool. And I think, um, you know, there is, there are consequences of even being fired if you misuse an AI tool, uh, that is outside of, uh, uh, of your, you know, work environment. So I think that will be a worry. Uh, second worry will be bias for sure. Um, I think that's the common worry among researchers that there is, that basically the tool was not created, uh, in a way which will reduce bias. And I suppose, you know, we trained to, to do that to some extent. And obviously there are many forms of bias, you know, it can be gender or like, uh, kinda racial or, you know, or age bias or, you know, any other form of bias.  
**Speaker 2** 00:27:10:  Uh, so these were, would probably be the main two, and then a third one probably, and you know, this one probably is these based on data and most on something that, um, is human would be just, um, a bit of a, an ethical maybe, I'm not sure if it's ethical, but it's a worry of like recreating rather than like innovating in a sense that, you know, uh, if you, I think often as a researcher, you're trying to think outside of a box, and if you follow too much, uh, like, uh, repetitive process when it comes to thought creation, that end goal is becoming a bit more difficult to achieve.  
**Speaker 1** 00:27:55:  Hmm. Right. Yeah. Yeah. That's very interesting. Thanks. Uh, so yeah, data privacy and basically giving, uh, uh, people surety that, uh, there is no bias involved. So bias means, uh, at all steps, not just at the initial suggestion, but also like you're talking about statistical analysis bias or when you're getting the project done in the visualization or if your tool carries some information from your previous project. So everything will be like strictly, uh, it should give you like unbiased sion strictly according to your project without carrying the information from previous participant or previous tools or products. Yeah. Did, did I understand correctly more?  
**Speaker 2** 00:28:46:  Yeah. Yeah. I, I think so.  
**Speaker 1** 00:28:48:  Okay. Great. Thanks. And, uh,  
**Speaker 1** 00:28:54:  What do you, uh, what, what, what are your thoughts about, uh, the, the ethical consideration of, I mean, ethical consideration of like sentiment analysis or like facial analysis? Uh, uh, for, for, for example, sometime researchers prefer to go outside to meet people in person for qualitative research interview because they feel they cannot get all the a hundred percent input from online meeting. Mm-Hmm. \<affirmative\>, uh, do you have similar opinion or do you, do you still go for online meeting or you think you will have better understanding about the participant if you look at their face and you are sitting with them? And, uh, what, what, what are your thoughts on this online, offline, and then sentiment analysis in both scenarios?  
**Speaker 2** 00:29:45:  Mm. Yeah, I feel like due to the nature at least of, of my jobs as a researcher, I think, you know, out of couple of hundred interviews I maybe had a couple which were in person. So I think because usually you work with clients that are, or with, you know, um, people that are quite far away, um, and they're spread across the globe, uh, it's rather a must that you need you, it is rather a must like the online interviewing. So, um, so yeah, I don't, I don't think there are, obviously there are things you lose, but I think you gain significantly more, which is the exposure to, to more people. So I wouldn't worry too much about, um, however, obviously, you know, if there is any facial recognition and um, that being used for something, I can imagine that would be a little bit of a worry. Um, I know some people just prefer only to record audio, not like video. Mm-Hmm. \<affirmative\> if you record as a researcher, if they, if you record video, like a lot of people want to make sure that it just stays within the research team. So, um, so yeah, I think that can often be more of a worry for people you are interviewing as a researcher than for yourself.  
**Speaker 1** 00:31:03:  Yeah. Right. Yeah, I totally understand and agree. Yeah. Thank you. Um, is there anything else that you would like to tell more about, uh, the use of AI in your job environment where it could help or if, if, if I missed something?  
**Speaker 2** 00:31:20:  Mm, no, I think, mm, I think, you know, the only thing I think you asked me at the beginning about the selection of people you interview, uh, I think it depends on the job. Like for some jobs as a researcher, like use internal, uh, data, but, um, but yeah, but for, I think for my previous job we didn't, and then it would be very helpful, um, to, to have help there. Um, but yeah, I think these more or less sound like, um, the, the things, uh, is there anything else that you want to ask? Any, anything else I can help with?  
**Speaker 1** 00:32:01:  Uh, okay, so what do you think are the low hanging fruits that rule that you really want to see very quickly from AI that can, I mean, I understand we talked a little bit of every step in, in your whole job, but what are the things that you like to really find frustrating that you have to do over and over again, and you feel like, okay, what if this thing could be automated or somebody else can provide me? Just like couple of examples as, as a final takeaway?  
**Speaker 2** 00:32:32:  Hmm. Yeah, definitely. They're like notes from interviews or transcription is one, um, which I feel is a low hanging fruit. Um, so basically, uh, getting like more getting insights auto automatically that you can compare to your own, like ideas of what are the key takeaways for interviews rather than like rereading the transcripts and notes, um, which are not perfect. That is definitely the lowest hanging fruit. And then, and then yeah, then some form of centralizing the data and insights from the projects as it develops, so you lose less time on the back and forth between stakeholders, I think is the second one. Um, does that make sense?  
**Speaker 1** 00:33:24:  Of course. Yeah. That, that totally makes sense. My job, uh, my background is data science, computational modeling and, uh, and simulations. So my job at this, uh, company is also to develop a data analysis framework like synthesis and analysis tool, uh, so that once your project is developing, you have different kind of insights you're driving from various stages, and, uh, then you can collaborate with other people and, uh, you can also share, uh, some insights to people, uh, with a different experience. Like people from your management team, they probably don't want to go into very detailed dashboards, maybe they just want to see the key takeaways, and then at every step you just share that kind of information for them. So yeah, I'm, I'm already kind of working on that. Uh, we are planning to launch our product by September. That would be the first, uh, MVP, uh, but then on the longer term, we will keep adding bit more functionalities. So, uh, I'm very glad to hear that you also, uh, feel that that thing could be very helpful. Uh, yeah, we are going into that direction.  
**Speaker 2** 00:34:34:  Cool. Sounds great. Well keep me updated. I'm very curious to hear how, how will you, how you'll do, and if, if there's anything else I can help with, um, let me, let me know. I'm, yeah, as I said, I'm assuming, you know, you'll use everything I said for internal purposes. Um, and, uh, yeah, I think that's, that's all from my side. Is there anything else that I can help with? Any other questions that you might have?  
**Speaker 1** 00:35:02:  Uh, no, I think we more or less covered, uh, all the things and uh, yeah, thank you very much for joining. I really enjoyed talking to you, um, about the Amazon watch. I will follow up, uh, with the email afterward and uh, yeah, we will definitely keep your email and, uh, once we have the product in the market, uh, we will inform you in case you're interested to explore.  
**Speaker 2** 00:35:23:  Sounds great. Well, have a good rest of the day. It was nice to meet you.  
**Speaker 1** 00:35:27:  Thanks. See you too. Bye-Bye  
**Speaker 2** 00:35:29:  Bye.

# Rytis Radavičius

Small business growth and entrepreneurship researcher, Kingston Business School 

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:00:00:  Hi, can you hear me? Can you hear me?  
**Speaker 1** 00:00:04:  Yes. Hi. Can you hear me?  
**Speaker 0** 00:00:09:  Yeah, I can hear you. Uh, you, but I'm also hearing, I'm also hearing Hello? I'm, I'm saying that I'm also hearing, I'm also hearing,  
**Speaker 1** 00:00:25:  Oh. Uh, but where, where is it coming from? Are  
**Speaker 0** 00:00:29:  I, it feels like it's coming from your side, so maybe if you want to use a headphone or headphones or, what about now, now your connection is breaking. I when you said What about now? I couldn't understand. Yeah.  
**Speaker 1** 00:00:52:  Wait, I'll, I'll, I'll, I'll try to use the headset.  
**Speaker 0** 00:00:56:  Okay, great. Thank you. Hello. Okay.  
**Speaker 1** 00:01:46:  How about now?  
**Speaker 0** 00:01:47:  Yeah, I, I, I think I can keep talking and let's see if I'm hearing. Okay. Now I'm not hearing back anything. It's, it's  
**Speaker 1** 00:01:54:  Better. Okay, good.  
**Speaker 0** 00:01:55:  Very good. Uh, thank you very much for joining. Thanks a lot for your time. Uh, so we have two other participants here, Dina, who is our, uh, UX and UA designer. And then we have Edda, uh,  
**Speaker 1** 00:02:09:  Designer of what,  
**Speaker 0** 00:02:11:  Uh, UX UA designer for the company called beings.com. I'm a product manager there. I'm a good rate. Uh, so my job is to, uh, work on, uh, research and insights of our, our tools that we are developing. We are developing AI products for, uh, market research. Okay. And Ida is a automatic note taker. Um, uh, do you mind if we keep it here to record the meeting?  
**Speaker 1** 00:02:38:  Oh, yeah. If you need it for, for, for your research purposes.  
**Speaker 0** 00:02:43:  Yeah. This is only for our internal use, just for by, uh, our product team.  
**Speaker 1** 00:02:49:  Yeah. If it's a standard procedure, that's fine.  
**Speaker 0** 00:02:53:  Okay, great. Thank you very much. Uh, could you please tell me how to pronounce your name? Rita? Yeah. Thanks a lot, Rita. Uh, so would you like to re Yeah. Re Yes. Uh, would you like to explain a little bit more about your job? What do you do? What is your day-to-Day responsibility and a little bit more about your research?  
**Speaker 1** 00:03:16:  I, I want to ask about, well, Dave was the one who contacted me. Yes. So he's, he's not going to be present, is he?  
**Speaker 0** 00:03:25:  Yeah. Yeah. Sorry, I should have clarified before. Yeah. He is the CEO of the company. He got another meeting, so it'll be only me and, uh, and d  
**Speaker 1** 00:03:34:  Mm-Hmm. \<affirmative\>. Okay.  
**Speaker 0** 00:03:37:  Yeah. And, and,  
**Speaker 1** 00:03:38:  Uh, and also, uh, he, he said his, he suggested in the email offered in the, in, in his not email message in LinkedIn, Amazon voucher. So how does this work? I asked him, but he didn't reply, so I was wondering.  
**Speaker 0** 00:03:56:  No, no worries. Yeah. At the end of the interview, you will get, uh, um, uh, Amazon voucher on your email. I I, I will.  
**Speaker 1** 00:04:04:  Alright. It's on, it is by email.  
**Speaker 0** 00:04:07:  Yeah. Uh, if you have any other preferred way we can follow that. No, no,  
**Speaker 1** 00:04:11:  It's fine. If I, yeah, if I can redeem it through email. That's all right. I was just wondering how, how it'll, how it'll be processed.  
**Speaker 0** 00:04:19:  No. Yeah. At the end of the interview, I will go to your Amazon business account and I'll, uh, send it to your email address. Okay.  
**Speaker 1** 00:04:26:  Yeah. Okay. Yeah. Uh, my, my currently main, uh, occupation, uh, so to speak is, uh, is that I'm, uh, finishing my PhD on, on business management, on, on small business growth, uh, particularly the, the startups. Uh, well, first 10 years of, uh, of organizational development and growth. So that's the research I'm doing academically. But also for, for many years I've been doing market research for, uh, clients from different countries. I'm Lithuanian originally, so I, I did extensive market researches for, for foreign, uh, clients there. And then I moved to uk to UK because of a business. And also I did market researches, uh, here for Lithuanian and German clients. I worked in Germany for a while doing similar things as a consultant or, or, uh, part-time employee who was, uh, helping to, in, in development of, of, of a, of a starting business. Right. So that's, uh, background in short.  
**Speaker 0** 00:05:45:  Thank you very much. Yeah. That's a very interesting, uh, uh, set of studies you have, you work in industry and now you're, uh, in academia. So please feel free to use examples from both sectors when I ask, uh, questions. So for, for example, uh, could you please tell us a bit more about the use of, uh, artificial intelligence in your research? Like in your current research, or if you have also used it in your previous role in market research?  
**Speaker 1** 00:06:11:  Mm-Hmm. No, I, I haven't used it previously because, uh, it was simply not there yet.  
**Speaker 0** 00:06:21:  Right.  
**Speaker 1** 00:06:23:  So it's a, it's rather recent development. And, uh, as far as my research is concerned, uh, no, not at the moment. Uh, I, I'm not excluding the possibility, but, uh, but my research simply does not require anything like that. I'm, I'm using, uh, some software for data analysis, but, uh, but that doesn't include, uh, machine learning tools, uh, because, uh, it's just simply not required.  
**Speaker 0** 00:06:56:  Yeah. And is, is that, uh, qualitative data or quantitative data and  
**Speaker 1** 00:07:03:  Qualitative interviews, but also quantitative, uh, data in terms of, uh, mm-Hmm.  
**Speaker 0** 00:07:11:  \<affirmative\>,  
**Speaker 1** 00:07:12:  Uh, spreadsheets and, uh, and, uh, annual returns. So turnover information is quantitative, but my interviews were like, this one, I assume Dave said it's a survey. Uh, and as far as I understand, this will be interview. So it's not technically a survey. Survey would be a questionnaire where you ask questions and I reply yes or no, or I provide very specific information, or I rate from a scale of five, one to five or five or one to seven certain phenomena or, or, or, or my impression of certain things. That's a survey. What we are doing now here is, uh, an interview, which is qualitative interview. Yeah, just a clarification. He mentioned survey in, in his message. It's not a survey.  
**Speaker 0** 00:08:07:  Okay. Right. Thanks. Yeah, sorry about that. So, so the surveys are not just sending a link for, to somebody to fill it up, but, uh, people also interview each other to get this also,  
**Speaker 1** 00:08:18:  Also sending a link. But a survey can be, can be done in person where you just answer questions, but the questions are preset. You have a certain questions and you don't ask open questions like, tell me about this or tell me about that. Uh, that's, that's qualitative interview. Survey is, is, is strictly quantitative.  
**Speaker 0** 00:08:39:  Right. Okay. Yes. Sorry about that misunderstanding.  
**Speaker 1** 00:08:43:  No, it's, it's, okay. I, I, I'm in research, so this is my, my, my daily, daily job. No, yeah.  
**Speaker 0** 00:08:50:  Thank you.  
**Speaker 1** 00:08:51:  But I understand that some people may, may mix them up and they, they say survey where it's just an interview. Right. All the other way around.  
**Speaker 0** 00:09:00:  Okay. \<laugh\>. Great. Thanks. And for, uh, qualitative research interviews that you're taking, uh, uh, can you tell us a little bit more, uh, how do you find the participant? How do you plan your project and, uh, uh, how do you decide about data collection and data analysis, that, what kind of tool you have to use?  
**Speaker 1** 00:09:23:  There was a, uh, well, uh, in my research design, I, I, I specified, uh, uh, the research, uh, sample. Mm-Hmm. \<affirmative\>. And, and then based on that, I approached, uh, small businesses. 'cause I'm interested in only in small business development, organic or natural development. Not, uh, not, not investments, but, uh, development from turnover. And then I emailed, uh, some business to some businesses I'm emailed. Others I found or even, uh, went to, to their premises in person, which tends to be a, a useful thing because they, they, they tend to appreciate your effort in, uh, trying to approach them. So if you come in person, if you show, uh, that you are really interested in not just, uh, not, not just an anonymous email, then they tend to, to give you that time. And also that was, these were in person interviews. So that was, uh, 2019 where, where there were no restrictions.  
**Speaker 1** 00:10:37:  And then I traveled to, uh, to other cities. I, my, my sample included businesses from New York, uh, London and Berlin. So I did everything I did in person. And then, well, you can call it a mach, you can call it a machine learning tool or, or artificial intelligence, uh, as you, as you call it. Uh, I used, uh, for interview processing, I used, uh, a tool called, uh, Otter, O-T-T-E-R. Yeah. ai. Right. So that is, that is at that point when, when it was 2019, 2020, this was not a buzzword ai. So it was just a, just a tool that, that, that is self-learning up to a point. And that was very useful by the way, uh, because it saved, uh, it saves a lot of time for researchers, both market and academic researchers who interviewed people like you are doing now. And then you need to transcribe those interviews in order to find certain teams, certain themes or certain subjects that, uh, that were discussed or simple or, or simply just words.  
**Speaker 1** 00:12:00:  Then you, then you, you upload, uh, your interview in, uh, in word format, or I think they used word ai, uh, auto AI used word format. And then, uh, it trans, it, it, it transcribes everything. Excuse me. You send the audio file and then you can, no, that was the other way around. You, of course, you send the audio file and then auto transcribes it, and then you can choose how to save it, whether it's Word or PDF, but also, uh, it's not precise as, as, as, as in case of any machines. They're not precise to, to the point of, uh, uh, of, to the satisfactory point. So I needed to, uh, to look through all of the transcribed interviews and then make adjustments while listening to the recordings.  
**Speaker 0** 00:12:56:  Mm-Hmm. \<affirmative\>, right? Yeah. Thanks. Thanks a lot. Yeah, I know that too. That's very interesting. Indeed. Uh, but I'm wondering that, uh, most people use those tools for online meetings. Uh, but are you saying that, uh, you have used order, uh, for offline meeting as well, when you were talking to people in person and then you were recording, you were taking the voice recording, and then you will go back to your office and then Yeah. You will do that. Okay.  
**Speaker 1** 00:13:22:  Yeah. Yeah. They, they definitely developed over the years, because back then it was, like I said, 2019, 2020, when I was transcribing, uh, these interviews, probably now they're offering some other services, but back then it was, uh, it was, uh, it was just, uh, just, uh, uh, just, uh, taking a, taking a recording. Mm-Hmm. \<affirmative\>, and then, uh, changing it into, into a text.  
**Speaker 0** 00:13:54:  Yeah.  
**Speaker 1** 00:13:56:  And that, that, that was done online. I did it online, yeah. But, uh, I, I had to connect to, uh, to, to the website to do it. I had to have an account. Yeah. But, uh, but yeah, I, I interviewed people using, uh, using, uh, an iPhone.  
**Speaker 0** 00:14:18:  Mm-hmm. \<affirmative\>. Right. Great. Thanks. And, uh,  
**Speaker 1** 00:14:23:  MP MP three, MP three format,  
**Speaker 0** 00:14:25:  I mean. Right. And after 2019, have you explored any other tool or you're still using gotter?  
**Speaker 1** 00:14:33:  No, I'm not using anything at the moment because I'm, I'm now in, uh, in the process of, uh, of, uh, making final revisions, post examination revisions. So I don't need it at the moment, but, uh, but of course, uh, if I continue doing qualit qua qualitative research, I, I will definitely use, uh, order or maybe some, some other tool. My professor suggested auto, so that's how I found about it.  
**Speaker 0** 00:15:01:  Yeah. And, uh, when you are uploading your audio, your recurring charter and getting transcription, uh, can you tell me a little bit more what kind of further information you found really useful as the end product, uh, for that, uh, exercise? Uh, did you get like, uh, summaries, uh, like a word cloud or what kind of outputs it was giving to you that you found really interesting?  
**Speaker 1** 00:15:27:  No, no, no. For, no, it, it, it, it does not, it does not, uh, uh, uh, well, I don't know what about now, but, uh, as far as I, I know artists still doesn't do that. It's, uh, it's, it's, it's data processing. So for that, I use the, well, in this case, not, not quantitative. I didn't need SPSS, but I used NVivo.  
**Speaker 0** 00:15:51:  Yeah.  
**Speaker 1** 00:15:52:  But Envivo is, it's technically not a, not a machine learning tool. It's just a program where you, where you upload your, your, your texts, your interviews, and, and then they, they can provide it, it can provide, uh, things that you mentioned, like word trees, or they can find specific words or phrases. It can find specific words or phrases in the text. Yeah. But, uh, author, no, for author, I only use author. I only used to, to transcribe the data. What I, what I, what was interesting with OT is that, uh, even, even when there were, uh, native English speakers, both in the United States, in the United States, it used to catch a little bit more and, and it was more precise. But, uh, in, uh, here in England, accent is a little different. So there were some issues I needed to, to make quite a few revisions. And then I sent it, I sent the file because one of my respondents wanted to have interviews, so I sent it to him. And he also, he found some more mistakes there. He said, no, that's different. And as far as Germans are concerned, for example, uh, uh, that involved more work because they, it, it didn't catch, uh, accents very well. So that's what I was missing. It would be good to to, to teach us to, to catch more accents. Yeah.  
**Speaker 0** 00:17:28:  Yeah. Of course. I mean, if you, you  
**Speaker 1** 00:17:30:  Understand, if you are not a native speaker, and then if you speak it, it may not, uh, uh, recognize certain, uh, words that you are saying. Yeah.  
**Speaker 0** 00:17:39:  Right. And  
**Speaker 1** 00:17:41:  Or misinterpret them.  
**Speaker 0** 00:17:43:  Sure, sure. I mean, the whole, uh, summarization after that depends on how well understood the first word. Otherwise, it can change the whole context of the game. And, uh, about the method of, uh, data collection, like you said that you were traveling across the globe to talk to people in person and then recording the, uh, audios. Uh, what, what, why did you pick up that method when, uh, you could have done it online? How  
**Speaker 1** 00:18:10:  \<crosstalk\> Oh, that, that makes, Matt makes the data much richer. That's the point of research that you have to go to. It's called field research, and not just sitting in the room. You, you also make some observations that are useful. I also observed how businesses are, what, what, what kind of premises are there, how people work, and, uh, and how owner managers interviewed only owner managers, how owner managers, uh, present themselves there. That is all useful online on screen. Uh, now it's a bit more common. And, uh,  
**Speaker 0** 00:18:52:  Yeah, COVID basically pushed everybody in that direction. So I'm wondering that  
**Speaker 1** 00:18:56:  It's a little, I I, sometimes you can do it. If it's just a, just a short interview like we are doing, or some, or even shorter interviews, then yes. But, but my interviews were from one hour, anything from one hour to three hours. And these were in depth interviews with people who are generally quite busy. So, and it's the same thing again, if you are showing, uh, real interest, and if you're showing that you're making an effort, then, then people are, people tend to help you.  
**Speaker 0** 00:19:29:  Right. And after 2019, since a lot of, you know, things changed, covid pushed a lot of people to go more online. Uh, and I totally agree with you explanation that, uh, uh, field research is a bit more realistic when you're actually there in person. Uh, but after that, uh, I'm sure people might have explored the possibility that to find a tool that can give them as realistic as possible experience of field research, even online, have you come across any tool that can, uh, that can give you as close as possible experience as field research, but online.  
**Speaker 1** 00:20:10:  But how can you, you need someone with a camera working all around unless, uh, unless there are these, unless you mean these platforms, uh, such as we are using now, uh, the Google talk or Zoom, or used to be Skype. Skype, but now it's almost gone. So teams, those things. But, uh, but if you're doing field research, or my case, these were interviews. But if you, if you're doing observation, if you're using ethnographic, ethnographic, uh, uh, uh, research design, then, uh, then you need to observe it yourself, how things are, and sometimes time it, so how much time does this take? How much time does it take from, to go from point A to point B in term in when it's, uh, uh, when, uh, when we're talking about factories, how people are moving and research is on efficiency, for example. Uh, what, what obstacles are there in place or how they're communicating all this?  
**Speaker 0** 00:21:22:  Right?  
**Speaker 1** 00:21:24:  There, there, there were some, there, there was, there were some, some studies that, uh, that used cameras. So they, they were putting cameras in offices. Yeah, that is, that is possible. But, uh, but it depends on research designs. Sometimes you just, uh, simply need to be there in place yourself because you, you, you're the one who is going to process the data, not a machine, and you're going to write about it.  
**Speaker 0** 00:21:53:  Right. And when you say observation, uh, and the things that are impossible to understand just by talking to them are, do, do, do you mean having a better understanding about their facial expressions or the sentiments, uh, that are carried out with words,  
**Speaker 1** 00:22:12:  Including that also, but, uh, but things that, uh, that not necessarily can be captured on camera. So unless a camera follows a person, you know? Right. You can install a camera in a, in an office or in a, in a workshop. Uh, but, uh,  
**Speaker 0** 00:22:35:  Right. Can, can, can, can you gimme some example, just like some concrete example? I, I understand your overall picture that we really need to be there. I'm just trying to understand, not fully replace the in-person interview, but let's say just in case, if someone want to get like, as close as possible experience, uh, what are your thoughts that, what could be still moved to online or what else we can do?  
**Speaker 1** 00:23:04:  One option could be, uh, installing a camera, or if we're talking about factory, for example, where workers are, are wearing heads, uh, some, some, some helmets or, or caps installing a camera there, a little one so that if someone agrees to take part in your research, then everybody knows that he's wearing, he's wearing a cap or, or a, or a helm with a, a camera installed. And, uh, if, let's say, I want to, to see, my, my research question is how, how is, uh, how is the, the owner manager organizing his or her day? Yeah. So, so then he or she would be wearing, or, or, or the camera pins to, to a jacket or, or,  
**Speaker 0** 00:24:11:  Yeah.  
**Speaker 1** 00:24:12:  Or, or an overcoat also. That would be an option. And then it would be the way you are following them. But then you would also need to, if it's a facial expression, like you mentioned, you also need another camera to, to, to check the facial expression or, or, or what that person is doing.  
**Speaker 0** 00:24:33:  Right.  
**Speaker 1** 00:24:34:  Uh, thanks.  
**Speaker 0** 00:24:36:  And, uh, let, let's say that, uh, you have collected data. Uh, you are, uh, you, you have done the profiling, what kind of participant you want. You have a very nice description of a project. You collected your data. Now, can you tell me a bit more about the data analysis and data visualization part? What kind of tool you mostly use and, uh, what kind of, uh, metrics you really want to see at data visualization part?  
**Speaker 1** 00:25:05:  I used, uh, InVivo. Mm-Hmm. \<affirmative\> software called InVivo for, for qualitative data processes. You know, n  
**Speaker 0** 00:25:14:  Yeah, I, I'm familiar with that, yes. Oh, okay.  
**Speaker 1** 00:25:17:  Yeah. So that I used. But, uh, but I used, I used it mostly to, to systematize the data I needed to, to do a lot of, uh, a lot of data analysis myself, not, not, not from machine, but myself, because, uh, I was looking at, uh, at, uh, at some, uh, hidden factors that influence, uh, successful organizational development. So it's not just words I was looking for, but I was looking for meanings.  
**Speaker 0** 00:25:55:  Yeah.  
**Speaker 1** 00:25:56:  And, uh, that, uh, was not possible back then. And I'm not sure it is possible now. It has to some extent. It may be possible, yes. But it's, it's the same thing as interpreting tools to translation tools. You need to have a up to a point. You can do that. Yes. Yes. It can be helpful as a, uh, as an assistant, but still you need to supervise it all. As I mentioned with, uh, with interview transcriptions, that you have to check the files, you have to check that product yourself to make sure that everything is done correctly. So, so something, something to capture meanings. Yes. If I, if I, let's say, had a, had a program where I could, uh, where I could put in a command that I'm looking for certain metrics or certain meanings, and then it, it could make suggestions from which I then could choose the most correct one, and then it could learn to, to find that pattern in all, all around interviews, across the interviews, because I, I did 34 interviews, so that was, uh, 1000 pages of text.  
**Speaker 1** 00:27:18:  So if I could do, could have done just one interview with the help of, of this, for example, finding, finding, uh, I was looking for themes. Initially. I was looking for themes and then grouping them into aggregates, and then, you know, and then into, into, into high abstraction aggregates. So that is, that can be done by a machine that is, that is, that is clear grouping, grouping themes. But, but finding, uh, themes in the text, uh, but not just words. And we've always good in finding words up to five, used to be five make mostly makes sense, five words after the word or before the word that, that before the search word and after the search word. But then a meaning, let's say, uh, I had some, I have some i a priori themes that comes from literature in, in, uh, in academic research. So if, if, uh, if, let's say a theme is efficiency. Yeah. Those, the, the, the interviewers, interviewees don't necessarily use the word efficiency, but you have to infer from what they're saying that this counts as efficiency or that counts as inefficiency. So that would've been helpful.  
**Speaker 0** 00:28:48:  Right.  
**Speaker 1** 00:28:49:  A tool like that to capture meanings and to, to look for, to look for, well in that case, or I can say behavioral patterns or, or linguistic patterns or, or, or, or, or, uh, or semantic patterns in what they're saying.  
**Speaker 0** 00:29:09:  Right. Thanks. And yeah, that's very interesting insight. And, uh, at the end of the project, uh, let's say you have data visualization, and then you want to share your finding with the, with other experts or people, uh, like your supervisor, or if you're in market research already, then people are in the management letter. So can you give, uh, some examples that, uh, how the information is usually exchanged? Did you usually prefer to show like charts, graphs or, or more words and information like, uh, very, uh, small summaries, like in PowerPoint slides, how the,  
**Speaker 1** 00:29:51:  Well, it depends. Uh, in a PhD, I had to use everything. I also, uh, uh, as an appendix, I, I included, uh, the, the whole, uh, the whole thematic template of, with the themes. Uh, and then, uh, and then tables with, uh, with some financial data or, or data or, or information about participants in general. So their age and, uh, their experience, years of, of experience and education, all that. PowerPoint slides, that would be more, or, or, or, or just slides in general? Uh, uh, a more succinct version of, uh, of it would be, would be conference presentation. Um, I will present my research in a, in a business management conference here in the UK in September. So for that, I will prepare a PowerPoint. Mm-Hmm. \<affirmative\> slides.  
**Speaker 0** 00:31:00:  Yeah. Right. Thanks. And, uh, so like you mentioned that you've used Dota like in 2019, and then it's mostly hands-on experience, and you're trying to get the themes and the sentiment by yourself by looking at the words and understanding them more. Uh, but now, the, the  
**Speaker 1** 00:31:19:  20 19, 20 2019, 2020\.  
**Speaker 0** 00:31:23:  Right. Okay. Mm-Hmm. \<affirmative\>. And, and, uh, now if you have to like, do these qualitative research interview from, like planning the project, then finding the participant, then collecting data and analyzing and visualizing and, uh, then presenting. So in this whole chain, uh, where do you think, uh, AI could be more helpful to, uh, save your time and, uh, give you a bit more authentic information? Where do you see the AI application in this whole market research chain  
**Speaker 1** 00:31:54:  Fi finding participants? Uh, I doubt it.  
**Speaker 0** 00:31:58:  Mm-Hmm. \<affirmative\>.  
**Speaker 1** 00:32:00:  Because unless it is, uh, unless it is, uh, it is, uh, it is random sampling. If it's random sampling, then, then could be, but then there are databases if it's random sampling. And then also you can use some programs like Python for, uh, for finding things online on the internet. So let's say I'm, I'm researching, uh, an antique business, antique shops, yeah. As an example. Mm-Hmm. \<affirmative\>, because restaurants are, um, more easy to find. There are, there are catalogs of restaurants, and then you, but, but if it's, if it's, let's say antique dealers, yeah. They're not necessarily registered as, uh, as, uh, as antique dealer dealers in, in, in some comprehensive catalog. So scanning the websites, scan scanning websites, scanning, uh, scanning different, uh, uh, databases and, uh, finding businesses that meet your criteria because you have to, you have to use certain criteria. Yeah. Mm-Hmm. \<affirmative\>,  
**Speaker 1** 00:33:23:  Let's say if you, if you limit your research to cities, so you have to, you have to input, uh, uh, London or York or Manchester. And then within those cities, all websites, all possible websites, scanning and looking for all possible businesses that are dealing in antiques. And at this point, because the problem with registers in, in, in certain businesses, I gave an example, which is, which is more difficult. There are lots of, uh, rexi rates are quite high. So if you, if we're talking about such businesses, then if, if there is a, if there is a database or a catalog, which is a few years old, they might not be there anymore. Or there may be some new ones, which you would be interested in as well. So that may be, that would be helpful. Uh, but I, I can tell you what, what it would be helpful, uh, in, in writing up, uh, not just thesis, also a research paper.  
**Speaker 1** 00:34:38:  Research article, empiric article. And that's, that's, that's what I've been thinking about as well, how to develop a tool like that, that it's the quotes, it's the quotations, references. Mm-Hmm. \<affirmative\>. So you know that if you're writing academic work, ev, everything has to be, uh, referenced. And you have a, you have a large list of, of, of sources of references at the end of your work, whether it's a thesis or, or, or a, or, uh, an article. There are some tools that, uh, you can integrate in Word, for example, I, I don't remember now the name, but there is a certain tool where, which, which, once you insert a quotation, once you insert articles, once you have, uh, once you feed it, all the articles that you read that, that you, that you, uh, reference in your work, then if you, if you're writing, uh, uh, say that Smith and Smith say this and that, and then it automatically adds, it, uh, adds the source, yeah.  
**Speaker 1** 00:35:49:  In brackets, but it's not exactly precise. And I, I, I did it all manually. And as far as I, I know, uh, uh, other researchers are also tend to do it manually in certain quotations, you know what I mean? Yes. In scientific literature, if you say that, oh, there is very little information on the subject of, uh, growth and small businesses, let's say. Yes. And then you have to, uh, to, to, to say whether it's your idea or whether it's somebody else's idea that you're using, you have to, to, to back up your, your statement.  
**Speaker 0** 00:36:33:  Yeah.  
**Speaker 1** 00:36:34:  And then you, you have to add the, so I, I was thinking about, about the way to, to simplify this somehow, but it's, it's, it's, it's quite complicated.  
**Speaker 0** 00:36:53:  Yeah.  
**Speaker 1** 00:36:56:  Because it has to be connected to internet. If you're writing something on, on, uh, in Word or, or Mac pages, then, uh, right. That's a problem because the current ones are not precise enough. And then it's, it's, it messes things up. And then there, there is some form, there are some formatting issues, and then you have to, to do it again yourself. So it's,  
**Speaker 0** 00:37:25:  Yeah, I totally agree with you. That's very nice point. You raised it. Yeah. If we, uh, yeah, if, if, if we are, if there is a,  
**Speaker 1** 00:37:32:  For referencing some tool for referencing  
**Speaker 0** 00:37:35:  Sure. Yes. Some tool for references, uh, references and also like a bit more automatic and more authentic that, uh, uh, not just like chat GPT, if you ask it, it give you something, it can still give you something, but then you don't really believe it. It's basically just a draft and, uh, you have to crosscheck everything. And you might end up by saying that 90% is totally wrong. All the suggestions. Yeah. Thank you very much. Yeah, that was very interesting. Uh, point, do you have, uh, it was really nice talking to you. Uh, yeah. I clearly learned a lot. And, uh, do you have any other question for us?  
**Speaker 1** 00:38:13:  Yeah. Uh, how, how old are you as business?  
**Speaker 0** 00:38:19:  Uh, well, the company started, uh, actually, uh, three years ago, but officially started, uh, uh, we got the more than a million pound, uh, this year. Uh, we got like more than 450 k from Google. So we are collaborating with Google. So they, we are launching our first product, uh, most probably this September. So our idea is basically to, uh, reduce the time that people, uh, uh, in market research they're spending on this whole research journey. And we are developing, uh, a platform that will provide you end-to-end solutions. You start planning your project, then you, uh, find your participant, then you collect data. We have our own video recording tool. And then Ida here is, uh, equivalent to what you said about order. It'll give you transcriptions. Then, uh, it'll also give you summary and the percentage of time or, uh, highlights of your interview, the kind of question asked. And then you can ask it questions back. If you have talked to somebody for an hour or longer, you built a lot of text. And, uh, then you can figure out that if you have already asked the right set of question or not. So at the end, uh, we will provide in-house, uh, data analysis functionalities as well. We will initially target for qualitative, but then, uh, we will move towards quantitative as well. Mm-Hmm. Because I think Okay,  
**Speaker 1** 00:39:49:  I see Yes. Is gone.  
**Speaker 0** 00:39:53:  Yeah. Because, uh, like you mentioned that sometime you talk to people, then you also have a different kind of data. You also take spreadsheets. Uh, so we will make sure that because there is not a clear a hundred percent very well defined boundary between qualitative and quantitative when it comes to market research, you need a bit of both. So what we are trying to do is empowering qualitative researcher so they can do what they're good at instead of just, uh, spending time on admin related tasks.  
**Speaker 1** 00:40:24:  I think that's useful.  
**Speaker 0** 00:40:26:  Yeah.  
**Speaker 1** 00:40:27:  What did, what what, uh, uh, I'm not sure if you're, if you are planning to develop something like that, but when I, when I was doing a, a market research, or let's say, let's say there was, uh, there was this, uh, software company from Estonia who wanted to enter UK market Mm-Hmm. \<affirmative\>. And I was looking for, for, well, first I did a general research on, on how things are here, which, which does not require any tools, but then I needed to, to identify businesses that would, uh, interest in their particular service. Hmm. And, and the sample was, the sample population was, was quite big. Yeah. And it was up to me to decide. So, and then I would also go and visit businesses, uh, and, and talk to them and present the company first, ask for an interview. But it took me a fair amount of time to, to just identify the, the businesses, let's say London area. Yeah. That would be useful.  
**Speaker 0** 00:41:36:  I totally agree with you. That that's very interesting point because we spent  
**Speaker 1** 00:41:40:  Yes, yes. Go on. Excuse me.  
**Speaker 0** 00:41:42:  No, yeah, I, I, I'm saying that I agree that we spend a lot of time to find the right audience. So that's where AI can, AI and machine learning can help a lot to give you initial set and also like creating some kind of overlap with your critical requirements that, uh, how much is the overlap or how much is the match, uh, between your target and your, your requirement.  
**Speaker 1** 00:42:06:  Correct. Correct. Yeah. So something like that.  
**Speaker 0** 00:42:13:  Yeah. Very good. Very good. Okay, great. Uh, uh, yeah, when we launch our product in September, we will definitely give you guys a heads up if you want to take a look and, uh Oh,  
**Speaker 1** 00:42:26:  Yeah. Oh yeah. Yeah. Please do. Yeah, please do. If I can be of any help.  
**Speaker 0** 00:42:32:  Right. Also, thank you very much. It was really interesting and really nice chatting with you. And, uh, I will follow up about the virtual later after the email.  
**Speaker 1** 00:42:42:  Likewise, \<inaudible\>.  
**Speaker 0** 00:42:43:  Thanks a lot. Thank you. Have a nice, uh, rest of the day.  
**Speaker 1** 00:42:48:  Yeah. Good luck with the project development.  
**Speaker 0** 00:42:52:  Thank you very much. Bye-Bye very much.  
**Speaker 1** 00:42:54:  Bye bye.

# Sarah Smith 

Qualitative Specialist and Behavioural Science Practice Lead, Oracle Life Sciences 

## Meeting summary: To be filled 

## Transcript:

**Speaker 0** 00:02:10:  Hello?

**Speaker 1** 00:02:12:  Hello. Hi, Sarah. Can you hear me well? Yes. Thank you very much. Thanks a lot for connecting, and thank you very much for your time. I'm, uh, uh, \<inaudible\>. I'm a product manager in the research and insights at uh, beings.com. It's a, uh, it's a newly, uh, set up startup company. Uh, we are developing AI products for, uh, market research and to improve customer experience. Uh, we have another participant here called ada. It's our, uh, note taker developed by our company. Uh, do you mind if we keep recording the meeting or you have any problem? No, no problem. Okay. Thank you very much. Uh, so, uh, can we start a bit, a little bit about you explaining, uh, about your job? What are your day-to-day responsibilities?  
**Speaker 2** 00:03:02:  Okay, for sure. Um, I apologize in advance that I've contracted covid, so I'm not at my absolute best today, \<laugh\>, but, um, do I  
**Speaker 2** 00:03:12:  \<laugh\>? Um, so, um, I work in, uh, pharmaceutical market research. So that comes with its own special challenges. Our audience is slightly different, it's slightly more sensitive. Um, we don't use all of the same sorts of techniques as our consumer market research colleagues do. Mm-Hmm, \<affirmative\>, um, uh, quite often we find that, um, tools that are designed in a consumer setting don't always fit the nature of our business. So that's kind of where we come from. I've been doing this for about, um, 25 years. Always qualitative, always pharma, um, always international. So I only deal with prescription products, lots of oncology, lots of rare disease.  
**Speaker 1** 00:04:01:  Very, very interesting. Thanks a lot. And, and like, given that your background is also from behavioral science, can you gimme like some example that, uh, how did you successfully, uh, integrated, uh, your experience from behavioral science into qualitative research at your current job?  
**Speaker 2** 00:04:21:  For sure. Um, so I've been specializing in behavioral science about the last eight years now. And actually I consult, rather than doing too much live research, I consult on other people's projects now, and I do that qualitatively and quantitatively. And our behavioral science falls into two camps. So first of all, it's using behavioral science and behavior change theory to help our clients do better things with their products more strategically, but probably more relevant for what you are asking is using our understanding of, um, why people do what they do, what biases are present to help us ask better questions. So I do a lot about, um, helping us try and make our respondent experience a bit better, try and make our, you know, reduce our attrition rate. So try and make things quicker, faster, more visual, less complicated, easy for people to do. I try and ask questions that, um, help people to tell the truth a bit better because, you know, if you said to somebody, tell me everything you've prescribed over the last six months, you know, you're not gonna get a very accurate answer. So trying to make it a bit more bite-size and anchored in actual behavior rather than just kind of, you know, try to get 'em to remember things.  
**Speaker 1** 00:05:28:  Yeah, right. Thanks. And, uh, you, you mentioned something that, uh, the traditional tools available for market search are not always very useful for your specific industry. So since you're like expert in this, uh, life sciences industry, so what are the unique challenges or, or the opportunity you would see, uh, that you're, that that someone can face if they wanna use AI in quality to research in your specific sector?  
**Speaker 2** 00:05:56:  I think there's a, a few things actually. So the first thing is, uh, lexicon. So our language is very specific. It contains a lot of very specific drug terms, body part terms, medical terms. And if the language model isn't built for pharma, it's, it takes a lot of training to get it up to speed. So when we first dabbled in speech to text for example, it really wasn't very good and we've had to spend a lot of time working with partners to kind of give their models enough information to get 'em used to that language. Um, the second thing is, um, GDPR. So we've, uh, anything sensitive obviously has much higher levels of GDPR. And then the third thing, I dunno if you've come across adverse event reporting.  
**Speaker 1** 00:06:39:  Yeah, I heard the name. Yes.  
**Speaker 2** 00:06:41:  Yeah. So anything that, any adverse event of any product, whether that is a dosing thing or a side effect, or it has to be reported within one business day of it, we, and the rules a different slightly. Sometimes it's from when I, as a researcher read the data. Yeah. But sometimes it's from when the patient or doctor says it. So if you're doing an online community, for example, someone's gotta be monitoring it really regularly to make sure that you don't miss it because then you miss your window of reporting. Mm.  
**Speaker 1** 00:07:10:  Sorry. So yeah. Okay. If, if I understood correctly, so you're saying that the, the kind of data or the quality of data is evolving so quickly that if those tools are not being updated regularly, then the automated possessions or the AI driven possessions may not be useful for you?  
**Speaker 2** 00:07:29:  I think it's the other way around, actually. I think they're getting better. I think most, most of the suppliers that we were talking to, to start with had developed their models based on consumer panels. So they just didn't have that understanding. So they would translate drug names as random English words, you know, um, but they are getting better. So I think over time that's improving and it will become a bit more fit for purpose. Um, so is otherwise I think most things, I'm trying to think what we use. So things like speech to text, they, they have to be trained on health things like, um, virtual eye tracking we use, and I think that probably applies like humans are humans wherever you are. But there is some debate at the moment about whether it still applies in China and Japan and Korea, for example, if your some, if your panel is developed based on people in, in the UK and America, those sorts of things.  
**Speaker 2** 00:08:20:  Um, there was something else I thought of then. I forgot what it was. Oh, yeah. Simulated data. I'm not had much experience with simulated panels. Um, I just haven't, people are talking about it. I get what it is, but I have not embraced that yet. But I do wonder when we come to that phase, you know, they're not simulated panels of doctors. I suspect they're gonna be simulated panels of people that buy products or, you know, it would, it would have to be built with doctor. 'cause they talk differently. They're much less emotionally connected to brands, for example, those sorts of tools that, you know, they're, they're trained not to be as emotional. That doesn't mean that they're not emotional. They absolutely are, but it's harder to find, they're not as connected to brands as you would be to, you know, if you were purchasing something.  
**Speaker 1** 00:09:05:  Yeah, right. That's very interesting. But my, my background is actually from data science, computational modeling and simulations. Okay. So, um, I, yeah, I'm, I'm very in, I'm coming from particle physics background, but for the last 14 years I've used the data science and all these data analysis techniques. So I'm very interested, uh, to understand a bit more about the simulation. When you talk about simulated data, are you like trying to see that when people use examples of user personas, like a different kind of example that this is a user story. So you want to basically to give that you want to create a simulation, so how it'll interact within a certain environment or c can can you tell me a little bit more about, uh, what do you call simulated data? I'm  
**Speaker 2** 00:09:51:  Very interested. I can try. I, I'm, I think my background is sort of the opposite of yours in the sense that I still have music on cassette and my, my children need to program the TiVo box for me. But, you know, so I'm a bit of a late adopter. Um, my understanding is when I listen to people talk about it, you know, when you go to conference and they're saying, how is AI gonna change qual research? Um, and the, the people are talking about where we would have a panel of doctors that sit there and we can go to them with any kind of question and they would answer it. Now you can do it without having to have the doctors present. So there there's less fatigue. You don't have to pay 'em as much. It's quicker, you know, then drop out. You get good response rates. The kinds of questions they're asking. I don't know so much my concealer colleagues, I know it in a lot with advertising testing. So you run your visual concepts or your, you know, your messaging past the panel and you can predict that people don't like this color. People tend not to like that word. You know, you can change the phrasing. I assume that's what it is. But that's an assumption on my part. And a ad testing actually is relatively small part of what we do.  
**Speaker 1** 00:10:56:  Yeah, yeah. I'm just concerned here about one tiny thing about introduction of bias in your simulated data because, uh, when, when you're talking to actual cases, they can say anything. The situation could be anything. But when you're creating simulation, those are like a hundred percent controlled by your understanding what you want them to do. So I'm wondering that, uh, how do, do you have any idea how to remove the bias from that simulated data?  
**Speaker 2** 00:11:26:  No, and it's one of the things that stops me embracing it a bit at the moment, to be honest, that it, I worry that it will not, that the answers will not be reflective of the kinds of respondents that I talk to. And I worry that, uh, it will, there will be an element of bias in it for sure. Mm-Hmm. \<affirmative\>. And the same thing with, um, you know, like the large language models, you only get out what you put in, don't you? So, you know, if we're building data built on, um, electronic health records, for example, with prescribing, you know, and there's a bias in prescribing, you're gonna get that bias played right back to you. So we know, for example, that women are undertreated that, you know, people of color are often undertreated people of low socio, um, economic groups are undertreated. And then do you get that played back in terms of likely diagnosis? Probably yes. Or treatment choice. So I think it's, um, we talk a lot about everything's ai we talk a lot about, but are we actually using it? Probably not so much.  
**Speaker 1** 00:12:20:  Yeah. Let's say that. Not not, yes. Yes. Not, not yet. And, uh, in terms of leveraging AI in qualitative research, can you describe some of the recent example where, where, where you have actually started to use it and uh, you were not that scared and you, you actually liked it or you found it useful?  
**Speaker 2** 00:12:41:  I really like the, the speech to text function, and I'm very interested in the future where that might go. So we've, we did work with one supplier that offered thematic analysis. So you, you put your one hour transcript in and then it transcribes it for you, which was good. And then it tells you what the main themes are and you can question it. So what do doctors think are the biggest barrier to prescribing, for example, that bit I absolutely hated because it was just factually wrong. So I'm sure the tech will get there, but it wasn't there yet. Um, but I'm really interested in something that can take my transcripts and say these doctors, they speak differently to those doctors. They use different words and the language that they're using is not reflective of the language that patients are using. And it's not reflective of the language that you are advertising to them in, you know, there's, there's a disconnect and if you fix that disconnect, then you can help doctors and patients communicate better. You know, you can give PE patients better information, improve their wellbeing, improve their outcomes. I've yet to make that work yet, but I feel like that's in the very, in the quite soon future as opposed to, you know, the more distant future for some of the other things.  
**Speaker 1** 00:13:53:  Right. Yeah. Thanks. So, so just to understand a bit more, so, so you're saying that the current tools are not giving you the right masses from the same transcript? Uh, they don't understand the context very well, you mean? So yeah, more  
**Speaker 2** 00:14:11:  Example, um, the, we asked about, uh, I can't remember exactly what was drivers and barriers to prescribing a heart drug, for example. Uh, and we talked about what the drivers and barriers were, and I'm interested in what comes up spontaneously. And then after about 45 minutes of interviewing, we said, well, what if this happened? Would that be a problem? And the doctors all went, oh yeah, that would be a problem. So when I asked the ai what's the biggest barrier, they said that thing that I'd prompted on at the end, but nobody had thought of it spontaneously. It's no current barrier. Like it's probably unlikely happen, but because they'd all said, yeah, that'd be a big barrier, that's the bit it picked up and it didn't seem able to kind of take into context the fact that that all the spontaneous stuff, a whole 45 minutes out of an hour long interview to talk about that at all.  
**Speaker 1** 00:14:57:  Yeah. So, so yeah, picking up on words could be powerful but also could derail the whole discussion if we, if you realize like really just picking up words and not understanding the overall context and Yeah. Did I understand correctly?  
**Speaker 2** 00:15:12:  Yeah, yeah. Um, I didn't talk about chatbots either. I dunno if that's interesting, but like mod chatbot moderators. So I hear a lot about, um, scaling up qual, you know, qual at scale. And to me that's not qual. It's something else. It's not quant and it's not qual either. It's somewhere in the middle. And I think there's a place for that. But I don't know, as, as a health researcher, we spend years being trained on helping people feel comfortable allowing them to talk about topics that are sensitive. Yeah. And I do get that maybe that level of removal and be quite good. You know, maybe some people prefer to talk to a chat bot. Mm-Hmm.  
**Speaker 1** 00:15:48:  \<affirmative\>. But  
**Speaker 2** 00:15:50:  A lot of time they don't. And then the difference between, you know, if somebody says, do you take your medication? And you go, oh yeah, I always take my medication. And what they mean is I always take it, but it's four hours late or you know, oh, I better say that I take it because you know, I'm gonna be in trouble. If not, or you know, I do take it this week, but I didn't take it last week. Like, the chat bot's not gonna pick up those things.  
**Speaker 1** 00:16:11:  Yeah. They will just pick up the word and, uh, yeah, they, they'll everything. And what, what, what do you think if it's happening like a live video interview and if chat bots or other automation or AI tools are also like scanning the facial expression or sentiment analysis and trying to figure out if person is kind of holding up the information or you think it's too much at the ethical, uh, conflict border that uh, we shouldn't even think about that.  
**Speaker 2** 00:16:39:  I dunno. I mean, I know that our company has been using it for, um, tracking responses to, uh, moving ads, like video ads, like TV ads. Mm-Hmm. \<affirmative\> um, in health we don't use that simply because they don't do TV ads for doctors very often. That's what I was gonna mention. Um, okay. So, um, sorry, my husband's on another call. Um, no  
**Speaker 1** 00:17:01:  Worries.  
**Speaker 2** 00:17:02:  Um, if it's loud, let me know 'cause I can move to the kitchen.  
**Speaker 1** 00:17:04:  No, no, that's totally fine.  
**Speaker 2** 00:17:06:  Um, so I, I don't have an issue with doing that. I dunno how reliable it is. It just doesn't really apply to my business because I, I think, I think there's so many reasons why, particularly if you know, you're talking to a chat bot, that I would be less respectful if I was talking to a chat bot and talking to you. You know, I might check my phone or, and there's lots of reasons why my facial expression might change.  
**Speaker 1** 00:17:31:  Oh yeah. That's very interesting. Yeah. Yes, indeed. And, uh, so like o over the last many years you have, uh, uh, moved from one management role to another management role and also in terms of qualitative research, you had very different roles. So, uh, how, how has your approach to the, uh, qualitative research world, uh, over the years and uh, especially in the light of, uh, these emerging technologies like AI these days?  
**Speaker 2** 00:18:01:  I think it's difficult because I think that it's very tempting this, like I said, this whole idea of quality scale. It's quick and fast and cheap and there's a bunch of open-ended questions. It's very tempting for our clients. I think it is quite different to what we sell, which is much more expensive and slower and bespoke and sensitive. And I maybe there's a place for both. I, I'm, like I said, I'm late to adopt quite often, but I think the world is changing and if we don't adapt, you know, it is going that way, there isn't a choice. So we need to adapt better. And maybe there are some things that need a very sensitive in-person approach and maybe there are a lot of things that don't, especially as the world gets more used to talking to chat bots, you know, it becomes more commonplace.  
**Speaker 2** 00:18:44:  We're used to it all the time. We did try a while ago doing, um, uh, interviews through, um, smart speakers and it was absolutely awful. I mean, if you have ever, I've tried to get mine to ask me when, when it's gonna rain and like four hours later she's still going, I'm sorry. I dunno the answer to that. You know, every time someone stops speaking it close the question and asked a different one ask. But the tech will improve and the large language models will get trained on health. So I think it will go that way. But I think there, I'd like to think there will always still be a place for a human connection for the rapport that you build.  
**Speaker 1** 00:19:17:  Sure, sure, sure. And like, like previously you said that when you're talking to chat bot, you would be less respectful. And with the rise of ai, if there are like more automated tools available in the market and people are interacting more with those chat botts or auto or AI tools, do you think that the human psyche will go by default towards becoming more rude and that could impact society? And could you consider like \<laugh\>,  
**Speaker 2** 00:19:44:  I dunno, I, there is something about being a keyboard warrior, you know, having that level of being removed from like you just don't care. Um, gosh, I hope not. That's a very depressing thought, isn't it? I think that the, the chat bots of the AI moderators are really good for finding out facts quickly, particularly if they can make the experience as a respondent more fun, simple visual. They're less good I think at finding out the deeper and why, why,  
**Speaker 1** 00:20:10:  Right? Yes. Yes, indeed. And, uh, so like you have also worked in the market research, but in like different sectors. So how would you like to tell a little bit more about the common, uh, inefficiencies that you have observed in traditional qualitative research processes in, in various sectors and uh, how do you think AI could address them or some, some low hanging fruits?  
**Speaker 2** 00:20:40:  Um, I've only really worked in pharma, so I've only got the one sort of benchmark talk from, I hear my consumer colleagues talking about having AI to write discussion guides for you, that doesn't work. It works sometimes for me. So for example, ad testings, if you're testing an ad, the questions are gonna be quite formulaic. AI could definitely help me with that, make sure I haven't missed anything. Write a first draft for me. But if I want to find out about what's your experience of living with lung cancer, I wanna write that myself. So I think it is a good, it's a bit like having a junior researcher. I think it's a bit like someone to do your first draft. Where I found it particularly successful is just helping me get away from that white page. You know, you're looking at a blank sheet and you're like, I know what I wanna say.  
**Speaker 2** 00:21:26:  I just dunno where to start. Sometimes it can be very helpful to just give you a, some ideas to get going and then you hate all of them, but now, because you hate all of those, you know where you wanna go next. So things like, um, um, I say I don't use it actually for writing background sections for proposals because again, health is so specific. Um, but I can see that it, it could do that in terms of 'cause human discussion guides. I think it could take some of the work out that the big thing for me is analysis. Content analysis used to take hours and hours and hours and now I can get speech to text, I can get it done instantly and I still gotta check the transcript 'cause it's not brilliant, but I can check it in an hour rather than, it used to take me about four or five hours to transcribe an hour and a half's interview. So that's really good. And I think if it can help me spot patterns in that data or spotlight like patterns in language that would've had to done manually, then that would've taken days. You know, at least I can look at it and go, yeah, I don't think that was interesting, but this one might be. And  
**Speaker 1** 00:22:29:  Yeah, that's, that. That's very interesting. Uh, but here we are talking about only this piece to text, uh, when, when you have the transcription from, uh, interviews and what, what do you think, is there anything else where AI can also help in your overall data analysis? And not just data analysis, but also data visualization or at the end, like, uh, presentation or helping you to, uh, summarize your data in a way so that you can talk to different audience. Like when you're talking to management board, you would, you, you're not probably discussing every tiny technical detail, but when you're talking to your team, maybe you are interested in different kind of setups. So do you see, uh, the opportunity of AI to help you there or have you already encountered something like that?  
**Speaker 2** 00:23:20:  I have not actually \<laugh\>, that's not something that, um, that I have considered. I think our, my quantitative colleagues that are looking at more numerical presentations, I know they do a lot of data visualization stuff that's automated. I, I don't have experience of that. The qual work that we do is very individual, so it's very difficult to make it, um, to automate it in any way. Um, I, I can see the potential for that. I can imagine asking something to help me reframe something in a language that is easier to understand or more formal. I'd never thought about it though before now.  
**Speaker 1** 00:23:58:  Mm-Hmm, \<affirmative\>. Okay, great. And, uh, let's say when you're preparing your product, like qualitative research product, your product planning, but then you're finding participant and then you are collecting data, uh, how often do you collaborate a lot with people to make these decision? Or, or, or you have like a, uh, you mostly decide these things by yourself because if you're collaborating, then you also spend a lot of time to get back those decisions and to communicate. So how do you manage this whole research journey from product planning to, uh, data collection and, uh, data analysis and at the end sharing the results?  
**Speaker 2** 00:24:42:  Um, well the up the bit, once I decide I wanna talk to 10 oncologists in Italy, for example, that I then will, that part is outsourced to specialist recruiters. So they, they hold panels of doctors who are willing to participate in research. They set up the appointments, their Italian moderator will go and do the interviews, um, and then they will feed back the audio. So the, there's no, I don't do anything in between saying, I write the questions, send them off, and then they come back with the audio.  
**Speaker 1** 00:25:11:  Okay. So the, so, so the agencies will take care of, uh, how the product is continuing, what can information they're collecting and they will hand it over to you, uh, just start here and you can analyze it in any way you want?  
**Speaker 2** 00:25:23:  Yes, exactly.  
**Speaker 1** 00:25:26:  Hmm. Okay. Great. And so what, what, what would be your advice to the research team or organization that just started to explore the use of AI in the, in, in your specific sector?  
**Speaker 2** 00:25:42:  Um, I guess it depends what you mean by ai, doesn't it? So whether you, whether you're thinking what kind of AI you're thinking of, it feels, feels like it's so many different things.  
**Speaker 1** 00:25:52:  No, I mean, uh, I, I understand they, they, the overuse of AI these days, because in in market many people are just using simple automation is sold as ai Yes. Simple bots, which are not really intelligent. That is just, uh, some \<inaudible\> statements running in the background that if somebody's asking this question, give them this answer yes. That that is not ai. I'm talking about somewhere, uh, for example, like, uh, you know, when, uh, there is a transcription available and then it can provide you automated summaries or something that can actually help you, something that has a hint of intelligence and not just automation.  
**Speaker 2** 00:26:31:  I would love that I wait for the day that that happens. That takes some of that. My worry with it is that at the moment I don't trust it. And I worry that people, people humans naturally are quite lazy. You know, if you, if it looks like a duck and it quacks like a duck, they're not gonna go and check it is actually a duck. So are they gonna pick up the hallucinations? And if they have to spend so much time looking for hallucinations in data, is there any benefit in having it start with? So I guess it'll come eventually it'll come, but I don't wanna be the person that's testing it out in the middle  
**Speaker 1** 00:27:04:  \<laugh\>. Okay. And can, can you tell me a bit more about these, uh, I understand there's, uh, a resistance to go towards, uh, or reluctance to go towards a new tool of the market, but, uh, can you tell me a bit more about these biggest barriers or, or the challenges that, uh, people can face in adopting AI in qualitative research in your field or, or in general, and how can we overcome that?  
**Speaker 2** 00:27:31:  So, um, let me see, the biggest challenges I there, I think there are some technical challenges. So do, for our company, for example, we can't work with any competitors. So if we don't have a product ourselves, we can't use anybody else's. So there's a technical challenge from that point of view. There's a, there's a, a general sense of mistrust. And I think because we don't understand quite how everything works, we are on the side of caution. So things, all the questions that I probably wouldn't ask that my colleagues ask, like, you know, how's your data stored? Where in what country? Where's your GDPR? You know, where'd you hold your data center? What you gonna do with my data? How do I know? Is it gonna be out there? Is it training chat GPT open ai, or are you gonna have a closed system? And if it's a closed system, how are you gonna train it well enough so it understands if you've gotta train it from scratch, you know, it has to be trained on something so, you know, is it ready? Um, and then there's the whole thing about do we trust it to be true?  
**Speaker 1** 00:28:43:  Yeah, yeah. So yeah, d data privacy is like your biggest concern. Your, your biggest concern is data privacy  
**Speaker 2** 00:28:51:  For my com. For me personally, maybe not so much. I I, I'm a bit oblivious to it. I just assume it's gonna work. And if you tell me it's private, then I'll just go, okay, it's private. But for my, my company is, uh, was very, very cautious and risk averse with regard to data security. Yeah,  
**Speaker 1** 00:29:08:  Right. Thanks. And let, let, let's say in, in ideal conditions that data security is solved and people are trusting it. Yeah. And uh, also from technical point of view, let's say whatever you wish for is, is granted. It's there, all the technical advancements are there. So in that ideal scenario, what, what would be your wishlist that you would like to see from, uh, ai? How, how it can really, uh, make, uh, the, uh, the day-to-day job of a qualitative researcher or an organization, uh, where many people are working together. Uh, how can it, uh, what, what, what, what are your reason for that kind of, uh, AI application that, how it can really help you better? Just, just some points.  
**Speaker 2** 00:29:53:  Okay. So generally, um, I would love it if it could take my 30 transcripts and tell me the themes accurately and maybe find some themes that I haven't spotted. Like I know it's gonna be product efficacy and safety. I know that's coming up, but maybe there's some themes amongst these people that I didn't notice. So I'd love it if do that, find some subtle themes. Um, I thought something else and then I forgot it. Hang on a minute. It'll come back to me. Oh, yes, I would love it. This is a slightly different topic, you know, internal filing, like everybody, how did, did we write a proposal on that about five years ago? Has anyone got that proposal? No, no one's got that proposal. You know, that sort of knowledge retrieval internally, um, that, yeah, that was amazing. So, you know, we've, we know we've done six studies on liver cancer, but we can't remember what the main products are or you know, what the issues were in the market at the time. Some way to kind of collate and retrieve that data that isn't manually scrolling through SharePoint files would be amazing.  
**Speaker 1** 00:30:53:  Yeah, that's very interesting. We are actually going into that direction, like when, uh, we, we are developing something, when you will interview people and, uh, uh, you are collecting a lot of words. Uh, you will have a lot of interviews and then, uh, your personalized, uh, research assistant, uh, will be there. It's an AI driven tool, so you can ask it question that, what is the answer of this question based on my data? So it'll go and of go through different kind of, uh, interviews that you have, uh, performed in that particular project space. It can give you answer, it can also compare the quality of answers, uh, given in different interviews. So I'm very happy to hear. Oh,  
**Speaker 2** 00:31:38:  Wow. Okay.  
**Speaker 1** 00:31:38:  I'm very happy to hear that. Uh, uh, you, you are interested in that.  
**Speaker 2** 00:31:43:  Yes. Yeah, that would be really good because you, you sort of gather all this knowledge and then bits of it drop out of your brain as you go over time, or you can't quite remember what the file was called. So trying to write the prompt is really difficult. So something that was a bit more intelligent that could say, did you mean one of these six things that would be really good.  
**Speaker 1** 00:32:00:  Yeah. So yeah, very efficient data management so that you can quickly get it just like you're doing it with your brain. You think about last year it happened and quickly it's coming. It's not taking time. Yeah. Right. Thank you. Thank you very much. I really enjoyed, uh, talking with you. Do you have any question for, for me?  
**Speaker 2** 00:32:21:  Um, do you think that you will have something in the, in the pharma space, or do you think you'll have something more general over time?  
**Speaker 1** 00:32:29:  Uh, I mean, we, we, uh, we want to target, uh, both like healthcare sector and, and FinTech in the beginning, but I really understand your reasons that we need a bit more training for, for pharma sector as well. Uh, because the rules and regulations are a bit stronger because we are dealing with very sensitive health data as you have, uh, explained. Uh, we will definitely be providing something for healthcare sector as well. Uh, but in next two to three months, we are launching our, uh, MVP, uh, first time in the market. So yeah, uh, it's definitely in our plans, if not interesting this year, but then, because the quality research is not really a sector specific thing, right? It's a, it's a, in every sector people are doing that, either FinTech, healthcare or, or any, any other sector. Uh, but, uh, yeah, when it comes to ai, we need to make sure that, uh, it's understanding the context and that field very well, because the same words may have a very different meaning in, in different sectors. And then if your tool is not understanding that, then the suggestions coming out of that could be very misleading. So instead of empowering humans, we could be really misleading. So, uh, yeah, we, we are very cautious about that concern and we will try to remove that, uh, that, that bias as well. But in terms of sectors needs, we, we, we are definitely targeting healthcare as well.  
**Speaker 2** 00:33:56:  Interesting. Oh, well, good luck.  
**Speaker 1** 00:33:59:  Thank you very much. Uh, thanks again for, for your time and, and talking to us. Uh, I will, uh, send the Amazon voucher through on your emails very soon. Oh, lovely. Thank you. Okay, great. Thanks. Thank.

# 

# Lisandro Diaz Villarruel

Senior Analyst specialised in Business Intelligence & Consumer Behaviour | Social Listening at Brandwatch | Associate Editor at E-International Relations

## Meeting summary:

Meeting Transcript Summary

**Percentage Talktime:**

* Speaker 0 (Interviewer): 41.7%  
* Speaker 1 (Interviewee): 58.3%

**Meeting Purpose:**

To understand the experiences of market research professionals using AI tools, identify inefficiencies in current workflows, and explore potential areas where AI could be helpful in market research.

**Key Themes:**

* AI adoption in market research  
* Challenges and limitations of current AI tools  
* Potential for AI to improve efficiency and accuracy in research  
* Importance of human expertise and oversight in AI-powered research

**Key Topics:**

* Use of AI for technical setup and research analysis  
* Differences in AI usage between qualitative and quantitative research  
* Challenges in sentiment analysis and understanding nuanced language  
* Benefits of AI as a sparring partner and second opinion in research  
* Importance of training and trust in AI adoption  
* Ideal AI features for market research (e.g., contextualized AI assistant)

**Key Actions:**

* Interviewee shared experiences and insights on using AI in market research.  
* Interviewer provided an overview of Beings, a startup developing AI products for market research.  
* Both parties agreed to keep in touch for future collaborations.

**Key Sentiment:**

* Overall positive and optimistic about the potential of AI in market research.  
* Acknowledgment of current limitations and the need for further development.  
* Emphasis on the importance of human expertise and responsible AI use.

**Quantitative Insights:**

* Interviewee works 70% with qualitative data and 30% with quantitative data.  
* The majority of AI usage is in the initial stages of a project, particularly for technical setup and desk-based research.

**Qualitative Insights:**

* AI is most helpful as a sparring partner and second opinion, but not yet reliable enough for full reliance.  
* Trust and authenticity are key concerns in AI adoption.  
* Greater transparency in AI methodology and reasoning would enhance trust.  
* A contextualized AI assistant tailored to specific research projects would be highly beneficial.

**List of Questions and Brief Responses:**

1. **Question:** Could you please comment on the kind of research you do when talking to different clients? Do you tend to use more qualitative methods or quantitative analysis as well?

   **Answer:** Currently, 70% qualitative, 30% quantitative. In previous roles, this was inverted.

2. **Question:** When you're working with clients, do they just hand over some data or quantitative-related information? Or do you interview them and collect qualitative data? How do you work with your clients?

   **Answer:** Works in social listening, with access to a large amount of social data. Segments the data to bring relevant insights to clients.

3. **Question:** Can you tell me a little bit more about these things you mentioned, like social listening and consumer intelligence? How is AI transforming the way companies gather and analyze this qualitative data from social media and other online sources?

   **Answer:** AI is useful for technical setup, like creating Boolean searches, and for research analysis, like verifying hypotheses and checking for relevant news.

4. **Question:** For different sectors or contexts, you still use AI, but how do you increase your trust level? Is it by involving more humans, or is there any other way you can train your AI tools to increase trust depending on the context?

   **Answer:** Training is key, but also developing a mindset to apply AI at the right moment and not relying on it too much.

5. **Question:** When you are using AI-powered tools for qualitative analysis tasks, do you use it for sentiment analysis or thematic coding, or how do you use it for your current work?

   **Answer:** Doesn't use standard AI tools for sentiment analysis. Instead, uses an in-house tool that still requires human input.

6. **Question:** How long have you been using that tool, and do you see any obvious disadvantages where you feel that AI can jump in and improve the performance of that tool?

   **Answer:** AI is not yet polished enough to understand irony or negative points discussed in a positive context. It needs to develop its understanding of human language, especially in informal contexts.

7. **Question:** If you have to give me some examples of when you're using AI tools, either in qualitative research or with quantitative nature products, but in different sectors, can you highlight some of the most important use cases of AI in different sectors?

   **Answer:** Two main uses: summarization and review of large datasets (quantitative) and getting a second opinion or testing hypotheses (qualitative).

8. **Question:** Can you walk me through some typical qualitative research projects that you managed, starting from project planning, finding data or participants, coding data, analyzing, and eventually interpreting or presenting the data to different stakeholders? At which step do you think AI could be most beneficial? Have you been using AI other than all the use cases you mentioned before?

   **Answer:** Uses AI in the initial phase for desk-based research and in the setup for things like synonym lists. For analysis, uses AI briefly to investigate ideas but relies on in-house tools. AI is not yet trusted for analysis or presentation.

9. **Question:** Can you give me an example of some of the tools that you use, either for data recording or data analysis, for both qualitative and quantitative sides? Just name the tools

   **Answer:** Sentiment analysis, emotion analysis, trend analysis, and analysis of trending topics or key buzzwords. All tools are in-house.

10. **Question:** Let's say there is no problem, in an idealistic scenario, what would be your wishlist in your whole product journey, from product planning to the final step when you are even getting final presentations from AI, based on the data that you're feeding? In this whole journey, where do you think AI can specifically help you more, or if there is some kind of repetition or anything else that I might have missed, in an idealistic scenario, what do you want AI to do for you as a researcher?

    **Answer:** In an ideal world, a contextualized AI assistant tailored to the specific research project that can understand commands in natural language and provide trustworthy information from the project's dataset.

11. **Question:** Do you have any suggestions on how we can improve that trust? What is the way to measure the trust there? Because if we have to quantify the trust, not everyone is an expert at the same level, AI is basically just a tool. How can we ensure that it's trustable?

    **Answer:** AI should provide clearer methodology and reasoning behind its output, like Perplexity does with references. This transparency would nurture trust.

## Transcript

**Speaker 0** 00:00:01:  Pre, I'm a product manager in the search and insights at a startup called Beings. We are developing AI products for market research, and, uh, currently we're talking to experts like you to understand better that, uh, what are your experience, uh, for using GI tools through different tool, uh, uh, you know, tools already available in the market, and, uh, what are the, uh, current inefficiencies that we can try to understand better where AI could be helpful. Uh, so yeah. Uh, this other participant is data our, uh, note taker that will record the video, uh, and audio and provide us a transcripts. Uh, yeah. Uh, can we start, uh, with you explaining a little bit about, uh, your job details, uh, like your day-to-day responsibilities and Yeah. What kinda work you do?  
**Speaker 1** 00:00:49:  Yeah, absolutely. So currently I'm working as a senior research analyst in, uh, Brandwatch, which is a \<inaudible\> company. And, uh, basically my day-to-Day. It, it varies, uh, depending on, uh, you know, where we are in the, uh, project life cycle, but, um, it could be mainly divided in, uh, three stages. One of them is the technical setup. So, uh, basically, uh, utilizing bullion, uh, to create the setup for, for our reports and be able to segment data, create the, uh, necessary dashboards that we, uh, then use to, uh, move on to the next stage, which is, uh, basically building, uh, basically build, building the, the, the research for our report, starting to, to write the report, uh, draw the insights, start creating the, um, yeah, all, all the opportunities, all the actionable insights, the recommendations. And then, uh, the la the latter part of the report would be the delivery, and of course, any feedback revisions that the client, um, uh, yeah, uh, once.  
**Speaker 1** 00:01:56:  And, uh, I guess on the topic of ai, I mainly, uh, utilize AI in the form of two steps. So I, uh, tend to, uh, use chat GPT and perplexity. Um, for the technical setup, I actually prefer using GPT for, uh, Boolean and, uh, for all things technical and when I need some assistance with, uh, the analysis, um, I tend to use perplexity, which I find there is a very, uh, a very, a very effective, uh, tool when it comes to, um, yeah, basically, uh, deep diving in, into different research topics. Uh, in my organization, we tend to, um, work with very different clients. Uh, the requests that they make for each of the projects tend to be, uh, quite tailored, quite personalized. So, uh, yeah, a AI really helps, um, when it comes to, uh, gaining an insight into, uh, perhaps a particular industry or for a particular product. Um, so yeah, I, I, I, I guess my use of AI does depend, uh, depending on where we are in the project.  
**Speaker 0** 00:03:00:  Yeah, very agree. Yeah. Thank you very much for the brilliant overview. Uh, could you please comment on, uh, the, the kind of research you do, uh, when you're, uh, talking to like different kind of clients? Uh, do you tend to use more qualitative methods or you you also use quantitative, uh, analysis as well?  
**Speaker 1** 00:03:18:  Yeah, so, um, at the moment in my current position, I would say I work pretty much 70% of the time with qualitative data, and then 30% of the time with quantitative data. In my earlier roles, uh, well, in my earlier role at, at Nielsen, actually, uh, Nielsen iq, that is, I, um, I would say probably that was inverted. I'd, I'd say I worked like 80 or 90% with, uh, quantitative data, and then the rest was, uh, usually, uh, you know, client verbatims, uh, that, you know, were basically quo analysis.  
**Speaker 0** 00:03:51:  Yeah. Right. Great. And when you're working with clients, uh, do they just, uh, hand, hand hand over you some kind of data or quantitative related information? Uh, or do you interview them and do you collect the qualitative data? How do you, um, how do you work with your clients?  
**Speaker 1** 00:04:09:  Yeah, actually, um, well, yeah. My, my industry at, at the moment is, uh, social listening. So what we do in, uh, in essence is, uh, well basically brand which has access to, um, I would say a, a, a, a very big part of, uh, all, all the social data that can be found in, for example, social networks as X, uh, Facebook, Instagram, uh, online news, basically all of the internet bar, some of the, um, sites that obviously have, uh, you know, certain privacy settings that, you know, don't allow us to, to search. But yeah, like I said, in essence, we have access to pretty much all of social media. And, uh, that's where the technical, uh, setup, uh, part comes in. Our job is to, our job as analysts and consultants is to segment that, uh, that large pie and bring to our client the relevant data. Um, and again, that really depends on, uh, their, uh, you know, project by project, uh, needs.  
**Speaker 0** 00:05:14:  Yeah. Brilliant. Uh, can you tell me a little bit more about this, uh, these, uh, yeah, the things you mentioned, like social, uh, listening and consumer intelligence. So, uh, can you gimme some more example that, how AI is transforming the way companies gather and analyze, uh, this qualitative data from social media and other online sources?  
**Speaker 1** 00:05:35:  Absolutely. Absolutely. So, um, I think, well, yeah, uh, I'll give you, I guess two, two examples of what I mentioned. I, I guess in, in my experience in the former part of the project, in, uh, all things, uh, set up, uh, it's very useful to, uh, I, I wouldn't say rely on AI because, uh, obviously the analyst has to, um, you know, take, uh, take responsibility, take accountability for, for what is done, for what we obviously then use to, um, create all the, all, all, all the research that we then present to our, to our clients. But certainly, AI is a, uh, is as of recently, my, my, my best friend when it comes to, for example, the technical setup, it's, it's very useful. Uh, well, just to give a bit of context, we work with Boolean, so we use a keyword based search. So it's very useful to, uh, use AI almost like as, as a sparring partner.  
**Speaker 1** 00:06:27:  So if I'm, uh, for example, working with a project, um, with, I don't know, TikTok, it's, and they, they wanna know how the rugby World Cup, when it's always useful to understand, uh, you know, may maybe what keywords, uh, people are using to refer to the, to the World Cup, to, uh, meta's, uh, involvement in it. Um, and yeah, for this part of the, of the report, uh, well, rather o of the project, I, I, I like to rely on, on, on chat, GPT, uh, given I've seen it's, um, yeah, in, in my experience, tends to have less errors than OO other, um, ais, uh, when it comes to, uh, outputting, uh, for instance, a line of bullion, a line of code, um, and it's good to, you know, bounce off with different ideas to see which keywords are going to be, uh, more, um, yeah, are, are gonna be appropriate for, for this, uh, part of the project.  
**Speaker 1** 00:07:26:  But then, for example, for the, uh, research, uh, slash analytical, uh, part of the report where we, uh, basically have to jump into all the data, read it, um, it's always useful to a, again, as a sparring partner, use the ai, uh, to verify any hypothesis you have, uh, check for any technicalities, uh, perhaps see if there's any other relevant news that could, uh, fit into the, um, into the, yeah, into the whole of the research. Um, additionally with our in-house, um, social listening tool, we, we have, for example, uh, an ai, um, feature that allows us to analyze the peaks in conversation, for example, uh, for a given timeframe, which is, is very useful. Um, Brandwatch in itself also has a, has and is developing an, uh, its own in-house, uh, generative ai, uh, tool, uh, to be able to, uh, summarize findings, uh, draw, uh, key insights and, uh, you know, obviously aid in the whole, uh, recommendations and, uh, strategy, uh, process.  
**Speaker 1** 00:08:33:  Uh, I guess one caveat is that none of these, uh, examples I've given are, um, I would say something you could trust a hundred percent. There still needs to be a, a, I would say, a pretty heavy, uh, human, um, QA process, uh, to ensure that the, uh, ba yeah, basically that all the output that you've asked, the, the AI to, uh, to kind of give you, uh, to ensure that this fits your needs. Um, 'cause, you know, of course, um, as we all know, um, things are not, uh, standard that, uh, we all think they are. Um, and, and, you know, AI is very shiny, but still that, that there is a lot of, uh, human input that must be, um, yeah, that, that must be put to, to ensure that everything runs smoothly.  
**Speaker 0** 00:09:18:  Yeah, I definitely, I totally agree. Uh, uh, it is just like the beginning of, uh, the use AI everywhere, and, uh, we need to find a way that, uh, how we can increase the trust. So I'm also very interested there when you mentioned that, uh, for different sectors or for different context, uh, you still use ai, uh, but how do you, uh, how do you increase your trust level there? Is it like involving more human, or is there any other way that, uh, you can still do some sort of training of your AI tools to increase the trust, depending on, uh, the different kind of context as well?  
**Speaker 1** 00:09:56:  Yeah, you saw the words outta my mouth. I think training is, is, is, is the key word in, in this, um, in, in this regard, I think, um, and I was having a, a really interesting discussion with a colleague of mine who, uh, you know, basically, uh, well, yeah, a team meeting, a, a knowledge sharing meeting where we were discussing the applications of AI in our line of work. And yeah, we basically came to the conclusion that most of the errors that come when using AI are not necessarily from the ai. I mean, sometimes, for example, I input a line of code, and the AI is simply unable to, uh, feed me back a, a valid line of code. Uh, again, uh, that, that's very much what you, what you mentioned, we are at the start of this AI revolution. Um, but I, I, I guess from my experience and, you know, all the, uh, mistakes I've made in the past, uh, by, uh, using AI perhaps heavily reliant on it, uh, I've come to realize, you know, indeed, I, I, I, I completely agree with you.  
**Speaker 1** 00:10:50:  It, it's, it's all about a training. And, uh, you know, even more than that, uh, developing a mindset to be able to apply AI at the right moment and at the right time. And, uh, again, not relying in it to much to, you know, basically, uh, keep the accountability, uh, within the human, um, as machines at the moment. It can't be made accountable for any, um, any mistakes. Um, but also, uh, I guess balancing out and, you know, being able to identify and say, oh, this is the moment I could use AI to be able to, uh, maximize the effectiveness and efficiency of this, uh, task, for example.  
**Speaker 0** 00:11:27:  Yeah. Yeah. Thank you very much. Yeah, that's, that's very nice. Uh, overview, uh, about your current work at Brand Wise, when you are using AI power tools for qualitative analysis tasks, uh, do you, do you use it for like a sentiment analysis or thematic coding, or how do you use it for your current work?  
**Speaker 1** 00:11:47:  Yeah. No, no, we, we don't use, um, well, in, in terms of sentiment analysis, we have our own tool that that does it. It's, uh, you know, certainly a very powerful tool. Our clients, uh, really appreciate us using sentiment analysis. Um, I do not use, uh, any, uh, I, I, I guess standard tool, uh, such as chat, GPT, perplexity or Gemini and, and so on to, uh, carry out sentiment analysis. We have seen that it, uh, it's still very, um, very green in that regard. And e even our own tool that, you know, it's been, um, how would they say this? Yeah, it's been performing for many years and, you know, they, we, we certainly have a whole team of engineers working on it. Um, you know, it still requires human, uh, human input to ensure that every, uh, for instance, mentioned that we, um, that we analyze is correctly, uh, categorized as a negative, neutral or, or positive, um, mentioned. So, again, uh, very much what we were, uh, discussing earlier, it, it, it still requires a bigger human input. And, uh, yeah, as a a, as sentiment analyst is a super powerful tool, it still requires, um, you know, uh, again, the analyst to ensure that everything is correct. And, um, yeah.  
**Speaker 0** 00:13:08:  Yeah, very glad to hear that you already have like, you know, tool that you feel like it's like already giving you much better performance than anything else in the market could have done it for you. So, um, how long have you been using that tool? And, uh, do you see any obvious disadvantage there that, where you feel that AI can jump in and improve the performance of that too?  
**Speaker 1** 00:13:31:  Uh, that's a good question. Um, well, a, a AI is that tool in, in the case of, for example, sentiment analysis or, um, peak or trough analysis, for instance. Um, I think where we are at now is the point where AI is, uh, a language expert without a doubt that that is, that is obvious. I think that's very clear. As soon as you, you know, you start to understand how chat GPT works in the, in, in the backend, how perplexity works in the backend. I'm gonna mention chat, GPT and perplexity a lot because I, I really love them both, and I, and I, uh, I use them a lot. Um, but I guess on the topic of sentiment analysis, I think that's really interesting for 'cause for example, um, yeah, we, we've, we've come to realize it in my organization, and I'm sure, uh, beyond that UMIs are not yet, uh, that polished to understand, for example, irony or to understand that maybe a, um, a negative point is being discussed in a positive context.  
**Speaker 1** 00:14:33:  And again, i, I, I guess, um, the, the whole notion of irony really links into that. Uh, but also, you know, for example, brand criticism, um, or, you know, I've recently been working in a project, um, with Lego around, uh, perceptions of plastic. And again, you know, uh, even when people were celebrating, I don't know, new technological advancements, um, in the realm of, uh, microplastics pollution, um, there was still a lot of inconsistency, um, with, uh, consumer, uh, well, actual consumer sentiment. Uh, so I think, uh, yeah, to, yeah, the, the bottom line is, uh, AI still needs, uh, at least the AI I use still needs to, uh, develop its understanding of human language, especially in informal contexts.  
**Speaker 0** 00:15:23:  Mm-Hmm, \<affirmative\>, yeah, that's very important indeed, because depending on the, uh, slight change in the context, the meaning of ai, AI could be very different, and then it would be very, uh, tricky to trust. Yes. The, you're getting, uh, so yeah, just to follow up on that, like, uh, you have worked in different sectors as well, uh, and if you have to, uh, gimme some examples that, uh, when you're using AI tools, either in qualitative research or with quantitative nature products, but in different sectors. So, uh, can you highlight some of the most important use cases of AI in different sectors?  
**Speaker 1** 00:16:01:  Uh, uh, yeah, of course. What, what do you mean with, uh, sectors? I guess? Do you mean like different industries or,  
**Speaker 0** 00:16:06:  Yeah, different industry, yes. Yes. Okay.  
**Speaker 1** 00:16:08:  Okay. Yeah, absolutely. Um, I would say, uh, the, the, the two main uses and, uh, what I, uh, have to thank AI most for, um, I, I, I guess in the, um, in the realm of, uh, you know, the, yeah, I guess the more quantitative, um, yeah, I, I, I, I, I guess in, in the most quantitative, um, uh, realm, you know, when, when, for example, we have, uh, you know, large spreadsheets, uh, we have where we have to sort data and, you know, go through, uh, I don't know, perhaps, uh, consumer, uh, comments or, or, or, or, or again, as you mentioned, uh, sentiment, it's, it's, um, it's a very good, uh, use case for summarization and, um, you know, basically reviewing everything, um, in a very neat way as well. I think after I'm done with, uh, you know, for example, analyzing a large data set, I always like to input that data dataset into an ai, uh, big caveat, an AI that can read it, because, you know, in many instances, I've, I, I, I've noticed that for example, chat, GPT is not a, basically, it hallucinates and it tells you, uh, that it understood the data and it gives you this, uh, conclusion, but actually it's, it's completely made up.  
**Speaker 1** 00:17:23:  So that, you know, that, that's a big caveat, I would say. But I guess to get a second opinion and that, and that's why I always call AI a, a sparring partner, I think it's, it's always good to, um, especially in a, in a remote setting for, for example, you're not, you're not, you know, uh, obviously off office time is, uh, has been vastly reduced since covid, uh, you no longer, uh, have the, uh, fortune to have to be sitting next to a colleague that will, um, perhaps, you know, you'll be able to bounce ideas off with them. So it's always good. I, I, I always like to use AI as a second opinion. Um, and as well, when it comes to the more qualitative part of the research, especially when I'm making, uh, you know, more assumptions or, you know, testing hypotheses and, uh, yeah, rather than given solid, uh, conclusions, um, that may, you know, lie, uh, outside of the scope of the project.  
**Speaker 1** 00:18:16:  But, you know, we, we still deem, uh, that this is, uh, you know, perhaps interesting, um, as a recommendation or maybe for further research, um, when we have, uh, you know, subsequent projects with the client. Um, yeah, it, it, it's always good to ask the, the, uh, the, the main questions, the, the, the main research questions that we have to, uh, again, get that second opinion, get that first hypothesis. And of course, you know, with the example of perplexity, for example, you know, it, it's a very, um, I consider it a very diligent process because it gives you like a lot of, a lot of examples, a lot of, a lot of references, and, uh, obviously for, uh, you know, clients, um, in the same industry might be useful to learn. Those, those examples that, um, yeah, that, that, that, that we, uh, that, that perhaps originated from, um, my sparring session with the ai, if that makes sense.  
**Speaker 0** 00:19:07:  No, yes. It's totally sensible. Yeah. Thank you. Uh, if we zoom out a little bit, uh, try to look at the bigger overview, uh, of the, uh, we were talking about research, but then if we talk about the project management under which the different kind of research is ongoing, uh, can you walk me through some typical qualitative research project that you managed? Basically starting from the product planning, and then you're finding data or finding participant, uh, and then how are you coding data, and then how are you analyzing? And then eventually at the end, uh, how do you interpret that data or, uh, you present that data to different stakeholders. So can you please walk me through this journey and, uh, at what, uh, at which step you think AI could be like most beneficial? And have you been using ai, uh, other than all the use cases that you mentioned before?  
**Speaker 1** 00:20:02:  Interesting question. Uh, yeah, I, well, I, I, I, I guess a brief, a, a brief run through of, um, how a project starts is, you know, we always, um, have an internal kickoff where we, uh, basically, um, yeah, de decide the, the, the shape that the project and the report are gonna take. Um, obviously that is, uh, then followed by a meeting with the client to better understand, um, their needs. Um, obviously that's very specific for, for, for, uh, you know, uh, yeah, for, for each project. Um, and then that's when we start building the, the setup. I, before the setup, there isn't a, um, a use case I've been able to apply to AI other than, um, learning, um, the, yeah, uh, perhaps I, I, I, I, I would say I use AI at the very start of a project to, uh, do some desk-based research in, into an industry.  
**Speaker 1** 00:20:56:  I personally work with a, a, you know, a very, um, you know, very different clients on a, on a weekly and monthly basis. So, uh, obviously I'm not a, I'm not an expert on it, on anything very, very, very, very far from that. So it's always good to, for example, use perplexity to understand, you know, what is, um, I don't know, what are the implications of, uh, plastic use, um, in this country. And, uh, what's general consumer sentiment, for example? Uh, since 20, uh, 21, um, it's, it's always good at the initial phase of a project to then be able to discuss with my, with, with my colleagues. Um, then obviously in, in the setup, I, I do like to use a, a AI quite a lot as I, as you, as you've clearly, um, heard. Um, but yeah, always good to care. Um, you know, given that we rely on keyword based, um, search, it's always good to get, for example, a list of synonyms or a list of different expressions, um, on, you know, how people express themselves in, in social media, which is, you know, vastly different to perhaps my previous experience in traditional, uh, market research.  
**Speaker 1** 00:22:01:  I, when I was working in more traditional market research, um, you know, AI wasn't as big as it is now. So, uh, you know, perhaps that's, that's also food for thought. I, I assume that, for example, when we created surveys, it would be good to, um, use AI again as a sparring partner to understand, uh, you know, how to frame something, how to avoid bias, which is, you know, super important in more traditional, um, methods of research. Um, and then of course, yeah, we, we undergo with the analysis, um, I, well, not only me, but everyone in my company, we obviously, we don't like to rely on ai. Um, when it comes to the analysis part, um, I use ai, uh, briefly to, you know, investigate some ideas, um, see if there's any other examples that that could be of, um, of use for perhaps any of the hypotheses or suggestions or recommendations I'm gonna put forward.  
**Speaker 1** 00:22:53:  Um, so yeah, I, I, I, I would say mainly AI fits, in my opinion, at the former stage of the, of the project, not at the latter. I think, um, when it comes to analysis, and then obviously when it comes to presentation and any feedback, I don't yet see how AI can, um, not that AI is not used. Uh, obviously when, um, I am preparing a presentation or perhaps when I have to, um, uh, adjust something in a report, AI is always present because, you know, I have to go into our platform, I have to analyze a peak. I, I use AI to, um, you know, tell me where the, what, you know, where and why a peak is happening in a certain conversation in a certain time period. So I wouldn't say, um, you know, yeah, I wouldn't say there is a part of the project that doesn't use ai, but there's, for me, there's certainly, uh, stages of the project that really, um, really benefit from ai.  
**Speaker 0** 00:23:52:  Right. So, so, so most of the benefit you think will come from the project, if I understood correctly, yes. From the project planning in the initial starting phase, but for analysis, you may still use ai, but uh, it'll not be part of the same big, uh, uh, product tool that you are using. You will keep going out of that tool. Okay. And mostly manual work. Yes. So can, can you gimme example of, uh, some of the tools that you use either for data recording or data analysis for both qualitative and quantitative, uh, side of the, just, just a name the tools?  
**Speaker 1** 00:24:24:  Yeah, absolutely. So, uh, you know, sentiment analysis is, um, I is a big one. Uh, another, another tool that, um, you know, again, still needs a lot of work is the emotion analysis. So, for example, uh, joy, anger, disgusted disappointment, you know, obviously a lot more specific and, you know, uh, something we can rely, um, a lot less on. Um, I, I've mentioned the analysis of trends, uh, but there's also the analysis of, of, of trending topics, uh, the, uh, the analysis of, of key buzzwords that, that consumers may use. Uh, well, consumers or, um, any user actually, um, online that would be, that, that would be in the main, if, if I were to say the two main ones that, that I use mostly are it's sentiment analysis and, uh, trend and, uh, yeah, trend analysis. Yeah, I, I I, I, I would summarize it in that because, uh, there, there is certain nuances to it.  
**Speaker 0** 00:25:22:  And are those tools already, like inbuilt, uh, from your company? Uh, are, are those inhouse? Are you, yes. Is there any other tool from industry that you are using from outside to, uh, to get like more deeper insights that you think that your in-house tools are not providing?  
**Speaker 1** 00:25:41:  No, no, no, no. It's, it's all in-house tools. Yeah. Brandwatch, I think is, is quite fortunate to have a very in solid, um, you know, backend and, uh, I guess a very strong software development team, uh, that is, uh, constantly updating, um, yeah, the, the, the tool we use, you know, comparable to, uh, power BI tableau, and all the, um, you know, well-known business intelligence tools that we use.  
**Speaker 0** 00:26:06:  Yeah. And when it comes to presentation to like different stakeholders, like if you have to talk to your expert of your level and then, uh, you have to give the similar information to someone from management board, do you think that the AI could help you there to give you tailored presentations or  
**Speaker 1** 00:26:29:  Interesting question. Um, again, yeah, like I said, I, uh, yeah, I think it's in, it's in the culture of my team and, you know, certainly I've, um, you know, learned a lot from that culture to not rely on ai, especially when it comes to, you know, the human to human interaction. And we like to sort of, uh, yeah, basically, as you very well said, you know, depending on the audience to tailor the, the report. I don't think my company just yet trusts AI to, to do that,  
**Speaker 0** 00:26:54:  So. Right. Thanks. And, uh, let's say that there is a no problem in, in idealistic scenario, there is no technical problem, there is no management problem, but what would be your wishlist, uh, in your whole product journey from product planning to the final step when you are even, let's say, even getting final presentations from ai, uh, based on the data that you're feeding, uh, in this whole journey, uh, what do you think where, where, where AI can specifically help you more, uh, the kind of work you're performing, or if there is some kind of repetition or any other thing that I might have missed, uh, in idealistic scenario, what do you want AI to do for you as a searcher?  
**Speaker 1** 00:27:38:  That's a very good question. Um, I've mentioned everything about the technical side of things, so I, I, I won't develop more on that. Um, yeah, I, I, I, I guess the only, the only great use case I could see is that on any of the, yeah, yeah, I, I, I, I guess it would be on assistance on each of the, uh, visual visualization tools that, that we have. Uh, obviously we have to deal with a massive amount of, of, um, of data points. Yes. I think certainly, yeah, thinking in a, in a, in an, yeah, in an ideal world, uh, for example, I, I've been working on a project with, you know, millions of mentions. Obviously, I will never be able to, to read every single one of them. I think ideally, uh, if we have some form of generative AI who specifically tailored to the dataset that we have created, so an a, an AI that, uh, would take in, uh, the whole technical setup and, you know, the processes, um, related to it, and then we could apply it to say, for example, hello ai, uh, please find all the mentions or tell me how consumers feel about this particular product, uh, or all within the context of the conversation.  
**Speaker 1** 00:28:58:  So yeah, I mean, I guess that would be my, my dream if I had, um, a, a wish from a genie in a bottle. Um, but yeah, perhaps a AI assistant, a contextualized AI assistant to, um, a specific research, uh, context would be, oh, sorry, uh, a specific research project, uh, would be good. Uh, but yeah, basically a, a, a gen AI that is built for the context of the project, um, that, that, that would be amazing, obviously, but I, I dunno how far we are from that.  
**Speaker 0** 00:29:27:  Right. So, uh, just just to rephrase, so if, if we understood correctly, you're basically looking for something like that can take commands in natural, like natural language processing Yes. And something like this, uh, small language model that are built just for you. And, uh, so you want to get like more trust that whatever information it's, uh, uh, driving from your small data set is particularly trained for you and, uh, the, uh, the quality, uh, or the authenticity of that information is, is much higher, is it?  
**Speaker 1** 00:30:03:  That's absolutely it. Yeah. That, that, that, that, that's correct. That's correct. And, um, yeah, I, I think you really hit the nail on the head with, um, trust and authent authenticity. I would only re Yeah, I, I would never use the word rely with AI at the moment. Um, but yeah, I would, me personally, and I'm sure the rest of my team, you know, my, my supervisors, my managers, um, you know, they would never to, they, they, they, they would never advise, um, any of, uh, any, yeah, any member of my team to, um, you know, rely, unless we were 100% sure that authenticity and, uh, you know, reliability in general, um, is something we can trust. Yeah.  
**Speaker 0** 00:30:44:  Any, any, do, do you have any suggestion? Just one last question. Any suggestion? How can we improve that trust? Uh, what, what is the way to measure the trust there? Because if, if we have to quantify the trust, not everyone is expert at the same level, AI is basically just a tool. Yes. How can we ensure that, that it's trustable?  
**Speaker 1** 00:31:03:  Yeah, absolutely. I, I, I think, um, one thing that ai, I, I wouldn't say it misses because, you know, for a general use case, you know, it, it, it, you know, for the, I don't know, whenever I'm not working and I wanna learn something, it's, it's great, but I think it kind of misses is the methodology. Um, and I think charge G PT is a good example of that. Charge G PT is not great at giving you a methodology of the reasoning behind the output that it's giving you. Um, and that's where, I guess the topic of hallucination and, uh, all the inconsistencies that come with ai, I think that's, um, you know, one of the biggest drawbacks, um, o obviously you can ask, uh, after you, you've received an output to, Hey, hey, chat gt, can you give me, um, your methodology, uh, how you, how, how, how, how have you analyzed this question I have asked you, and how have you given me this, uh, this answer basically?  
**Speaker 0** 00:31:58:  Very good.  
**Speaker 1** 00:31:58:  Um, I think a more clear and more intuitive and a more, um, human friendly way of understanding how the process behind the, the, as the AI assistant, um, yeah, how everything works would be very beneficial and would really nurture trust. I can't, I guess the best example in mind I have is, are you familiar with the, the \<inaudible\> of perplexity?  
**Speaker 0** 00:32:23:  Yeah, yeah.  
**Speaker 1** 00:32:24:  Yeah. So the, the way it gives you like, the different reference points and how after each point it, it, it gives you like a little number which points to the specific, um, website or specific source that it's, uh, retrieved that information or, you know, made the judgment from  
**Speaker 0** 00:32:40:  Yeah.  
**Speaker 1** 00:32:40:  I think that's, you know, why perplexity is my favorite, uh, unofficial AI tool. Yeah. Um, so yeah, I wonder if you know that, that that could be developed further to, um, yeah. Uh, nurture trust, as I mentioned.  
**Speaker 0** 00:32:53:  Right. Okay. Great. Yeah. So, so you want to see a little bit more references that, uh, yes. What's the reason behind the methodology  
**Speaker 1** 00:32:58:  Yes. Understand how, how the methodology basically, and, you know, how we have come to these, um, uh, judgements and suggestions.  
**Speaker 0** 00:33:06:  Yeah. Okay. Brilliant. Thank you very much, Liandra. Really enjoyed talking with you. Likewise. Thank you. You have any question for me?  
**Speaker 1** 00:33:14:  Uh, I guess, you know, I'm, I'm interested in understanding, uh, obviously 'cause you, you seem very, very, very knowledgeable, ai, um, I, I guess I'd like to you understand what you know, uh, what what beings is about, uh, a bit in, in a bit more detail. Sounds very interesting, uh, what you're working on. So yeah, I guess any insight you can give me, uh, would be very much valued.  
**Speaker 0** 00:33:31:  Uh, very, very good. Yeah. Uh, so yeah, my, my background is from, uh, data science, computational modeling and simulations. And, uh, and, and from company side, uh, we are developing AI products. Uh, initially we are targeting qualitative research to, to, to make it a bit more, uh, user friendly so that those tools, like you're saying that you're looking for an AI assistant, we are also developing something that could be your personalized, uh, research assistant. Oh, wow. Uh, that will help mainly, uh, from project planning. So we have like, uh, different number of steps. Uh, once you're, like, initially your project planning, you're thinking what an idea you are gathering information, um, then you are talking about, uh, how to continue this product, uh, what kind of participants you need, or what kind of data sets you need. We can call it like profiling and then next year going, uh, to recruit participant if it's pure qualitative research or find another way to find some other data sets.  
**Speaker 0** 00:34:29:  So, uh, initially we were targeting only qualitative research from product planning. Then, uh, recruiting participant, uh, uh, and then, uh, recording data. Uh, just like we are talking now on Google meets, uh, people use a lot of Zoom and, uh, Microsoft teams. So we have built our own video recording tool. Okay. Uh, so that's like, uh, the resolution is much higher than the other tools available in the market. Uh, and from those, uh, video recording tool, you can also do the live streaming to multiple platforms at the same time if you're launching some product. Uh, or you can also use it to like one-to-one discussion like we are having. You can also use it for focus group discussions. And then, uh, ADA here, uh, that's, uh, recording your meeting. Uh, it'll be participant there. It'll record, uh, all the transcript and they hand it over to you the, those transcript.  
**Speaker 0** 00:35:22:  And then you can talk to it by, through like natural language processing, where you can ask it question that, gimme the summaries, gimme the key insight, those kind of things. And then, uh, we are also planning to build some kind of dashboard. So the kind of information that you always want to see at the end of a qualitative product, or perhaps quantitative as well. Uh, we don't have the quantitative right now, but we are planning that, uh, because I, through this interview, I started to learn that qualitative is not like a hundred percent qualitative when you're talking in market research. There is always some sort of a Excel sheet missing or some other way, some kind of data. So we still need to figure out a way that, how to, uh, automate that part as well. Uh, and we are launching our first product, MVP will be launched in sometime in September.  
**Speaker 1** 00:36:13:  That's fascinating. That's really fascinating. Um, yeah. Uh, I guess I'm, uh, oh, sorry, did I cut you off?  
**Speaker 0** 00:36:20:  No, no, no. Please go ahead.  
**Speaker 1** 00:36:21:  Oh, yeah, no, no, that, that sounds really fascinating. And I'll, I'll, I'll be sure to follow, um, all the successes that, um, are gonna come your way. That sounds like a great idea, honestly. And yeah, I, I, absolutely, I think, um, you know, I I, I've worked in traditional market research, mainly quantitative, like I said, 80 or sometimes 90, sometimes even a hundred percent. Uh, numbers, numbers, numbers. Um, and now I'm working mostly in qualitative, and yeah, it's, um, yeah, you can never get one without the other. It's like two sides of the same coin. Um, so, uh, so yeah. Mm-Hmm, \<affirmative\>  
**Speaker 0** 00:36:53:  Very good. Yeah. I'm very happy to hear that, uh, the things that we are trying to build, we have strong overlap with what you, what your wishlist.  
**Speaker 1** 00:36:59:  Absolutely. Absolutely. Absolutely. I, I, I, I look forward to having that, um, AI assistant with me, \<laugh\> at some point in the future.  
**Speaker 0** 00:37:06:  Yeah, we will, we will keep you in the loop. Thank you very much, Alessandra and I will follow with the Amazon virtual, uh, through the email.  
**Speaker 1** 00:37:12:  Thank you so much, group. It's been a pleasure to meet you. And, um, yeah, let's keep in touch. Uh, yeah, it's been very nice to, to talk to you.  
**Speaker 0** 00:37:18:  Sure. Yeah. Uh, yeah. Very nice chatting with you. Have a nice rest of the day.  
**Speaker 1** 00:37:21:  You too. Thank you. Bye. Thank you my friend. Bye-Bye.

# Vishen Thumbadoo: Ipsos

Associate Director at Ipsos 

## Meeting summary:

**1\. Percentage Talktime:**

* **Speaker 0 (Interviewer):** 45%  
* **Speaker 1 (Interviewee):** 55%

**2\. Meeting Purpose:**

* **Speaker 0:** To understand the role of AI in market research from an expert's perspective at a large research company.  
* **Speaker 1:** To share insights on the use of AI in market research, particularly in quantitative analysis and report automation.

**3\. Key Themes:**

* **AI's Impact on Market Research:** How AI is transforming processes, improving efficiency, and changing job roles.  
* **AI's Limitations and Challenges:** The need for human expertise in interpreting qualitative data, concerns about biases, and the importance of data privacy.  
* **The Future of AI in Market Research:** Potential for AI to eliminate the need for programming languages, development of self-learning tools, and the need for more inclusive AI models.

**4\. Key Topics:**

* **Use of AI in Market Research:** Translation, report automation, data analysis, and brand comparison.  
* **Tools and Technologies:** In-house AI tools, Dimensions, Askia, Tableau, Power BI, Power Query, SQL databases, and cloud-based data.  
* **Data Collection Methods:** Online surveys, telephone surveys, and postal surveys.  
* **Challenges:** Data cleaning, standardization, and maintaining the human touch in research.  
* **Ethical Considerations:** Bias in AI tools and the need for inclusivity.

**5\. Key Actions:**

* **Speaker 0:** Will follow up with the interviewee about a voucher via email.

**6\. Key Sentiment:**

* **Speaker 1:** Cautiously optimistic about AI, recognizing its potential while acknowledging its limitations and the need for ethical considerations.

**7\. Quantitative Insights:**

* None explicitly mentioned.

**8\. Qualitative Insights:**

* AI is becoming increasingly prevalent in market research, particularly in quantitative analysis.  
* AI tools are not yet capable of fully replacing human expertise in qualitative research and data interpretation.  
* There is a need for more user-friendly and adaptable AI tools that can learn and improve over time.  
* Ethical considerations, such as bias and inclusivity, are crucial in the development and use of AI in market research.

**9\. List of All Questions with Detailed Responses:**

* **Speaker 0:** Would you like to tell me a little bit about yourself and about your job and your day-to-day responsibilities?  
  * **Speaker 1:** I work for IPSOS Mori, one of the largest research companies in the world. I mainly focus on the technical side of quantitative research, using AI for programming, translations, data analysis, and report automation.  
* **Speaker 0:** Is AI one of the strongest technologies you've seen in market research?  
  * **Speaker 1:** Yes, AI is very strong, but it still needs human oversight and understanding to be effective.  
* **Speaker 0:** Does your company use in-house tools or rely on outside market tools?  
  * **Speaker 1:** We use a lot of in-house tools, but we also leverage AI to help with programming and other tasks.  
* **Speaker 0:** Under your current job responsibilities, what's the fraction of quantitative work versus qualitative research?  
  * **Speaker 1:** I mostly work with quantitative research.  
* **Speaker 0:** Could you explain how you use AI tools for translation in quantitative research?  
  * **Speaker 1:** We use AI to translate reports into multiple languages for multi-market studies. This is done using a system that starts with English and then translates into other languages, ensuring conciseness and accuracy.  
* **Speaker 0:** What tools were you using for translation before the rise of AI?  
  * **Speaker 1:** We used a translation team to manually translate reports into different languages.  
* **Speaker 0:** What are your concerns about losing meaning in translation from one language to another?  
  * **Speaker 1:** We still need someone to check the translations because AI can't always capture the nuances and cultural context of different languages.  
* **Speaker 0:** Could you please give me some examples of the use cases of your quantitative analysis?  
  * **Speaker 1:** We analyze brands, compare them across markets, look at growth over time, and analyze significant differences. We've worked on projects for Google, YouTube, Shell, and others.  
* **Speaker 0:** How do you collect data for these analyses?  
  * **Speaker 1:** We collect data through online surveys, telephone surveys, and even postal surveys to reach different demographics.  
* **Speaker 0:** Where would you say your job is most focused – data analysis, interpretation, or presentation?  
  * **Speaker 1:** My job is mostly focused on the deliverable side, which involves data representation, using graphics, applying statistical testing, and automating reports.  
* **Speaker 0:** Could you explain more about the automation you use in your work?  
  * **Speaker 1:** We use internal software to take data and output it into PowerPoint, PDFs, or Power BI reports. This automation helps maintain accuracy and efficiency when dealing with large amounts of data.  
* **Speaker 0:** What do you think about the representation and interpretation of results using AI tools when sharing with stakeholders?  
  * **Speaker 1:** Automation tools give us what we ask for on a large scale. AI can try to analyze data, but it cannot replace the researcher's role in interpreting results and providing commentary.  
* **Speaker 0:** What common tools do you use for data analysis and visualization?  
  * **Speaker 1:** We use a lot of in-house tools, as well as Dimensions, Askia, Tableau, Power BI, and Power Query.  
* **Speaker 0:** How do you manage online meetings and interviews, and does AI help with that?  
  * **Speaker 1:** AI doesn't really help with personal management or networking. While video calls and virtual meetings are helpful, building relationships in the research business often involves attending events and socializing with people.  
* **Speaker 0:** Have you ever used AI for automatic transcriptions in online meetings?  
  * **Speaker 1:** Yes, we tried using Microsoft Teams' transcription feature, but it was difficult due to the nuances of human speech. We found it easier to have someone take notes or rely on the recording itself.  
* **Speaker 0:** Did you try exploring any other tools for automatic transcription?  
  * **Speaker 1:** No, we primarily use Microsoft Teams because it's the company's preferred tool.  
* **Speaker 0:** What would be your definition of personalized AI, and how much of it is acceptable?  
  * **Speaker 1:** Personalized AI should evolve and remember things about the user. It's helpful for tasks like shopping recommendations or controlling smart home devices. However, it needs to be more inclusive and work well for people with different accents and languages.  
* **Speaker 0:** What are your views on AI's role in market research, the challenges it can solve, and the ethical considerations?  
  * **Speaker 1:** AI can be helpful in certain projects, but not all, especially those involving private data. There are ethical considerations like data privacy and potential biases that need to be addressed.  
* **Speaker 0:** Do you collaborate with the qualitative research side in your company? If so, how?  
  * **Speaker 1:** We sometimes collaborate on tasks like data cleaning or statistical testing. However, qualitative research is often more specialized and requires a deep understanding of the context, making it less suitable for AI intervention.  
* **Speaker 0:** Can you give me an example of the kind of information you share with the qualitative team?  
  * **Speaker 1:** We might share cleaned data or results of statistical tests. However, qualitative analysis often requires a different approach and skillset.  
* **Speaker 0:** In an ideal scenario, what would you like AI to do in both qualitative and quantitative research?  
  * **Speaker 1:** Ideally, AI would eliminate the need for complex programming languages and understand plain English. It would also be helpful to have self-learning tools that can grow and adapt over time.

## Transcript

**Speaker 0** 00:00:01:  Hello? Hi, can you hear me well?  
**Speaker 1** 00:00:05:  Yes, I can. Thank you.  
**Speaker 0** 00:00:07:  Thank you very much for connecting. Thanks a lot for your time.  
**Speaker 1** 00:00:10:  No worries.  
**Speaker 0** 00:00:11:  Um, uh, uh, I'm \<inaudible\>. I'm a product manager in the research and insights at uh, beings.com. Mm-Hmm. \<affirmative\>, this is startup. We are trying to develop AI products for market research. Mm-Hmm.  
**Speaker 1** 00:00:25:  \<affirmative\>.  
**Speaker 0** 00:00:26:  And I would like to know more about, uh, your experience and expertise and, uh, I'll ask you some questions around the same line. Would you like to, uh, tell me a little bit about yourself and, uh, about your job and your day to day responsibilities?  
**Speaker 1** 00:00:44:  Okay. Well, I work for IPS Murray, so it's one of the largest research companies in the world. Um, work on, uh, a variety of jobs. Um, um, mainly focused on the technical side, so for the deliverable side. So we use a lot of AI for, for programming and for translations, um, on big projects like Coca-Cola, um, Google, YouTube, um, um, and it's generally, it's generally a day-to-day thing, I suppose. Um, um, I've been working for the same company for over 20 years, so \<laugh\>, so I've been in the business a long time. So, um, you know, it's, um, you know, so I've seen it change a lot over the years, especially, you know, with, um, bringing AI helps my job quite a lot.  
**Speaker 0** 00:01:37:  Yeah, very good. Yeah, thanks a lot. So, yeah, you have seen a lot of, uh, evolution of different kind of technology over the year and, uh, what, what are your opinion that, uh, is AI one of the, uh, strongest one so far or  
**Speaker 1** 00:01:54:  Strongest as well?  
**Speaker 0** 00:01:56:  Yeah, okay. And like you said that you're one of the largest company in the world, and in terms of tools that you're using, uh, are you developing at home or like, like it's in-house tools or you Yeah. Rely on most of the tools from out outside market as well?  
**Speaker 1** 00:02:15:  No, we, we use a lot of in-house tools, so, um, we have things we've developed ourself, but then also like to use AI to, with programming languages, you know, I don't think there's such a, um, there's not such a need to spend so much time writing lines and lads of code anymore. So, um, I wouldn't say that you need to get rid of programmers because you still need a programmer to understand it. So, um, and, you know, you can't always get a hundred percent what you need from the ai, you know, so, um, a lot of the time, if you are, if you are needing a script to be written, to perform a certain function with data, for example, yeah. Um, then you'd probably, um, you'd probably be able to tell chat GPT, which we have our own internal chat, GPT, which we, we have one which we grab, um, all the different kind of AI from the internet and have them all in one place. So, um, you know, you can, you can ask, ask the AI to write you a standardized code to perform a certain function. Then you can go and tweak it yourself afterwards to kind of make it more appropriate to your data file and your data set and maybe, um, and your deliverables. Um, yeah.  
**Speaker 0** 00:03:29:  Yeah. Thanks. Yeah, that's, that's true indeed. I mean, of course, uh, in the longer term, we are also not trying to replace anybody. It's basically just, uh, uh, empowering, uh, the expert in that field. Either it's the quantitative work, qualitative researcher. Uh,  
**Speaker 1** 00:03:45:  Well, I think, I think in some ways I think some work will get replaced. I think, uh, um, there's gonna be less need for, like I said, for somebody sitting there typing lines and lines of code, you're not gonna need that anymore. And it's the, it's the same way that, you know, every kind of business evolves and moves on. I think, you know, 30 years ago you'd rent a video and you'd sit there and watch it on a Saturday, whereas you don't need to do that anymore because now you've got Netflix. Yeah. So like, um, everything does move on eventually. And I think, um, um, especially with, um, with ai, it really does help with the technical side of things. But on the other hand, you always need somebody to understand it because it's, um, you can't, it's very hard just to take code from the internet and use it, and it won't, most of the time it won't work.  
**Speaker 0** 00:04:34:  So, yeah, that's true. Yeah, it's a lot of context there, uh, where it'll work or where it won't work. And, uh, so under, under your current job responsibilities, so could you tell me an overview of the fraction of, uh, quantitative work, qualitative research? Uh,  
**Speaker 1** 00:04:51:  Uh, generally only work with, um, quant, um, quantitative research.  
**Speaker 0** 00:04:56:  You work mostly with quantitative?  
**Speaker 1** 00:04:58:  Quantitative, yeah. So, uh, I don't really work much with, um, qual work.  
**Speaker 0** 00:05:04:  Okay. Right. Thanks. And, uh, you mentioned about like, uh, you're using in-house tools for transcription. So were you mentioning that, uh, uh, even for the quantitative purpose, whenever you discuss your project with somebody and some, how, how would you like to explain a little bit more about those?  
**Speaker 1** 00:05:22:  Well, how, how we use it for translation. So for example, we may be doing reports for, um, um, multi-market study, and that at that point you're gonna need reports, um, in every market in their language. Something that as, as well fits the template that we've decided on. You know, and it's impossible. Say if you've got maybe 20 markets and maybe like 400 reports in each market, it's impossible to do that by eye. So you need to have a system that can, um, maybe start off with English and then go and translate these all into the different, um, countries, you know, depending on what the report is, you know? So, um, um, you know, we'd use AI to do that, you know, to do it in something that's concise, you know, you know, for example, you may have a sentence in English, which is much bigger in Danish, for example. Yeah. But you need to know how to make that a bit more concise in that, in it can't be a direct translation all the time.  
**Speaker 0** 00:06:24:  Right. Yeah, that's, that's very interesting. So, uh, I'm just trying to understand the trend here. So AI is like very recent thing. So what were, what kind of tool were you using before that, before the rise of AI for translation?  
**Speaker 1** 00:06:39:  We use a translations team.  
**Speaker 0** 00:06:42:  I just,  
**Speaker 1** 00:06:43:  Yeah. We use, we'd use a translations team to, you know, 'cause we're a global company, so we'd have translations, you know, we'd have all of our, um, imagine if you're doing a report, you know, all the points around the report would be in text. You can put them into a document, and then you'd have another column with all the translations into Arabic, into Hindi Yeah. Chinese into, you know, and you can have everything like that. And we'd have somebody do that for us. Yeah. You know, and then, then we'd use that, obviously we'd still use that with programming and macros to Yeah. Um, to pull the text in where, and change the column where needed. But, um, now you don't really need to do that so much. On the other hand, you still need someone to check it, you know, because, you know, if you, if you always just press the computer a hundred percent, sometimes it would go wrong and you could, you could end up with problems.  
**Speaker 0** 00:07:35:  Yep. And, uh, what are your concern about losing meaning in translation from one language to another? Are you a bit concerned about that, or how do you make sure that that is still covered?  
**Speaker 1** 00:07:49:  Well, that's, that's why sometimes it still needs somebody to, to look at this. You know, it's like you can never really, um, a hundred percent trust it until you have somebody who understands it. So, like, um, if we were trans and translating English into Chinese, for example, you may need somebody who's Chinese to read it and say whether that makes sense, you know, because there are cultural differences the way that we re we, the way that we reference things. And, um, sometimes LA language is a lot based around, um, colloquial kind of things that we, um, that we use and, um, our own kind of, um, it's just, uh, our own kind, you know, it's, it's quite, um, is quite localized sometimes the way that we, um, respond to things. Yeah. So, um, sometimes in China, something, something may be a quite a rude way of speaking, you know, so like, as in, you know, you'd need someone to kind of see, just to make sure that it's appropriate to be publicized.  
**Speaker 0** 00:08:45:  Right. Yeah, I totally agree. And, uh, could you please tell me a bit more about, uh, the use cases of your, uh, some examples of your quantitative analysis that, uh, what was the purpose of those analysis?  
**Speaker 1** 00:09:01:  What type of jobs?  
**Speaker 0** 00:09:02:  Uh, like what, what, what kind of analysis were those? Like were you just running some surveys or analyzing Excel sheets, or if you can explain a little bit more? Yeah. Well,  
**Speaker 1** 00:09:10:  It depends. There's lots of different ones. There's like, you know, analyzing, um, you know, for like Google and YouTube, you're analyzing brands and comparing brands over different markets, you know, for, like, for petrol stations, like Shell, you'll be, um, analyzing all of the different petrol stations against each other in different markets and the growth over time, you know, so, um, and looking at significant difference on all of these projects. Um, a a lot of the time it's branding, A lot of the time it's, you know, brand comparison and analysis. And sometimes it's, it could be, you know, for pet food for example. Yeah. And like in, um, pet food, what the pets like, rather than like what the, the human wants the pet to eat, you know, and it's like, as in, um, you know, we do a lot of, all types of analysis we're doing in, you know, use, you know, with all of the different projects.  
**Speaker 0** 00:10:01:  And, and, and how do you collect that data?  
**Speaker 1** 00:10:05:  Data's all collected by, uh, online surveys. Some are telephone surveys, some are even postal.  
**Speaker 0** 00:10:13:  And is that a different team that decide that? What do they want to do and how do they want to collect the data? Or do you, you still have like, uh, you still have to collaborate with them? Well,  
**Speaker 1** 00:10:21:  It's, it's part of a multi mode research. So, you know, you, you have all, you know, you know, to gain research, you need to gain information from all different demographics. So there are only a certain amount of people who would do online research and some only certain amount who will do postal and some who do telephone. Like, it's gonna be very difficult sometimes to gain the, um, you know, the 65 plus category from the online. So like, 'cause in, you have to have another way of doing it. A lot of time. Telephone's the best way to do that. Mm-Hmm. \<affirmative\>. Um, and for younger people it's a lot easier to do it online 'cause they're not so always interested in, I think nowadays a lot of people are worried about taking a phone call when they don't know where it's from. So, um, for the younger people, sometimes it's easier online. Um, um, and again, with postal, I'm not sure how many people really use that anymore, but it's still used amongst the more elderly of, um, the demographic.  
**Speaker 0** 00:11:16:  Mm-Hmm. \<affirmative\>. Yeah. Very interesting. And, uh, where, where would you say your job most realize, is it at the data analysis part, or data interpretation or presentation?  
**Speaker 1** 00:11:28:  It's more around the deliverable side. So like, as in once we've, um, once we've decided once, once the jobs have been commissioned, and, um, yeah, you know, um, once the data's all been collected, you know, we, we decide about how we're gonna represent the data and how we're gonna use graphics to work with the data and make it visually appealing. And, um, and also maybe to apply statistical testing and, um, and automation of reports as well, because you can't do them by hand. You know, you can obviously do one by hand, but you can't do a thousand by hand. So automation of deliverables, there's a lot of what I'm focused in.  
**Speaker 0** 00:12:07:  Yeah. Thanks a lot. Yeah. Could you please explain a little bit more about automation that, uh, what kind of automation is that? Are you using AI there or, uh, do you still do something?  
**Speaker 1** 00:12:17:  Kinda, we use an API for like a, we've got an internal, um, software that we've, um, built to take the data and output the data to pump into PowerPoint and make PDFs, or sometimes even to make like Power BI reports. Um, we've got other online tools that we use, but which are internal. But, um, mainly just to put everything, um, um, to tag tag a, a predefined template. Yeah. So say if you say, if you had a template for Coca-Cola, for example, with 10 slides, then you, um, you know, you tell the, you tell the software where all the data needs to go. Then for each of the reports, we'd switch, like the column or some variables will switch the, you know, the colors may switch or the, the market will switch. The language could switch, um, different kind of confidence interval with the seek testing. You know, there's a whole load of things. Everything can be variable with automation. But you see, the reason we do it for automation is because you, you can't do something like that manually and guarantee accuracy. Yeah.  
**Speaker 0** 00:13:22:  And does doesn't mean that you have to be very strictly following the rules for keeping the, uh, dataset information in the similar data formats so that you, it's always like, possible to those tools and you can get something easily to reduce the work. Yeah, yeah.  
**Speaker 1** 00:13:38:  We work, we work with people to make sure that we've got a data file that will work with the software. I think the ideal solution is to have everything in a database where we can all take data from in the format that we like. Yeah. I think that's the, that is what we're working towards at the moment. So having everything in a database, I think if you're serious about data, you know, you, you wanna be working with a, a database, you know, a SQL database, um, then everybody can take, take what they want in the format they need for, because it's never gonna be the case where every software takes data in the same format. It will never happen. Of course. You know, so, um, on the other hand, I think more, you know, you know, um, more, more data files are, you know, using like, as in Jason data files and um, um, um, and cloud based and cloud-based data as well. So, um, you know, I think more software is actually becoming a bit more, um, flexible with that.  
**Speaker 0** 00:14:38:  Yeah. Yeah. Very good. Thanks. Yeah, I'm, I'm very interested to learn all these things because my background is also from data science simulations. Yeah. So, yeah. Uh, automation and then, uh, what, what, what do you think about, uh, representation or like interpretation of those results, uh, when you are sharing it with your o other stakeholders? Like people who are at the management level, like you Mm-Hmm. \<affirmative\> Or if you have to share it with, uh, with more technical audience. Uh, are these tools giving you different kind of, uh, outputs or different interpretation of same dataset?  
**Speaker 1** 00:15:21:  Uh, we not, it's hard questions to answer 'cause it's, um, you see, we, we'd say what with using AI to interpret this, you mean?  
**Speaker 0** 00:15:30:  Yeah, yeah. I, I mean also AI are also like automation of tools.  
**Speaker 1** 00:15:35:  Well, no, automation is only gonna give you what you ask it to give you. It's just, um, you know, automation is, the information you put in is what you get out on a mass scale. Whereas the ai, AI is slightly different where AI is going to maybe try and analyze it for you, which in some cases, yes, that works. Like, for example, we have our own in-house online tools, which we, we can put the data in and say, for example, if you had a, um, survey on football stadiums, for example, then it could tell you the type of season ticket holder that you have. The kind of where the area they would be, the demographic they would be from the, the, the kind of wage, they would be the gender. They would be just maybe for certain areas and like, um, um, you could find out a lot through, um, discover functions.  
**Speaker 1** 00:16:21:  But on the other hand, like as in taking reports and writing commentary on something from what you've needed, that is really what the researchers are there for. You know, so that is the, that's the essential part of a researcher's role. So, um, if you took that, you know, it's really why it's, it's very hard for a AI can identify a few things, but AI can't be the, um, the end part. It cannot be the end part because somebody needs to look, say AI might, may, I may identify something but may not understand that it's not with the current, um, climate the way people want to think about or what people, what's what's in people's mind at the moment. So, um, I think AI is more of a recommendation rather than the solution.  
**Speaker 0** 00:17:09:  Yeah. Yeah. Yes, indeed. And, uh, in terms of, uh, uh, common tools that you use for data analysis and visualization, could, could you please name some of them or,  
**Speaker 1** 00:17:24:  Um, analysis of data? Um, we've got our own in, we have in-House tools, to be honest, we have a lot of in-house tools that we use. Um, you know, for data, you know, for data processing. We still use Dimensions and askia, you know, but, um, um, but then, um, for, for data analysis, online tools like Tableau, you know, we still use Tableau and Power bi. Yeah. You know, we do a lot of data transformation using, um, um, power Query sometimes even, you know. Yeah. The Microsoft Power package is brilliant, you know, so it's, um, we do a lot using that or depends, depends what the need is. You know, we've got people with the right path code if needed or, and we have our own internal software as well for doing certain things, which is not available outside.  
**Speaker 0** 00:18:15:  Uh, okay. Alright. And so, so that was like mostly about, uh, the quantitative side, but since you're also at the management level, so I believe that you are also like talking to various stakeholders. Mm-Hmm. \<affirmative\> sometimes purely like just interviews or talking to them and not sharing enough quantitative information, but we can call it qualitative. Right. Your online meetings or face-to-face meetings or strategy building or, Mm-Hmm. \<affirmative\>, how, how do you manage those kind of interviews and do you think AI is helping you there for your personal management, uh, as well?  
**Speaker 1** 00:18:50:  I don't think so. I think that's, uh, I think sometimes you've got it or you don't. And, um, especially in the technical environment, um, yeah, it's not really a so apparent sometimes the personal skills. So I think sometimes being around the business, I think it's, it's good to meet many people in the business, especially the research business. Like for example, I go to research events, I make sure I make contacts. I, I part of the research club, you go there and you socialize with people and meet people. So you're in the business and it really, sometimes it helps, you know, when you know other people from other companies, you know, especially if say like a project maybe have moved from Kantar to ours, you know, um, sometimes they've already heard of you, you know? Um, I don't think AI helps with that side of anything really. I think, um, you know, yeah. Yeah. We've got like, as in virtual meetings like this, so we've got something, we've got something that's helped. But, um, um, um, is that ai, is that just normal technology? You know, there's video call. Video calls have actually been around for many, many years. We just use it more now.  
**Speaker 0** 00:19:59:  No. Yeah. Well, yeah, I totally agree. Yeah, like video calls, uh, has been around for a long time. I mean, basically Covid I think pushed everybody into that direction. Otherwise, on my previous job I started using hybrid meeting mode from 20 20 10\. Yeah. And, and here, I, I understand the videos are already on, but, uh, have you ever felt the need that once you are having online meetings and there is some kind of tool that can provide you automatic transcriptions or something?  
**Speaker 1** 00:20:32:  Tried it before? I've tried it before. I've tried it before. Like, um, certain times when, um, we'd have meetings instead of taking notes, we tried to get, um, teams to do the transcript. And it was quite difficult because it's like, um, it takes everything it takes. Mm. Ah, well, um, and the way we speak our, our a certain way of talking is we don't speak like robots. So, um, we found it generally easier for somebody just to sit and put, make bullet point kind of comments, you know,  
**Speaker 0** 00:21:02:  Just with hands.  
**Speaker 1** 00:21:03:  Well, yeah. Or typing it up or anything. Just, just for someone to bullet point everything, because you don't really need to add in every single part of your language, you know, especially like, um, you know, if you talk about like, um, um, there are some languages say like if you were to speak like German, German is quite to the point. They only say enough of what you need to know of the sentence. English probably explains it more. Arabic is very expressive language. So like, as in there are so many, a lot more words in Arabic, you know, than there are in English. So like, as in, um, in, but to know what the task is or what we need to remember, you only need to know a little bit of that so that the transcript, um, it's not bad. I wouldn't say it's bad because you could have someone go through and edit it afterwards.  
**Speaker 1** 00:21:50:  But on the other hand, if you need that, you could have someone just take notes, you know? Yeah. Because you've got, you've got the recording. I think sometimes the, the recording is there anyway, which is good. The AI to record the actual, actual thing and make a video. 'cause sometimes the best way to go through it is to listen to what people said, the way they've said things, and whether, um, someone may have said something but not seem confident about it. And you can only really tell that by looking at them, you know, rather than you might think, oh, well that was important because of the way that they responded. Yeah. You know, rather than, um, it being, um, um, how do you say, like, um, if it takes down everything, someone may have said something as a kind of joke. They may have made a kind of joke, but AI is not gonna know whether it's a joke or not.  
**Speaker 0** 00:22:36:  Yeah,  
**Speaker 1** 00:22:37:  Yeah. If you know what I mean. Yeah,  
**Speaker 0** 00:22:39:  No, yeah. I totally agree. And how can, can I ask, how long ago have you used that?  
**Speaker 1** 00:22:46:  Um, mainly since Covid, I think maybe before Covid, actually. Before Covid, yeah. Probably started using that 2018 maybe? Well, maybe when it started, uh, perhaps 2018, maybe.  
**Speaker 0** 00:23:00:  Well, the Covid started like in February, March, 2020\.  
**Speaker 1** 00:23:04:  Yeah. But like we start, we were using this before then anyway. Yeah. Okay. Because, um, you know, it's quite difficult to get everybody in a meeting room, so Sure. You, you've always got this, you know, we, we've got people from all over the globe. We work on international projects, so it's very rare you're gonna get everybody in a room.  
**Speaker 0** 00:23:21:  And, and, and, and after that experience you did, did you try to explore any other tool that could have helped you with this kind of stuff? Or, or you just did give up on that idea that  
**Speaker 1** 00:23:32:  No, we didn't, we didn't look at anything else. I think because the company usually invest in one thing. So we use, uh, Microsoft team. Yeah, we use Microsoft Teams to do it. So because the company uses that and everything runs through that, Oracle meetings are scheduled that way. Um, a lot of information is kept there. Um, we tend to use that because that's the company's tool. Um, we haven't really looked into using anything else, but, um, you know, also there's the danger if you, you know, to lose the, the, the personal kind of touch to it as well. I think using AI is good to a certain extent, but then, you know, you can't let it do everything for you. It makes you lazy after a while, you know?  
**Speaker 0** 00:24:14:  Yeah. That's, uh, that, that's true. And what, what would be your definition of personalized ai? How, how much is acceptable?  
**Speaker 1** 00:24:25:  Well, no, I think it's always gotta grow. I think like as in personalized ai, like, um, you mean like as in remembering things about you?  
**Speaker 0** 00:24:34:  Okay.  
**Speaker 1** 00:24:35:  Yeah. So like, um, so you have it with the supermarket now, you can go to the supermarket and it remembers all the things you like, and it gives you offers depending on what you buy, which we've always wanted there, you know, because we really don't wanna walk through every single aisle to buy everything, you know? So it's, um, if you shop online now, there is that, there's also, you know, your cookies you have online, you know, and you know, that remembers what you do. And it will say, it will send you a picture of, do you wanna buy this pair of Levi's jeans because you've been looking at it, it's gonna tell you. And yes, it's targeted advertisement, you know, and that's a part of ai. And I think it's good because sometimes you would've never found it. You haven't got time to spend searching for these things all the time.  
**Speaker 1** 00:25:19:  You know, you have your, like, um, your echo, um, sorry, it's gonna respond in a minute if I say its name too much, but then, you know, like the, the, the thing in the house, which talks \<laugh\>. Okay, \<laugh\>, yeah. I don't wanna say it because it'll answer me now, but like, um, you have those that, that helps since, since I've had that, I haven't touched a single light switch, you know? Yeah. And, you know, and also if I, if I walk, I walk in and I want it to play music, it plays music, you know? Right. Want to hear the radio. It does that. It wants to turn the TV on and off. It does it, it puts the volume up, up and down, you know, it does so many things. I think it's, you know, it's, people might say it's making us lazy, but then it's just a simple task, why not have it that way?  
**Speaker 1** 00:25:59:  But on the other hand, it works very well with English voice, you know? Yeah. You know, when if, if you speak, um, sometimes in a foreign accent, it won't understand. So is it really that good? You know, is it really that good if it doesn't understand a foreign accent speaking English very well? It doesn't, it doesn't, it's not very good with that. If you ask your one, your, um, personal AI in the house, if you ask it to play a foreign song, say the name of the song, most of the time it won't understand it. So, you know, is it only targeted to a certain demo demographic? You know, you know, that's, that's the, that's sometimes the, the thing I think about who's it targeted to, where, um, the col the, you know, the voice recognition is based on data that they've collected. Yeah. You know, so, and I think it's been shown that, you know, I think, um, most of the data collectively stuff is by white European men. Yeah. So like, as in, um, maybe they need to expand that to make it open to everybody else, and you'll, you'll get more buy-in if it, if it works for everybody else, you know? Yeah.  
**Speaker 0** 00:27:08:  Oh yeah. That's totally true. Yeah. I'm, I'm also not native English speaker, so I totally understand what you're saying.  
**Speaker 1** 00:27:13:  Yeah. It's, you know, it's, it's definitely, it needs to be more inclusive, and I think in a, in a time of, in, you know, diversity, I think, you know, that's one thing which definitely needs to up its game.  
**Speaker 0** 00:27:25:  Sure. Sure. And, and, and overall in the field of market research, uh, what, what do you, what, what are your views about, uh, the relationship of AI and then, uh, future challenges that it can quickly solve, and then also the ethical considerations that are coming with it. What, how, how do you see it, the, the bigger overview?  
**Speaker 1** 00:27:45:  Well, it depends, you know, like we have some projects where we can't use ai, you know, some projects where you cannot put the data through chat gt, they just won't have it, you know? So, you know, there's a lot of projects with private data, you know, we have to deal with those things ourself. Um, whereas on the other hand, we work with a lot of public studies where the data is, is available publicly. So, um, with those, we can use it. Um, some people are scared of it, um, because it's new, but then, you know, I'm sure that people were scared of an ATM at one point in their life, you know, and they were so used to going to somebody to ask for your money, you know? So, um, I think that's just a matter of time for, um, I think times will change, you know, um, people change and like, obviously the new generation comes in, the generation now who are all about 20 years old, when they're about 50, it'll become commonplace. Yeah.  
**Speaker 0** 00:28:40:  And, uh, do you collaborate often with the, the qualitative research side in your company?  
**Speaker 1** 00:28:46:  Sometimes. Sometimes they, they want us, some, you know, sometimes, because I think they, they like to think about ways to automate their processes, which we can to a certain extent with maybe it's a translations or some statistical testing for them. Um, they don't really deal with mass reporting or like, big data sets. It's not always so big. It's so usually drilled down to it's so specialized. Yeah. So it's, um, it's a lot of the time it's, um, um, the feedback's not really given in so much of a data dump where coal work is a lot of time based with a data dump. Mm-Hmm. Um, I mean, no qu quant work is based in a data dump. Coal work is not. So the, the cowork a lot of the time will, um, they have a kind of a picture of what they want and end up working with us to a certain extent. But with, um, a graphics team, we have the graphics team work with us to kind of produce what we're making. You know, it's, it's, it's a lot more targeted  
**Speaker 0** 00:29:51:  And yeah. Thanks. And can you gimme some example that, what kind of information is, uh, most common to share with the qual team from your side? Uh, what kind of help do you usually want or not just help? Do they even collaborate together on some process projects?  
**Speaker 1** 00:30:08:  It's very hard. I think, you know, to be honest, a lot of the time maybe just data cleaning or like, as in statistical testing, you know, with what they're doing with Qual, you know, the ANA analysis, you know, is, is very, is, you know, it's very targeted and it's very kind of, um, it's separate. It's personal to the job. Sometimes, you know, you, you really need a quote researcher working on it to understand what to take from the results.  
**Speaker 0** 00:30:34:  Yeah.  
**Speaker 1** 00:30:35:  You know, it's, um, I, I find, I find AI is very hard to use with that.  
**Speaker 0** 00:30:41:  So, sorry, with that means in Qual side or quant side,  
**Speaker 1** 00:30:44:  Qual side, I find it very hard to use with Qual side. I think it's, it's not a standard, I think quant projects, there's a lot of standardization that can be done, you know? Yeah. You know, so like, um, the more, the more things you can standardize, the more you can put on a conveyor belt, you know, and it's, um, qu quant has, is, you know, we, we can do elements of the project, but it's not really been something that's really been introduced to the technical side of the company. It's usually always stays with purely researchers. Yeah. You know.  
**Speaker 0** 00:31:17:  And what if you think there is no, let, let's say an ideal scenario, what if there is no, uh, bond on technical advancement side, what would be your wishlist? What kind of things you would like AI to do both in qual or quant sites?  
**Speaker 1** 00:31:32:  Uh, well, you want AI eventually to get rid of programming language. You want it to be plain English, the right type English, or type your language and tell you what to do. You know, I think programming language is the, it's the same way that they've got the, the legal system is written in legalese, so it's difficult to understand and only the lawyer can understand. So, you know, if they make things plain, simple English to do it, which I think, you know, the closest thing to that is probably Excel. You know, Excel Power Query is probably the closest thing to it really being so simple, you know, I'd like to see things like that. You know, I think the, um, um, you know, I think there's this, um, may maybe even like, um, it's hard to say. I think like, you know, we've, we've a lot say we use, we work with a lot of online tools and online tools come and go so quickly because it changes, you know?  
**Speaker 1** 00:32:26:  Mm-Hmm. You, you want say like a online tool like that can grow rather than stay the same. And it doesn't need to really be so much developed so much, it's just, it grows by what you put into it and give it advice, you know, like a learning machine, you know. Yeah. You know, but then, um, you know that that's what you want. You wanna see like, as in it's with anything, you know, data is only as powerful as what you put into it. So a learning machine would be something which would, would be, would be good if there were no bound boundaries.  
**Speaker 0** 00:32:58:  Yep. Very good. Yeah. Thank you very much. Uh, I really enjoyed talking to you.  
**Speaker 1** 00:33:05:  Yeah, fine. Same.  
**Speaker 0** 00:33:07:  Do you have any question for me?  
**Speaker 1** 00:33:09:  No, no, no, no, no. I think that's, um, that was a good conversation. I think that's, um, it's generally, you know, this is the kind of conversation I have daily, so it was quite easy for me \<laugh\>, so, um, \<laugh\>. So, um, yeah, it's, um, yeah, it's nice, nice to talk about this kind of stuff.  
**Speaker 0** 00:33:25:  Yeah. Thank, thank you very much. I will follow up about the voucher, uh, through the email in. Okay. Thanks a lot. Have a nice weekend. Thanks  
**Speaker 1** 00:33:33:  To you. Take care then. Bye-Bye. You  
**Speaker 0** 00:33:35:  Too. Bye.

# Tak Ha: Ipsos 

Research Director in the Media Development team at Ipsos UK, working with some of the biggest consumer tech and media brands.

## Meeting summary:

Meeting Summary: AI in Market Research

**Talk Time:**

* Speaker 0 (Tak Ha): 80%  
* Speaker 1 (Interviewer): 20%

**Meeting Purpose:**

* Understand the current use of AI in market research and explore future opportunities and challenges, particularly from the perspective of a research director at Ipsos.

**Key Themes:**

* Evolution of market research in the technology sector  
* Implementation and challenges of AI in research  
* Balancing human expertise with AI capabilities  
* Ethical considerations and data privacy concerns

**Key Topics:**

* Use cases of AI in research (assistive technologies, analysis, automation)  
* Limitations and biases of AI in qualitative research  
* Ideal features of an AI-powered research platform  
* Data privacy and security when using AI  
* Potential of synthetic respondents in research

**Key Actions:**

* Ipsos is actively exploring AI integration in various ways.  
* They have a proprietary AI interface for internal use.  
* They are focusing on retraining AI models for specific sectors.  
* They are exploring ethical considerations and data privacy challenges.  
* The interviewer will share information about Beings' upcoming qualitative research platform.

**Key Sentiment:**

* Tak Ha is optimistic about AI's potential but emphasizes the need for human oversight and critical thinking.  
* He expresses concerns about AI's limitations in generating nuanced insights and its potential to replace human jobs.  
* He also emphasizes the importance of broader system transformation for successful AI integration.

**Quantitative Insights:**

* None explicitly mentioned in the conversation.

**Qualitative Insights:**

* AI is being used to automate labor-intensive tasks, speed up analysis, and translate/summarize data.  
* Researchers are still hesitant to fully rely on AI for generating insights or final deliverables.  
* There are ethical concerns about transparency and the potential for AI to replace human jobs.  
* The success of AI integration depends on a holistic approach that considers system transformation and data privacy.

**Questions with Responses:**

* **Q:** What are the most significant shifts you've observed in market research?  
  * **A:** The shift to digital research and the move from exploration to monetization.  
* **Q:** How is AI being integrated into your research projects?  
  * **A:** Through a proprietary AI interface for various tasks like summarization, translation, and analysis.  
* **Q:** What are the challenges of using AI in research?  
  * **A:** Accuracy concerns, potential for bias, and the need for human expertise for nuanced insights.  
* **Q:** What would be the ideal features of an AI-powered research tool?  
  * **A:** Ability to synthesize tasks, adapt to different project stages, and integrate seamlessly with existing systems.  
* **Q:** How are you addressing data privacy concerns when using AI?  
  * **A:** By retraining models on client data securely and adhering to strict data handling protocols.  
* **Q:** What emerging technologies are you most excited about?  
  * **A:** Conversational AI and the potential for synthetic respondents.

## Transcript

**Speaker 0** 00:00:01:  Uh, whatever works better for you.  
**Speaker 1** 00:00:03:  Okay, great. Uh, thanks a lot. Uh, so yeah. Uh, my name is Grith. I'm a, a product manager in research and insights at, uh, this company, a startup company called Beings. Uh, we are developing AI products for, uh, market research. And, uh, yeah, currently we're exploring, uh, how those people who have been already working in market research, uh, what is their opinion about, uh, different kind of pain points and inefficiencies that could be solved, uh, with ai. So, uh, yeah. Can we start a little bit with you, uh, describing your job, what kinda work you do? What are your day-to-Day responsibilities? And then I can start asking more, uh, tailored questions.  
**Speaker 0** 00:00:47:  Sure. So I am a, uh, research director, um, in a, uh, uh, Ipsos in the uk. I work specifically in the media research team. So my clients are, uh, media based, uh, and platform, uh, companies, uh, but also, uh, content makers, um, uh, and, uh, sort of trade bodies regard, you know, related to the, the media industry. Um, so, um, covering broadcasters, covering entertainment brands, covering, uh, social media platforms, um, uh, and other sort of internet, uh, companies, um, uh, who are funded by advertising, let's say. So, you know, uh, a number of those, uh, uh, social media and other types of, um, uh, content platforms.  
**Speaker 1** 00:01:27:  Yeah. Great. Thanks.  
**Speaker 0** 00:01:29:  Um, yeah, and so my day-to-Day responsibilities are for, uh, overall account management, um, uh, people and financial responsibility with that. Uh, and also obviously, uh, uh, clients, client management and, uh, and, uh, delivery, uh, uh, and general sort of revenue responsibilities, uh, associated with that.  
**Speaker 1** 00:01:49:  Okay, brilliant. Uh, so you mentioned that you have experience with like a very different kind of sectors, uh, but you're doing market research for all of them. So what, what, what kept you passionate about, uh, uh, market research and staying in that field over the years?  
**Speaker 0** 00:02:04:  Um, I've always had a, an interest in technology, uh, primarily and, uh, I think, uh, with technology and it's, uh, evolutions, I, my career has covered a lot of hardware, but, uh, uh, in, in the past, uh, well over a decade now, it feels like, uh, technology is pretty, pretty much centered in the software and, uh, services space. And, uh, that's where, uh, a lot of the, the media giants of now are very much, uh, from a technology roots. Uh, if you look at the Googles and the Facebooks and, uh, and, uh, you know, meta now, you know, the, the, the, the general kind of, uh, profile of these companies is very much, uh, technology, but media led because they're, they're serving up content, um, serving up, uh, things that people spend their time with. Um, so yeah, that's why I'm, I'm still in the space, and it is an evolving, uh, uh, space.  
**Speaker 0** 00:02:50:  You know, other sectors are relatively slow, uh, compared to technology. So insights, uh, that we, that we, we capture and that we analyze and we provide, uh, we'll refresh on a regular basis. Uh, the, the, the landscape does not stay as static as, uh, other sectors where, you know, uh, insight might, might last for two or three years because it is, uh, uh, a relatively stable and, um, set industry. Uh, whereas ours is, uh, always changing, always prone to disruption, new challenges, uh, regulation, you name it. There's always factors that are going to make these companies, uh, uh, review what they're doing and, and, and be onto the, the front foot for the next thing. So, um, unsurprisingly, AI is, is, is quite hot topic, uh, off the back of that because it is very much, uh, front and center for a lot of companies.  
**Speaker 1** 00:03:36:  Yeah. Great. So, so, yeah. Uh, now it's ci, but over the years in your career, uh, would you like to tell that, what were the most significant shifts that you observed in the market research methodology that, and, uh, and client needs over the years?  
**Speaker 0** 00:03:53:  Well, I think probably there's a, well, there's been a number of shifts. It's kind of hard to summarize them entirely, but certainly from a, uh, uh, conducting a research when everything became digital, um, there was obviously a, a huge, uh, impact, uh, when, uh, when we went from having to do telephone or face-to-face work, to doing everything online, and then the move from online, uh, from a, uh, desktop kind of, uh, computer, laptop computer to mobile computer meant that, you know, the profile of of whom we need to speak to has always changed, and the methods and, and the questions and the, the re the survey experiences that we build for, uh, have to adapt to that as well. So they've been fairly significant. Uh, and a lot of the, and again, related back to the, that core client base, a lot of the companies went from being, uh, I guess initially, uh, exploratory and then, uh, into more, uh, demonstrative, uh, insight needs.  
**Speaker 0** 00:04:45:  You know, before they were just kind of exploring where their technology could go, and then they, they figured out that it was very much, you know, centered around their respec, you know, the, the, the respective sort of, um, uh, whether it be information or news or entertainment or connections and people or, uh, sharing, et cetera. They found their calling and then those companies went deep into research, into those, in, into those respective areas. Uh, and a lot of the work became monetized. It's about how we monetize audiences rather than just grow audiences. And so, yeah, there's been a kinda subtle shift, and that ma that's been very much a mature cycle, and now we have a new exploratory phase around these new technologies and the new hardware and software experiences, uh, very much an integration of the two and how they're going to kind factor in.  
**Speaker 0** 00:05:27:  So are we going to be, you know, using AR headsets or are we going to be, uh, uh, speaking AI and, and so on? Uh, you know, speaking to ai, there's, there's a whole lot of variety of, of work. So it, it seems to be going back through a cycle of exploration. Uh, that's my observation. So as I, I mentioned before, very, very, uh, agile and, uh, evolving space. Not many things stay the same for very long in comparison to, uh, other sectors that I've, uh, that, that, that I observe and, and have worked in, in the past, um, outside of technologies. Mm-hmm.  
**Speaker 1** 00:05:58:  \<affirmative\>, right? Yeah. Brilliant. Thank you very much for a great overview. And, uh, so in terms of AI tools, how was your experience, uh, in this integration of AI tools or platforms into your research project? And, uh, could you share some specific examples that, uh, how AI was applied and what impact it had on those products?  
**Speaker 0** 00:06:18:  Uh, we have a, a reasonably good, um, an active innovation, uh, uh, mindset within Ipsos or where I'm, and so we've been very, very much trying to get, uh, AI into, uh, into the hands of our researchers, generally speaking. So, uh, we have our own proprietary, uh, AI interface, excuse me. Uh, um, yeah, so we have our own proprietary interface, um, and that covers off, uh, um, you know, the general assistive technologies that, uh, use, you know, a lot of people have experienced, uh, with like the PET GPT and the Gemini. Uh, oh, excuse me.  
**Speaker 1** 00:07:00:  Please don't worry. Take, take a break if you want water.  
**Speaker 0** 00:07:03:  Uh, yeah, I'll, uh, I'll just, um, sorry. Yeah, so we have, uh, a lot of assistive use, so helping to summarize, uh, helping to, to, um, uh, do notation, uh, to translate, um, to, uh, make day-to-day work office work, I guess, easier. And then there are specific, uh, industry specific, uh, uses as well with regards to, you know, analysis, um, uh, and other types of, uh, I guess research, research specific, uh, usage, whether it be transcription and then, uh, summarizing of, of insights, um, uh, desk research and a number of other sort of, uh, uh, previously very manual labor intensive jobs, which can be sped up and, and made a bit more efficient. We are definitely, uh, doing a lot in this space and looking to ways it can, uh, be used across different sort of, um, teams. So we have, you know, teams who work in coding and, and programming, and they're finding, uh, a lot of benefit from the, the amounts of, um, uh, coding it can generate when we're trying to build, uh, and develop our products, uh, uh, uh, as well as like, you know, transcription and, uh, other types of, you know, previous, uh, uh, work where a video can be analyzed in seconds rather than someone rewatching it in real time.  
**Speaker 0** 00:08:21:  Uh, and, and drawing out insights. There's a lot of human, um, human initiated and human reviewed, uh, kind of, uh, processes in place. We are very much aware that \<laugh\>, uh, letting the AI do the work isn't actually the work. Um, it's not what we're really, uh, kind of commissioned and known to do. So it's about making it a, a, a another part of the software repertoire that, you know, in the same way that people will use excel to do some heavy calculations and, and such, you would use your AI to do other types of work that, you know, previously would be maybe, uh, done manually. Uh, and it's about speeding and, and making it more efficient. I would say some of the generative tools where it's creating from scratch is a little less, um, uh, relied on at this moment, but it, uh, it's an evolving space, uh, as I'm sure as you know, it's, it's something that's, uh, it's being trialed, is being worked on, but, uh, we're very much in to make sure that the, the AI element isn't without a human intelligence factor, uh, along the way.  
**Speaker 0** 00:09:20:  So lots of possibilities, I guess. Uh, there's, there's a number of, um, uh, trials, um, and experiments on how we're incorporating it, uh, and how we roll it out as a global company as well, because obviously, um, uh, I source is in many, many countries and has many, many protocols and many, many clients that have different agreements around AI as well. So, uh, how we use it and, and, and so on is still, uh, uh, still being shaped up. But, uh, yeah, quite a number of different use cases, uh, and, and across both individually and across different types of teams. Um, it is very much a, a focal point of, of how we, we we change and, and transform how we work.  
**Speaker 1** 00:09:59:  Right. Yeah. Great. Thank you very much for covering a bit of both qualitative and quantitative examples. So in terms of unique, like you mentioned mostly about the opportunities that where it can work, what do you think about the, uh, the challenges that AI users can face in both qualitative and quantitative analysis side?  
**Speaker 0** 00:10:17:  I say, I suppose a, uh, uh, a very sort of, uh, day-to-day element is obviously it's, we cannot make a presumption that it is always correct. We know that the hallucinations are there, and, uh, and then there is, uh, some, some dangers, uh, associated with relying on the AI to do work that's, uh, that makes you lazy, ultimately, that makes people, uh, take for granted that what it, it turns out because it always reads believably that's, uh, it is actually accurate, or I think maybe not. So much of that is purely about accuracy. It's whether or not it's actually insightful. It's whether or not it's telling us something that is meaningful for us to present to our clients, to frame insight around build, uh, recommendations and strategies off the back of where sometimes I would say AI as a tendency to be surface level. It doesn't read between the lines, it doesn't provide the nuances of understanding Mm-Hmm.  
**Speaker 0** 00:11:08:  In quite the same way. And as clever as the prompts can be, and as much as you try and train the data, the ai, it's still some, some distance away from being trained to the same way as experienced research teams and, and, uh, industry experts and research professionals are, are going to be trained and in, and there's a, a difference in the articulations or from AI output that from a human output. And, uh, I've yet to read an AI output that doesn't seem, doesn't read better than, uh, a good writer what a good writer would produce. Mm-Hmm, \<affirmative\>.  
**Speaker 1** 00:11:40:  Yeah. Yeah, that's true indeed. Like many of the researchers, they are worried about this impact that, uh, uh, of course we can use it to empower humans, but of course, uh, they don't know how to trust it. So, so, so in very idealistic scenario, if we, there is no problem with funding and technology, so, and if, if you have to design an ideal AI platform or tool for market research, so what are the feature, main feature or capabilities that that would be must have, uh, that, that, that you would really like to see in that tool and how it could be different from the existing platforms that you're using now?  
**Speaker 0** 00:12:19:  Uh, I mean, that's tricky because I think every, every team and, and not, not as teams, but sometimes, uh, leaders within those teams and, and the way that they want to run their, their projects and their accounts might vary to the point where you want to have some degree of automation. And, uh, and, and, uh, I guess I think in some scenarios, an AI as an employee is great when it's the, the, the tasks it's responsible for are reviewed by a, a, a more senior human. But, you know, if you can get an AI to do labor mm-hmm, uh, you know, laborist tasks, um, which are very much standard and, uh, repetitive or, you know, are, are very much things that could be prone to human error if done at scale. Yeah. Um, I, I would say, you know, having AI as a, a replacement for those task-based elements, uh, is, is good.  
**Speaker 0** 00:13:14:  What I would say is most jobs are made up of a series of tasks, and it's whether or not an AI can be, uh, an agent within the AI or a platform, a home can be built from a series of agents that can synthesize tasks as well as a human, so that, you know, that tasks one through to 10 happen all the time on this, these projects. But it's whether or not knowing once they're done, what happens, what it does next, and what it, what it needs to do, and who it needs to, uh, consult with in order to get information to do the next set of tasks. A lot of the AI transformation that I read about, and, and, and I guess, uh, uh, hear people talk about as well is that it's, it's, it feels quite surface level. It's like we've got a lot of processes or labor intensive tasks as I, I've outlined, and we can replicate or, uh, remove them from humans, um, because the AI can do it faster, better, uh, 24, 24 hours a day, et cetera.  
**Speaker 0** 00:14:10:  But the, the synthesis of the task at the real job, it's a series of tasks that people do and, and, and, and have a handle of. And, and, you know, uh, different stages of different projects and different lifestyle, uh, uh, uh, different, um, uh, cycles of, of where projects are at is, is the skill and is the thing that the experience builds on and then is able to then take to the next level to being client facing output. That's, I think that bridge is still always going to be necessary. And until I see an AI get smart enough that it can do that well without, uh, needing a human to completely spoonfeed it, everything, yeah. I would say that, you know, that's the, the, uh, the ideal blend. I am somewhat cynical of whether or not gen ai, because it is so impressive and powerful, at least on the surface, that's what we train it on, ends up being stuff that has been produced by a gen ai.  
**Speaker 0** 00:15:00:  So we end up in a cycle of, you know, okay, genai can, can produce all these things. It produces more of these things, and then AI just then gets retrained on stuff. It's already produced and it's incrementally going to improve, but not, there won't be the step change for it to allow it to take on, yeah. Greater, um, uh, greater sort of, uh, capabilities with that actually becomes transformative, but also maybe a risk to human jobs. At least that's, that's where I feel like it is at the moment. I'm, I'm, I'm waiting to see if, if, if the technology, the core technologies are going to be able to take that next step, because it ultimately, they need to be trained. And if they're only ever trained on stuff that is being produced by themselves now, then the weight of, of all content, the danger is that, you know, it just doesn't, it doesn't push beyond ceiling where it, it could it actually get to.  
**Speaker 0** 00:15:51:  Um, and, you know, and then there's also a more ethical question around whether it should, because humans still need jobs, and, uh, there is some jobs that, you know, humans still have delight and joy in doing and delivering. So we need to maintain that. And so that balance is, is key and, and, uh, I think an AI platform that can strike that balance is, is good. Um, and I guess, uh, my, my my, my additional point is, um, uh, as I mentioned, yeah, it, a lot of AI transformation that I, I see is, is quite surface level. Yeah. You know, things we do, we're just gonna do with AI instead, but it's not doing anything that it, I guess it's, it's just replacing things that already exist rather than create or enable something that currently can't be done, whether or not it's ab ability to fuse different things together and, and then, uh, tackle, uh, uh, data analysis or, um, other sort of, uh, processes in a more seamless way.  
**Speaker 0** 00:16:46:  Um, that's something else I've, I've seen it described as, you know, AI transformation is very much the, the new sexy language of digital transformation, which in itself is just a rebadged way of getting people to sign off on general business improvement and systems improvement. Because if we just concentrate on making sure that our database is here, talk to each other, and then that, you know, employees or people who need to access that data are able to do so seamlessly without having to jump into three or four different interfaces to, to extract what they need in order to then synthesize it for their end goal, that's not necessarily an AI thing. It's not always necessarily a digital, uh, thing. It's sometimes just about how we, we were set up. And so many companies, I guess big companies like mine, um, but, you know, I don't think uniquely, but many companies grow through their acquisitions as well as their organic growth.  
**Speaker 0** 00:17:36:  And those acquisitions mean they just adopt new systems as part of it, and they try to integrate some big sunsets, some others, but you just end up with a lot of different systems. Uh, I'm sure if I talk to my, my cohorts in different regions, they'll have different, have different systems, different processes off the back of those, that transformation is as important, if not more important in order for an AI transformation to, to be the end result. So a systems transformation overall is still fundamental, and that's just, you know, I'm sure business consulting firms have been talking about that for years. Yeah. And not necessarily finding elegant and powerful solutions to that because we then all talked about digital transformation, but that, that in itself is just now being re-skinned. It feels as AI transformation. But, um, I think there's something they, if, if a company can fundamentally get those things right, the ai, the ai AI transformation will feel more powerful. Whereas if you don't get those other things right, your AI transformation feels like it's just surface level and whatever platforms and, and tasks you're doing for those platforms, it's just going to be a skin on top of what you exist, you know, what you do at the moment. That that ultimately does not change that much. It'll just get faster or more, more efficient, but not necessarily  
**Speaker 1** 00:18:45:  Yeah,  
**Speaker 0** 00:18:45:  Yeah. Not necessarily a revolution.  
**Speaker 1** 00:18:47:  Yeah. Yeah. No, yeah. Thanks a lot for, yeah, you, you covered a, a few very interesting point. One was about the ethical consideration and another one that we, uh, tend to change using different kind of, you know, tools for different part of the same project. So that I, I want to explore the second part a little bit more first, for example, when somebody's, uh, doing this end-to-end research, either it's a qualitative or quantitative, you do the product planning, then you look for the participant, or you look for the kind of tool or, or a way to create data or generate data, and then you do the data analysis. So in this whole, and then eventually you are, uh, presenting those results after interpretation of your results in the, in, in, in automated way or in manual way. So in this whole end-to-end research process, both for qualitative and quantitative, uh, so at the product management level as well, uh, have you been already using AI to, uh, save your time from the possibility of using different tools for different steps or, uh, how AI is impacting your thi this part, so, so that you can focus more on your actual research, uh, rather than just jumping between different tools for different part of the project?  
**Speaker 0** 00:19:59:  Uh, it's, it's, it's been used, I think, to speed up, as I've described, uh, speed up certain things that, uh, have gone before. So yeah, we've definitely had projects where, uh, we are able to, uh, apply the AI to, uh, provide, I guess top line first level analysis of, of a data set, whether that's qualitative or quantitative. It can do a first draft. What then happens is, uh, the human researchers take that first draft, assess what feels good to explore further, what to maybe deprioritize and to start shaping the research. And then it may go into, uh, um, uh, more usage of the AI and other sort of, um, uh, automate automated sort of functions or other types of, um, quicker analysis. We're not using it quite so much, I feel, and it's maybe an opportunity, but, uh, we're not using it to necessarily generate new things.  
**Speaker 0** 00:20:51:  We aren't building our, our data decks completely in ai. It can do some of it, but it won't do all of it. We, we haven't fully integrated it into, uh, deliverables in that, in that regard. Um, and this becomes down to also, uh, uh, a governance and, uh, and a ethics thing around whether or not a client is paying for that. You know, within our, uh, our, our, our contract agreements, some of these clients are not at this moment in time paying for us to use ai. They're paying for our company. And if we have to be transparent about then saying that some of these materials would've been produced by ai, checked by a human, but nonetheless produced by an ai, and, and we, uh, we have to strike that guideline, uh, the right guideline on when that's needs to be, uh, explicit, and when that needs to be sort of just implicit, as in, uh, yes, I think we can assume that assistive use of ai, just to summarize some things or to, to, to create a, a top line summary is fine, but a whole report, at least by AI, is not.  
**Speaker 0** 00:21:50:  So, you know, there are certain degrees of transparency that we have to, to go, to go through there. So yeah, from an end-to-end point of view, I think it's still like a, at the moment, an assistive tool more as a, in the assistive tool space on certain processes, it's used more so as I mentioned. So, uh, if we have a, a, a qualitative transcription requirement or, um, a translation of, uh, a number of different countries responses to open-ended questions, we can quickly put an AI onto that to sum to, to translate, summarize, and produces the quotes that we want to illustrate a particular finding with. And that can be done much more quicker than, uh, a human trying to translate, uh, uh, uh, several languages, and then to find the best quotes within those languages to illustrate certain key points. And AI can do that better. Yeah.  
**Speaker 1** 00:22:35:  Okay. Yeah. Great. Thank you very much. And, uh, for these various, uh, uh, steps and this end-to-end research from product planning, data collection, data analysis to data result, uh, interpretation, can you give me example of some of the tools that you have used that, that you're using for different kind of, uh, uh, you know, uh,  
**Speaker 0** 00:22:56:  AI tools or just, um,  
**Speaker 1** 00:22:58:  Yeah, regular or ai? Uh, basically I'm trying to understand how the whole journey takes place and, uh, what kind of tools?  
**Speaker 0** 00:23:04:  Yeah. Well, we, we have a number of different platforms that can read data, but a lot of our data is nonetheless exported to, uh, uh, either thinking like survey reporter, um, uh, or an SBSS, uh, and uncommonly. We have data processing, produced data tables, traditional cross tabulations, um, uh, to a specification that we've, we've, we've set with them, uh, um, uh, in the setup stage. The data is ultimately analyzed, and most of our deliverables, not all, but most of our deliverables are produced in, uh, a combination of Excel and PowerPoint. And, and then sometimes their reports are produced in, in Word as well. Infographics and other types of design materials are, are, are, are then produced by design, uh, some of the, uh, the design team. Um, but ultimately, yeah, it's, I would say it's fairly traditional outputs. We have some video, uh, from our qualitative work that sometimes gets produced, uh, as part of that, but that's still edited by human, uh, and, and, you know, uh, we aren't using AI to automate a number of those stages as such.  
**Speaker 0** 00:24:09:  Um, with regards to our AI tools, as I mentioned, we have a proprietary interface, so everything is, is done within our walls, uh, on, on the platform that we built. Um, that platform has licenses with all the major, uh, open AI and Gemini and, uh, cloud and umra, and a number of other types of, um, softwares. So that's, you know, we can choose which one we are going to use for different types of use cases. Some are better for written text, some are better for, uh, producing code and, and so on. So we, we we're trying to build a, a single platform for internal usage. That's what, that's what, that's what we have.  
**Speaker 1** 00:24:44:  Yeah. Very good. Uh, like you work for different sectors and, uh, you know, the a uh, when you ask questions to ai, uh, it may not give you, uh, more appropriate answer for healthcare for the similar query. Mm-Hmm, \<affirmative\>, your answer could be sector specific. So, uh, do you, do, uh, how do you make sure that you're using the right tool for different sector? Can you use, how, how do you reduce the bias there?  
**Speaker 0** 00:25:10:  Um, that comes down to usually us retraining our, our platform prior to specific sector use cases. So we have like a, a library of like prompts that work better for different sectors or different methodologies or different approaches, but we all would often retrain them, uh, prior to the response so that it, it, it comes pre-trained, essentially with a lot of Ipsos residual, uh, library of knowledge, of course, many studies, uh, uh, that, uh, that it can upload into a sort of pre-trained state. So it knows it's market. It's a market research agent. It knows it's built off a global company with however many years pedigree and, and a number of different techniques. You don't need to tell it what qualitative research is. You don't need to tell it, uh, these sort of things. But then we would often retrain specifically for a sector saying, okay, here are the latest industry reports that we have, uh, or here's most recent sales or viewing data we have, or et cetera.  
**Speaker 0** 00:26:03:  So we, we give it a data set in order for it to have a bit more of a tighter, uh, reference point with regards to where the prompts are going to go when we are asking about trends or, um, uh, you know, yeah, buying behaviors or, you know, other types of, uh, data kind of based queries. We're still not asking it to generate kind of like hypotheses of like, what might be causing things, because we're not, we're not, we're not naive enough to think that it's going to know we're, we're going to use it to maybe synthesize our own views better, rather than having it try to conjecture things because we know it's prone to hallucination. So wouldn't, wouldn't ask, tell, help us write our answers and correct answers. So yeah, we often have the extra of retraining, and certainly one of the steps that we're trying to, uh, to navigate is how we retrain it on client data and keep that secure. Because if a client is, uh, having its data uploaded, it's obviously very, very sensitive and it can't be made available to anyone else. So everything is very much, uh, locked and secured a way so we can re, we can retrain some of our answers specific to what a client has provided with us, but there's obviously a lot of data handling and other types of security that we, that we have to be conscious of.  
**Speaker 1** 00:27:12:  Yeah. Very good. Yeah, thanks a lot. And, uh, at the end of each analysis, like at the end of each project, either it's qualitative or quantitative, can you gimme some examples of the common formats by which you, uh, you guys discuss your results? Like if you're discussing it with your technical team or if you're discussing with the, the other member or for management board, of course, the, the kind of di information is very different for the different target audience. So what are some common formats that you would like to see at the end of each analysis for quantitative or qualitative? What are the formats or the tools you use for,  
**Speaker 0** 00:27:50:  Um, the, the, the format or the outputs are often, uh, yeah, text, uh, or, uh, table based. Um, we, as I said, we don't tend to, at this moment in time, rely on the AI to produce visuals. Mm-Hmm, \<affirmative\>, um, uh, it, it, I'm, I, I believe it will be a, uh, uh, an inevitability that it will produce, like, you know, produce me a chart that demonstrates the points that we've extracted from, um, from the analysis and such. But at this moment, yeah, that, that isn't within the ai that's on the human researcher to produce once it's happy with, with the data that's being compiled and such. So a lot of the formats on, on outputs are summaries and, uh, and we would prompt and, and ask the, uh, the output to be, you know, um, uh, audience specific. So if we know it's a, a board level presentation, it needs to have the tone and appropriate, uh, brevity that what works for board level, pro, uh, board level presentation or, um, uh, or stakeholders.  
**Speaker 0** 00:28:48:  Similarly, if we know it's going to a, uh, an insight team, uh, uh, who are going to spend much more time with it, it will be more detailed. And, uh, the, the prompts that we would use to, to create that output would be having that sort of persona, uh, create, uh, adopted. So it knows this time around, it needs to be much more detailed oriented, cite the figures, cite, um, uh, and explain more than just condensing to, you know, bullet points or summaries and such. So the, the, the format is mixed, uh, with regards to the, the, the tone and, and as it's, the onus holds on the humans to be ensuring that their sector specific, uh, audience specific, stakeholder specific, or with regards to how they are producing their outputs, um, from the ai.  
**Speaker 1** 00:29:31:  Yeah. Yeah. Very interesting. Yeah, thanks a lot. Uh, just one last question now. Uh, so in terms of the, uh, there are a lot of emerging technologies coming time to time. So these days it's ai. So, uh, what, uh, what, what, what are the, uh, different sides of AI that you're mostly excited about? Or is there any other emerging technology that you would suggest us to keep an eye on that could trans transform market search even better?  
**Speaker 0** 00:29:57:  Um, I think AI has dominated everything because I feel like it's, it's, it's, it's the interfacing with it. It's, it's much more, um, con conversational and, um, that conversational element, uh, I feel is potentially a, a, the real kind of landscape shift. Uh, you don't need to be technical to produce technical outputs. You don't need to learn programming, uh, uh, in great detail and, and master it because, you know, the AI can do a lot of that heavy lifting for you. Um, in terms of other technologies that I don't know, uh, I'm not, I'm, I'm, I'm skeptical about things like AR and VR and the workplace. Uh, I feel like it has certain use cases for certain industry, but for my industry in market research, um, it, it, it feels fanciful to need an AR overlay. Um, we, we, we as, uh, as a species have spent many years looking at a 2D screen, and I think that's sufficient for a lot of, uh, work.  
**Speaker 0** 00:30:53:  Uh, only in certain scenarios can I envisage a 3D environment being better. So I'm, I'm skeptical about VR and ar, uh, elements. Um, AI feels like the one that's, if it can be very conversational to the point where it, it, it can be much more voice driven, um, uh, and then yes, question of what it can produce. And as I alluded to earlier, the synthesis of various tasks into being a more complete holistic job, and then AI get to that level, and then what does that mean for the, the shape of our workplace and, and how we hire, uh, in a, in a worst case scenario, you may not have very junior levels of staff because that junior level, their work can be produced by AIS agents, uh, and such. And then you bring people in to manage Theis, uh, and to, to, to scrutinize and oversee everything they produce, um, rather than to, you know, to have people do that sort of labor in the long term.  
**Speaker 0** 00:31:47:  But I think there's a danger to that because you, you still need people to know what to do in order for them to oversee everything. So there's a, a balancing act to be, uh, to be struck there. So, yeah, I think, uh, for me, I don't think it, I don't think we've, we've cracked the, the whole AI transformation, let alone, you know, uh, other, uh, softwares at this moment in time. Um, and I think a lot of the, the real power will come from that broader systems transformation. I mentioned if we can make sure that our organizations are optimized to improve with ai, because I would say Ipsos is not, we've got so many different systems and processes that it's quite hard for a single person to navigate it, let alone an AI that doesn't have the full guidance. Uh, and that might be, you know, that might lend itself to a, a more leaner organization that's built from, from the ground up with AI and its art rather than, um, uh, one that is transforming or trying to, uh, move into this space.  
**Speaker 0** 00:32:40:  Um, as with anything, the larger you are, the more investment you can have in it, but it could be a smaller player. Yeah. Uh, that actually has something much more centric, uh, AI centric, um, uh, usage that, that is more transformative in, in, in the way it, it works. And that might be the model that, uh, is easier to sort of see where the potential lies. I feel like as a big company like mine, it's a bit of an oil tanker, uh, \<laugh\>, uh, with regards to its turning circle with, uh, with it, but we're investing a lot in it. We're, we're very curious and trying to drive a lot of change, um, mainly to prevent, mainly prevent to be, uh, uh, being completely, um, uh, caught off guard by a small player that comes outta nowhere. So,  
**Speaker 1** 00:33:19:  Yeah. Yeah. I totally agree. Yeah. Thank you very much. Do you have any, uh, question for me about your company or,  
**Speaker 0** 00:33:27:  I, I think, you know, uh, I I, I, I have to say, I don't know a lot a great deal about, um, some of the smart startups in this space and what they are doing that our own innovation, uh, team is not covering off. So I'd be curious to see what your platform ultimately looks like, uh, as you develop out your tools and, uh, and whether or not you, you build for a niche and expand out of that, versus trying to serve everybody with something more generic. I feel like, um, I don't know how you want to kind of, to, to, to basically find your, your, your place in the market because I, I think there's a lot of, uh, a lot of, uh, work going into this, uh, from both startup and also from, uh, institutions like mine, um, with how AI will be used in the sector.  
**Speaker 1** 00:34:12:  Yeah. Uh, yeah, we are trying to provide end-to-end solutions platform basically for, uh, initially targeting the qualitative research side and trying to help that, how somebody's starting from project planning to participant recruitment and then, uh, data analysis and recording and, and at the end, uh, how you interpret, uh, that data. So we are trying to cover this whole landscape and, uh, we are already collaborating with Google to, uh, improve our understanding of various things and, uh, to get different kind of consumer insight. Mm-Hmm. \<affirmative\> and, uh, uh, we are planning to launch our product in, uh, in, in September the first MVP. Okay. Uh, so yeah, we will keep you in the loop, uh, to let you know how, how does it look like when, uh, when it's hitting the market. Yeah.  
**Speaker 0** 00:34:58:  I suppose my only other final point is, uh, actually regarding, um, synthetic, um, respondents, which is another frontier with, again, a lot of ethical questions, uh, concerning it. But if it's, one of the greatest challenges we have is finding people to do the research, the relevant people to do the research at scale, especially, uh, it might start off qualitative, but it may be the question that if we can do enough qualitative data to model out synthetic respondents to allow us to quantitative data off the back of the synthetic data, that could be a another game changer, because a lot of our work involves finding, trying to find very niche samples, and we can't do them quantitative at a global scale, but if we could do a small amount in qualitatively and build out synthetic respondents off the back of that, it could be that we could approximate the, the overall market sizing, uh, and, uh, other types of quantitative needs, uh, measurement needs, um, for, for, for our clients to validate their ideas going forward. Um, but that's a, that's an area of specialism that I'm not, uh, completely, um, uh, close to and such, but I know it's, it's hot in the industry as well.  
**Speaker 1** 00:36:04:  No, yeah. Th thank you very much. That's a very important point actually. My, my specialization is from data science, the computational modeling and, uh, and simulations. So I, I've thought about that, but I'm very in, uh, happy to hear that you feel the use case of this idea. Uh, yeah, we, we might explore that.  
**Speaker 0** 00:36:22:  I think it's gonna be an interesting one because I think some companies will champion it and other people will be cynical of it. And as with anything, when is the, uh, opposing views will have to see exactly where it all nets out with regards to what the industry adopts as a, a common practice or a best practice, uh, in, in this space. Um, very much we'll wait and see, uh, um, but I'm sure it'll evolve in the next, uh, within the next year for sure.  
**Speaker 1** 00:36:43:  Right. Okay. Very good. Uh, thank you very much. It, I really enjoyed talking to you and sorry for keeping you a little longer. That's  
**Speaker 0** 00:36:51:  Okay. No problem.  
**Speaker 1** 00:36:52:  Yeah. Uh, I will follow up with the Amazon watch on your email, uh, very soon. Uh, thanks again for your time and, uh, have a nice rest of the day.  
**Speaker 0** 00:37:01:  Yep, you too. Interesting to talk to you. Thank you, gore.  
**Speaker 1** 00:37:03:  Bye-Bye  
**Speaker 0** 00:37:04:  Bye-Bye.  
