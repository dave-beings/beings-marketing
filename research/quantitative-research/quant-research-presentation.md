# Quantitative Research Presentation

*Converted from: Beings Quant Research Presentation.mp4 (1).docx*  
*Original size: 19K, Conversion date: September 10, 2025*

 Speaker 0 (0:00 - 0:55): 
Momentarily. Um, we're just gonna give you a quick w stop tour, um, of the quant findings, um, to give you some context as you're going through it. But we've also tried to write it in a way that hopefully it's fairly self-explanatory as well. Um, and as ever, throw questions back at us. Add comments to the mirror board, um, as you see fit, if anything is unclear or pro, um, kind of prompts further, um, questioning. Um, so just to quickly recap before I hand over to, uh, Juliet, to kind of go through the meat of, um, the findings, just to kind of set the scene again on kind of what we're looking at here and kind of how we got to these results. So this is a, an overview of the kind of two core surveys. So the one that you sent out via prolific and Anna kind of cleared about a hundred or so of those responses, um, that seemed, um, relevant in terms of their role and, um, to whereas more importantly, the kind of the type of responses they were giving. 
 Speaker 0 (0:55 - 2:05): 
Like the reality is a lot of those roles don't necessarily fit perfectly neatly into our two kind of core ICP criteria around, uh, research teams and research agencies, but they are kind of relevant and give us interesting insights to consider. So, um, we're combining that with the further survey that Anna distributed by prolific, which we only got a very small number of responses in the end, I think it was about 12, that we deemed appropriately kind of matched to what we were looking for. Tough audience to recruit by the looks of things. Um, anyway, that's kind of given us a total of about 120 responses that we've included within this. Um, there are some quarters, very slight differences between these two surveys just in terms of some of the question phrasing and things. So, um, that hasn't really affected kind of the, the results, but made the analysis a little interesting at times. But, um, but all good. Just wanted to kind of flag that in case we have to go back to any of it. Um, and yeah, and I hope it's just given us a really good picture of kind of how we can prioritize some of the insights we found from the qual. So, um, I will, um, hand over to Juliet to go through, uh, the rest of it. 
 Speaker 1 (2:06 - 3:32): 
Yeah, great. Thanks. So, um, just, um, quickly some sort of understanding, further understanding about who, who answered the survey and, and, um, types of research that they carry out and how often they use AI tools. So, um, in terms of segmenting the data, we had about, um, uh, quite many more agency, um, respondents. Um, so compared to in-house and um, academia, um, overall, uh, the primary focus in research is that using MI mixed methods, um, followed by qual research. Um, and in terms of how often the respondents are using AI tools in their research process, um, the majority are using AI tools daily. So 30%, sorry, uh, the majority are using them weekly. So 33% followed by daily at 30%. Um, so they're using them more often than not. Um, um, and now to sort of just have an overview, um, in the qual research we learned that, um, researchers really want something, really want a solution or a tool that's gonna enhance and improve the entire research process for them. 
 Speaker 1 (3:32 - 4:37): 
Um, so speeding up tedious and manual tasks, um, making their work more enjoyable and, and, you know, ultimately helping them have a bigger impact, um, through their work. Um, and based on that qual survey, uh, based on the quant survey data, we can now we've started to prioritize these opportunities, um, that, that we previously identified. Um, and, um, that will really sort of shape Bing's pro proposition. Um, ultimately, researchers want AI tools that are fast, accurate, and trustworthy. Um, you know, they're spending way too much time on admin tasks, manual tasks, and they just want a tool that's gonna help them achieve more with less effort, most importantly, without sacrificing quality. So what's the opportunity? The opportunity for beings is to automate the heavy lifting, speed up deli the delivery of insights and make the research process more enjoyable, efficient, and impactful ever than ever before. 
 Speaker 1 (4:37 - 5:47): 
Um, and I, I wanted to pull these, this over from, um, the previous qual study. We identified these five S's and I think it's important to sort of keep these in mind as we go through because, um, it's clear these, these five sort of themes, um, are really, really prominent throughout, and we should pri be prioritizing them. Um, so sorry. Um, so, um, the jobs to be done, uh, that we went over last time, uh, the way that we, um, the way that we, uh, work out the job to be done, the jobs to be done, we give them this opportunity score, and that opportunity score is based on how important the researchers think that particular job or, or, or task is. And then also how satisfied they are with that current, their current solution or their current tools. Um, and then based on those op um, importance and satisfaction scores, we, we can get an opportunity score. 
 Speaker 1 (5:47 - 6:56): 
And that basically means, um, that we, the higher the score, the bigger the opportunity there is to innovate or improve the solution, um, because it means that the task is really important, um, and their current ways of doing things aren't meeting their expectations. Um, and anything over 10 is deemed as a, as a strong opportunity. Um, so based on that, um, we were able to identify the, the top opportunities for beings, and they, those really lie in allowing researchers to quickly process large volumes of qualitative data without compromising on depth or quality. Um, so you can see here the, the, the top jobs are highlighted in green, so it's really about, um, um, speed, um, scale. So being able to process large amounts of data quickly, so speed and scale, getting to insights faster, um, which is all about speed, but also strengthening the results, um, which then goes into the delivering clear and actionable insights. 
 Speaker 1 (6:57 - 8:34): 
Um, so again, strengthening those outcomes and those recommendations and those insights, um, handling more data without losing the depth and quality, um, and then also reducing time spent on admin tasks. Um, so really bringing those five s's into it as well. We're able to sort of flesh those out more with these jobs to be done. And identifying which have the highest opportunities. Um, I think it's important to note that, um, although, you know, these jobs to be done have low opportunity scores, um, they're still important and they shouldn't be ignored. Um, the, you know, the bigger, um, the bigger t the bigger, the bigger focus should be on tackling these high priority tasks like speeding up data processing and delivering actionable insight insights. But then there's also value in helping researchers enjoy their work. So by addressing the pressures of their roles with solutions that make their jobs easier, will naturally combine to a more co sorry, would naturally contribute to this more enjoyable experience for them. Um, so by achieving their practical needs, like managing data and reducing admin work beings can also make their work more satisfying and engage and engaging in the long run. Um, so I think it's important to flag that these shouldn't be disregarded, um, but the focus should be on, um, effectively sort of prioritizing these and addressing these pressures first. 
 Speaker 1 (8:36 - 10:18): 
Um, so I just mentioned the biggest opportunity in terms of jobs to be done is speeding up the speeding up large scale data processing. And that this is really further highlighted, um, by the fact that researchers main pain, the main pain point or the main, um, challenge is the overwhelming amount of time spent on manual tasks. Um, which you can see in this, in this data here. Um, so time consuming tasks, delivering insights quickly, judge juggling multiple projects, um, and ensuring unbiased research design are the top challenges researchers face. Um, so, um, yeah, I think, um, it's also important to notice while I was segmenting the, the, the data that came from both the surveys, so both from your survey and also Anna's, um, in Anna's surveys, which was sent out to those more targeted profiles, um, the participants who are more closely aligned to our target profiles, 70% of those respondents said that recruiting high quality participants quickly and at scale was a major challenge. It was actually their second biggest pain point. So I think it's important that we, um, also have that in mind when we're thinking about, um, top pains and challenges. Um, 'cause as you can see here, it's only 33% overall, um, who said that? But yeah, it was 70% in Anna's. So is it, is it also considered a major, major pain point? 
 Speaker 0 (10:18 - 10:58): 
I think as well, just to kind of add in here, like when we're starting to ladder these things together, like you can already see some really interesting narratives that kind of pull together those top jobs to be done where there's the biggest opportunity and kind of marrying them with some of these pains that we know people are facing. Like, you know, with that kind of top dog to be done around pro uh, processing large volumes of qua qualitative data in a way that's rich and accurate and all the rest of it, but also done under this kind of pressure to deliver quickly. Like we can start to build up some kind of interesting, um, narratives and stories around that, that kind of help position beings against both that goal and that kind of current challenge that they're facing. Yeah. 
 Speaker 1 (11:01 - 12:21): 
Um, yeah, so building onto that story, uh, so, you know, given that time consuming and, and manual tasks are the biggest pain points for researchers, um, it's, it's, it's no surprise that having a user friendly interface and, and something that's easy to use is their top priority when they're looking for, um, a tool to help them, um, with sort of making their research less time consuming and, um, more efficient. So, you know, having a tool that's difficult to navigate and would only add more time and frustration, trying to understand how to use it, um, um, is, yeah, sorry that, so having that ha having a tool that's easy to use, um, and, and simplified is key. Um, on top of that, um, ease of use and user friendliness. Uh, the top other sort of features or attributes that they're looking for in a solution are, um, integration customization. So having those more tailored features that are specifically de designed for researchers and also, um, security are really important, um, for researchers when they're looking for a tool. 
 Speaker 1 (12:30 - 13:52): 
Um, when it comes to the barriers, so what are those inertia, the things that are holding them back or blocking them from making a decision to using a tool? Um, uh, what, what are the top, what are those top barriers? What are they most concerned with, um, or skeptical about? The number one thing was, uh, trust and accuracy concerns with AI output. So, um, you know, often they require human and oversight, especially with the more t complex tasks, like understanding nuances in human behavior or the ability to capture those more nuances in, in, um, language or dialects or accents. Um, so overall, um, researchers are, you know, really concerned about whether AI will produce accurate, accurate, and reliable results. Um, and, you know, they feel that human oversight is needed when it, when they're using these tools because they can't fully trust that they're not gonna hallucinate or, you know, not fully understand the data or the context that's put into them. Um, so yeah, they, they, they really need confidence that AI is gonna be able to deliver these re reliable results while keeping, um, their data secure as well. 
 Speaker 0 (13:53 - 14:24): 
Yeah, and I think this is, like, this is interesting to see these kind of top these coming out as the kind of priorities from all of the, um, blockers that we discovered in the qual. 'cause I think it plays really nicely into our kind of like AI and HR kind of narrative, you know, that combination of AI and, and human intelligence combined, um, and I think kind of really gives us a nice platform for that proposition because we, we really know that that is kind of top of their list in terms of things that they worry about. And so it creates a nice space for us to, to play into. 
 Speaker 1 (14:26 - 16:15): 
Yeah. Um, so then, um, we, one of the questions in the survey was to understand what are the actual tools that people are currently using, researchers are currently using, and, and how, you know, um, how, um, prevalent they are. So I think it's, it's important to flag here that this particular question was quite challenging to ac accurately quantify. Um, there was lots of misspellings as you can imagine, or inconsistent formatting with how the, the responses were written. So, um, this is, this current analysis is the best overview we have currently, um, but it's not fully inclusive of all the, all the results. Um, but that being mentioned, it is accurate in terms of what are the biggest players that we should be aware of right now, um, in terms of popularity and, and, and how, you know, which is being used. So chat GPT emerged as the most popular, followed by Gemini and some others that you can see here. So n nvo, uh, uh, NVivo, sorry, and, um, Claude Copilot and Dovetail. Um, and these are what we're gonna really dive further into, um, in the next, in the next stage of the research, in the next stage of the project. Um, so yeah, I think it is, it's, it's, it's good to see this as, especially as like these new ones, like NVivo we haven't actually heard of before or seen come up in previous research. So, um, yeah, we've got some good competitors to really dive into, um, next week. 
 Speaker 0 (16:16 - 17:30): 
And I think it's interesting that a things that chat GBT and emini are coming up so much. I mean, it's, it's not surprising in that they're obviously free tools to a degree, so they're very accessible, um, and probably just kind of help with some of that initial heavy lifting. But we also saw, um, I think earlier on, you know, that researchers want tools that work within their research context that aren't just kind of generic AI tools, but that actually understand what they're trying to do and are specific to research. So, um, it's, I think that's kind of interesting and particularly when we start to look at that competitor, um, mapping along the research journey to kind of understand is there, is there a big gap that's missing that like people are not for choosing those tools, but using these kind of generic open source tools that they don't necessarily fully trust [laugh], um, but they're defaulting to. I just think it's something interesting to just kind of like, dig into a bit more, we look at competitors. 'cause um, yeah, I think again, it's like looking at if we can marry up some of those blockers with this to try and understand a bit more about, like the behavior be behind people choosing some of the, those tools over some of the bespoke research tools. 
 Speaker 1 (17:30 - 17:46): 
Yeah, and I think also it'll be the key thing here is to really understand about like what part of the research journey are they helping with and, and which area of that research journey is, is most underserved as well. 
 Speaker 1 (17:49 - 18:40): 
Um, so now we've come to the, like qual side of the quant [laugh] survey. Um, so there was some really rich insights here. Um, a lot of language, a lot of quotes, um, that we can use to sort of inform that messaging as well when we come onto that and, and really understanding what are the words, what are the, what's the language that people are using to talk about AI tools and, and to talk about their experience, they've, they've had using them. Um, so yeah, we've got some really, we've got some quotes here, but like I said, there's a lot of rich data that we can, um, come back to in a separate document. Um, so in terms of the question, um, of, you know, what are the expectations that researchers have when it comes to using AI tools? Um, we've sort of categorized the, the quo findings, the quo answers into, um, some key themes. 
 Speaker 1 (18:40 - 19:48): 
And we've, we've labeled them, we've, we've come up with a labeling system of high to low. Um, we've done this because, because likeness said earlier, the, the way that the quality data was collected, it wasn't possible to fully quantify, um, the ordering and prioritize these key themes, but we have been able to sort of identify or give some indication to the ha, you know, the prevalence across the responses in terms of these key themes. So, um, yeah, that's, that's what this labeling here is. Um, so in terms of what the, the high, you know, the, the most prevalent theme that came out, um, in terms of expectations, it goes in line with everything that we've sort of been building up with that story in terms of making their work faster, more efficient and, and simpler. So, you know, there was quite an over overall the, the, the perspective or the perception of AI tools was really positive in the fact of, um, speed, time and efficiency, um, especially in relation to tasks of cleaning and analyzing the data. 
 Speaker 1 (19:49 - 20:50): 
Um, but again, uh, that, that human oversight was mentioned as well, that, you know, it helps reduce that time, but still, um, not forgetting the, the, the human side of going over it. Um, but, but overall AI tools, um, yeah, really, really help with that speed and efficiency. And also, I think it's key, a few people were mentioning that, you know, they're becoming, these tools are becoming easier to work with and they're improving at a fast rate. So I think the insight, um, there really is, there is expectations that these tools are gonna get better and more efficient as well. So, um, keeping up with that is, is gonna be key to, um, to match those expectations. Um, so yeah, I, I, I'll leave, I'll leave the quotes for you to sort of go through in your own time, um, as well. 
 Speaker 1 (20:52 - 21:57): 
So after that, um, expectations, what, what the theme that came out was the idea about job security and it replacing human insight. So there was a, a reoccurring concern, um, or worry that came out from the researchers that AI might take over their jobs, make, maybe make their roles unnecessary. Um, and, you know, a strong belief that AI should not replace human researchers, but help and aids them, um, with their work. Um, I liked this little quote here about AI will essentially, um, co-PI be a co-pilot to researchers. And I think that's like, um, something that we can keep thinking about, especially in terms of how are we gonna talk about, um, beings as what, what are the descriptive words that we're gonna give it? Um, 'cause I remember last session we said something about like co co co chef and sous chef or co-pilot to research who came out in that quote there. 
 Speaker 1 (21:59 - 23:22): 
Um, and I think also in, in one of the interviews that we analyzed from the, from the qual, from the qual stage was the idea of it, um, the tool being a sparring partner. So again, that play on words there, I think we can really sort of, um, refine that and come up with a, a really good, um, way of describing it. Um, so ethical concerns and data privacy also came out as quite a, a, a, a important theme on and concern. So the idea about how AI handles sensitive data and whether it's being used ethically, um, and securely, um, so, you know, in terms of asking them about their expectations, concerns came back to it. So I think that's really important to constantly be thinking about how are we gonna address those concerns, um, and ensure that, you know, people feel com confident and, and trusting in, in the tool that they're using. Um, so another thi theme that came up was this idea of struggling to get buy-in, um, and having some sort of leadership or stakeholder hesitation when it comes to, um, AI tools. So, um, they stressed the need to show quick wins to help convince others to adopt ai. Um, yeah. 
 Speaker 1 (23:25 - 24:12): 
Um, and again, another, like we saw in those, you know, the inertia or the barriers to adopting or using an AI tool, um, that idea is also, you know, come through here as a theme in terms of, um, uh, the, the reliability or, and the trusting in using an AI tool. Um, you know, in terms of how accurate it is, how many, you know, reducing those errors and, um, you know, want not having to check every single entry or every single response that they get back. Um, so in terms of expectations, they're really hoping that AI tools will become more reliable and they won't have to set second, second, check it all the time, um, every time. 
 Speaker 0 (24:15 - 24:49): 
I think interestingly as well, these are obviously all unprompted core questions, and actually there's some really kind of strong, um, parallels here with what we saw in the qual when we were kind of, if you look at the blockers, um, side of things, um, as well as some of the pools. Like, there's some kind of really strong correlation between the themes here as well, which is, um, kind of good to see. 'cause it's just kind of building on the confidence, um, confide those being kind of key areas as to for us to address and focus on, um, in terms of the proposition and how we, how we position beings. 
 Speaker 1 (24:50 - 25:52): 
Yeah, exactly. Um, so then, uh, the next question, the next quote question was asking them to sort of define the ideal AI tool. And again, like, it's, like Ness said is really, is it's a lot of these were unprompted questions and it, so therefore it's giving us a lot of confidence in even further prioritizing, um, those insights. So, um, we know that one of the key, the the number one feature that they're looking for in an e, um, AI tool is something that's easy to use and user friendly, got a user friendly interface. So that came out in, in the qual, um, question of asking to def define the ideal AI tool. Um, they don't want something that's gonna overwhelm them and add more time to their work. They want something that's easy to use, intuitive, um, and time, like not time consuming, um, or fiddly. 
 Speaker 1 (25:53 - 27:02): 
Uh, another, um, really important or high, highly frequently mentioned theme that came out in this question was the idea of efficient and fast data processing. So, um, again, that was really represented from the jobs to be done question. Um, they want something fast and accurate that they can put on in a, a large quantity of data and, and the output is gonna be, um, accurate and, um, quickly summarize their research questions. Um, you know, they don't want something that's gonna hallucinate, um, and reliable, robust, fast. You know, these quotes are really, really strong in terms of that language that we want to use when we're thinking about how to talk about, um, beings and, and, um, and the features and benefits, um, summarization and thematic analysis. So this was a common request that an AI tool has the ability to summarize long transcripts, identify key themes, and, and organize data. So making analysis easier and quicker. 
 Speaker 1 (27:04 - 28:22): 
Um, so yeah, automation, automation is, is key here. Uh, the next theme, natural language processing and contextual understanding. So this came out, you know, that, that, um, concern or skepticism that AI tools can't, don't have the ability to fully understand the context or nuances of language, um, when describing their perfect AI tool. They want something that will be able to understand the tone and context and accurately sort of interpret those responses with the emotion, um, that's given. So, um, you know, we, that this is a, a theme that came out of the, the qual research in terms of those local dialects or language or, you know, the way that people describe things they might use be using square, square words or talking about things in a, a way that AI tools at the moment might misconstrue or not fully understand the context of them. Um, trustworthy and accurate trustworthiness and accuracy, um, is really important in a tool for respondents. 
 Speaker 1 (28:22 - 29:13): 
Um, again, like I said, they don't wanna be double checking for mistakes. And finally, um, integration and visualization capabilities are really important in AI tools. They want people, researchers want something that's gonna integrate with their current platforms or their current tools, um, and also the ability to offer these different outputs based on, um, you know, who, which stakeholders are they presenting to, um, and therefore what visual output, how shall I visualize the data? So they want tools that are gonna be able to offer them the flexibility to sort of create, um, different findings, um, to different people and stakeholders in all formats, um, that each of the audience will understand in their own way. 
 Speaker 1 (29:15 - 30:29): 
Um, so now we're, um, now the question of, uh, key opportunities and features for improving research with AI tools. So we asked respondents, we asked in the, um, survey, what are the key opportunities? What do you think the key opportunities are in, um, research? And what are those features that will be able to help with those opportunities? Um, so what are the key themes that we identified? Um, so firstly, data analysis and coding. Um, respondent see, one of the biggest opportunities is making data analysis easier, um, using AI to automatically code data, identify patterns and all themes faster. Um, so this is really about making research more efficient and, um, save time from this. They, they suggested features like, um, NC recognition, anomaly detection, and sentiment analysis. Um, and, you know, you can find the quotes sort of on this side that, um, are talking all about those different features. 
 Speaker 1 (30:29 - 31:32): 
So, you know, someone's put here about relationship mapping, identify relationships between context, the anon detection, so identifying outliers or unusual patterns. Um, so that will all really help with that opportunity of data analysis and coding. Um, so next is the idea about speed and efficiency. Um, in especially for tasks like transcription, data analysis and summarization, and the features that sort of were mentioned related to this theme, were having tools that have more processing power automated systems, especially for large handling large data sets. Um, real time coding analysis would help reduce time spent on repetitive tasks, um, or having, you know, a specialized search function for quick access to key themes and insights. Um, and again, you'll be able to see sort of more detailed, uh, quotes related to those specific features mentioned here on the right. 
 Speaker 1 (31:35 - 32:37): 
Um, next, another theme. The next theme that came out, um, is transcription summarization. So the opportunity here to improve the transcriptions, um, uh, services, especially when it comes to interviews, when there's, like I said, there's nuances and accents or, or, or language. Um, uh, they want tools that can sumi summarize large amounts of texts more efficiently, um, which goes in line with their, you know, number one jobs to be done, which comes to managing big and large data sets. So, which features will help with this high quality transcription, um, tools that can handle diverse accents. Uh, speech to tech software, uh, would be really valuable. Um, editable, um, editable scrap transcript, um, as well, which can help with the tagging and the analysis. 
 Speaker 1 (32:44 - 34:22): 
Um, so next is, um, pat pattern recognition and theme identification. So respondents really think that there's a big opportunity here for AI to further enhance research by automatic automatically recognizing patterns and identifying themes. Um, so, you know, some, some of the features mentioned are entity recognition, interactive visualization tools, um, sentiment analysis, um, uh, you can see here. So smart data visualization, emotions detection, um, ability to pass large amounts of data held in different formats. Um, uh, I think, yeah, the next theme is the idea about going back to this idea about, um, how AI should assist and help rather than be replacing human researchers. Um, so that human element really needs to remain in the analysis and interpretation of data. Um, and so in, in for this opportunity to sort of have that human touch and interpretability, um, what features were mentioned. So people, um, mentioned, um, the ability to manually refine AI generated insights, um, and then some other features that sort of have already been mentioned in previous, um, themes. 
 Speaker 1 (34:22 - 35:59): 
So emotion detection again. Um, but I think this, the, this point here is really that ability for the researchers to collaborate with the tool, um, and manually sort of work together, um, once the tool had sort of done that bulk initial analysis. Um, and finally, um, and again, a theme that's apparent in most of the questions is this idea of data privacy and security. So respondents see a clear need to improve data privacy and security and I AI tools, and there's an opportunity there. Um, and yeah, the AI tools and systems really need to be transparent and trustworthy. Um, how are they using, what do they do? How do they store that sensitive information that's given? Um, and the features that were mentioned with this idea of built in compliance with global recognition data, encrypt, encryption, secure storage, um, GDPR compliant solutions that can run offline. So these are some features and or, or ways of speaking about the features, features that we should really consider when we're talking about how being stores that, um, and how seriously we take, uh, privacy and security when it comes to, um, data, all data, not just, you know, all data sensitive but sensitive data. Um, and again, you'll be able to see further sort of quotes and information, um, on the board. 
 Speaker 1 (36:02 - 36:03): 
Um, so yeah. 
 Speaker 0 (36:04 - 36:53): 
Yeah. And then I just wanted to, thanks Julia. Sorry I didn't mean to jump in there. Um, so just this last note, we'll just work coming to the end. We'll talk about next steps in a second. But just a note on audience segment segment prioritization. 'cause that was kind of one of the big things that we wanted to focus on with this research is kind of answer that question of like, who do we, who do we go after? Who do we focus all our, of our efforts on between these two groups of in, in-house research teams versus research agencies? And the reality is that the way that we, this way that this quantitative was collected and the kind of the fact that we didn't get really precise targeting of those two groups in terms of where the participants originated from. We haven't got a quantifiable answer to that question yet to definitively say there appears to be more opportunity with an X versus Y. 
 Speaker 0 (36:53 - 38:05): 
But we've kind of discussed this at length as a team. And ultimately I think based on all of the research we've done so far, the qual, the qu, the conversations we've had ongoing kind of in between our hypothesis still remains that there's a greater opportunity for banks within in-house research teams. Um, and really that's something that we should continue to drive forward and look at how we can validate further through testing in live channels and actually, you know, follow that hypothesis and, and look at, um, uh, the opportunities of the net. And that's not to say that we can completely exclude research agencies, you know, we can still continue to have a testing stream around that, but um, if we want to have a kind of working hypothesis to, to drive, um, decision making, that's kind of where we've, uh, we've landed on that and there's some justification for that here just to kind of bring that to life, um, that I'll let you read in your own time and we can have a separate conversation around that, um, separately as well, um, to kind of keep the dialogue going. But just wanted to kind of mention it specifically just because we don't, don't have that quite quantifiable kind of final kind of line in the sand, um, there. But we do have kind of pretty strong kind of gut feeling, um, around um, where we should take this. 
 Speaker 1 (38:08 - 38:13): 
Yeah. So that takes us onto next steps. Um, nest, do you wanna go? 
 Speaker 0 (38:14 - 39:05): 
Yeah, absolutely. So the, David, just the three things that we've spoken about in terms of things we want to finish off with the final week. Um, we've got working together as part of this first phase of the project. So, um, as we've discussed, we're gonna refine the messaging framework. So using, now that we have this qu quant data to kind of help us refine, um, which jobs to be done problems, et cetera, we should focus on, we can use this to kind of build out, um, and refine that messaging framework that we've started working on already, um, to give us the kind of firm foundation to be working with moving forwards. Um, we'll also then, um, build on the competitor work we've done so far, um, in terms of just mapping those compressors along that research journey, um, so that we can identify any of those kind of underserved stages that would be, uh, pertinent for us to kind of really focus our efforts on. 
 Speaker 0 (39:05 - 40:04): 
Um, particularly as you said yourself from a kind of roadmap point of view. 'cause that, does that give us kind of any clear opportunities to go after or prioritize. Um, and then finally creating a plan for the website. So again, you know, it's a really key part of the whole process or all of the marketing activity that, um, will be up and running again, kind of all roads lead to that and it being, you know, a huge part of as effectively communicating the value proposition and supporting that kind of solution and valuation process that most prospects will go through. And so we'll put together a plan of what we think that should include, um, and uh, some best practices around that so that we're in a really good place to brief, um, that in, um, as and when we're ready to. Um, so yeah, so I think that's very much everything for now. We'll leave, uh, this with you to digest obvious as I said, any questions or if you wanna jump on a call next week to talk to any of it all, just let us know. Um, and we'll be happy to, um, to do that. Yeah. Alright.

---
*Converted from original Word document.*
