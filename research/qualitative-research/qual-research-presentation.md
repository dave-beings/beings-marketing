# Qualitative Research Presentation

*Converted from: Beings Qual Research Presentation.mp4 (1).docx*  
*Original size: 28K, Conversion date: September 10, 2025*

Speaker 0 (0:01 - 0:10): 
Yeah. Exciting. Okay. So, um, I, let's hand over and we'll, um, we'll get going. Um, 
Speaker 1 (0:11 - 0:14): 
Oh yeah, I'm very excited. I'm sorry I couldn't do it a bit later. 
Speaker 0 (0:15 - 0:31): 
No worries. We always just want more time and more, like it's always that bit like if we want more, could we do, but I think it's in a really good position to share and, um, talk you through and talk about sort of next steps in. So, um, but I don't want to steal any thunder, so I'll not anymore <laugh> 
Speaker 2 (0:32 - 0:44): 
And Dave, I thought I'd just introduce myself 'cause I know. Yeah. So I'm Julia, I'm the growth strategist at the scale up and I've been working closely with Anna to, to dive into all of the insights for today. 
Speaker 1 (0:45 - 0:49): 
Lovely to meet you. Seen your name on various things, but it great to meet you in person. Here's 
Speaker 2 (0:49 - 0:50): 
The face, <laugh>, 
Speaker 0 (0:51 - 0:51): 
<laugh>. 
Speaker 1 (0:53 - 1:03): 
Go for it. I'm excited for this. Very excited. I know this is sort of like I part one of of two 'cause we kind of all, all gets grounded in the quant as well, but, um, yeah, very excited. 
Speaker 0 (1:04 - 1:05): 
Perfect. 
Speaker 3 (1:07 - 2:05): 
No, exactly. So, um, yeah, Dave's kind of, uh, stole what I was to say. So, um, just in today's session, um, objects is really of today is just to kind of have a touch point and update on some of the learnings that we've gained, um, around some of the core hypothesis that we were, uh, looking to test and, and gather more insights around, um, in the qual research. So, um, just kind of updating you on, you know, new things and the learnings around those core research objectives. Um, so we were interested, obviously our two main objectives is sort of understanding who would buy beings and why. Um, and then also how to best communicate the product to them. Um, so specifically we were looking at in order to understand why they'd buy the product, what problems they'd pay to solve, and then also what, why would they choose to look for a solution like beings and what is it that's triggering them to kind of act on, on those problems, look for a new way. 
Speaker 3 (2:06 - 2:56): 
Um, and then also thinking about how we communicate the product important for us to kind of draw out, um, what is it that excites 'em about beings? So what particular features, benefits or attributes, give them confidence, it's the solution that will solve those problems. Um, but also understanding some of the core blockers and what could make beings a difficult sell into, um, the target audience. So that's the main chunk of it today is just sort of update on, on those learnings from the qual. Um, but then also as you mentioned, we want to, then the next stage of that is the quant research where we'll be sort of testing and prioritizing these insights and also looking to segment them, um, by audiences so we can get to kind of clear articulation of where the biggest opportunity is, um, and what that audience value proposition is for, for that. 
Speaker 3 (2:57 - 4:13): 
Um, so that's what we're gonna get through today. Um, in terms of just a recap on what, how we've, what we've been doing and to get to this stage. So, um, obviously <inaudible> conducted quite a few audience interviews. In fact, I think he'd done about 20 or so, uh, or about 19 or so. Um, so we were making use of that, um, insight and that data. So Juliet's been working through synthesizing all the, the data from those transcripts into what we call sort of voice of the customer database, which is basically, um, just a collection of kind of all the different coding that we do on the call data. Um, but we've got, therefore got kind of all the language that they're talking about when they talk about these problems and, and goals that we, we need to talk around in order to understand the value. Um, so we've been synthesizing those, that data from Garret's interviews, but we also conducted our own, uh, additional interviews to fill, um, the gaps in terms of personas that weren't so well represented in, in who GRI had spoken to. Um, so this is a kind of better view of who we spoke to. Um, so majority of the people that we spoke to were, um, qua researchers. Um, we had a pretty even 
Speaker 1 (4:15 - 4:17): 
You source prolific people, is that your No, 
Speaker 3 (4:17 - 4:24): 
Yeah. So in terms of the recruitment method, no, we did it all through LinkedIn. Um, so kind of direct outreach to I think appropriate 
Speaker 1 (4:25 - 4:30): 
So that Sales navigator plus meet Alfred in terms of, or something in terms of the tools and stack. 
Speaker 3 (4:30 - 4:44): 
So we actually actually managed to do it pretty manually, um, because we have quite a large network of researchers anyway, so I was just re reaching out to those. Okay. Um, but yeah, we did seven additional interviews, so, um, yeah, it wasn't too much to, to do that. 
Speaker 1 (4:44 - 4:48): 
Cool. And in terms of coding and tagging, just again out of interest, what did you use for that? 
Speaker 3 (4:49 - 6:03): 
So we, we really structured that around, um, firstly our hypothesis. So any insights that were around those hypothesized jobs that we'd already discussed in the hypothesis mapping. So all the different areas in which beings we was, we are presuming that beings provides value to the data. So we structured around those hypotheses and then we also coded it by, um, our specific method for analyzing data, which is jobs to be done triggers, um, and those four forces as well of progress. Um, but we, we've actually shared the language database down here, so you'll be able to sort of have a look through that as well. Fabulous. In your own time. So yeah, in terms of pretty evenly split between in-house teams and agencies, um, the predominant focus for my recruitment, the additional interviews was around the UX product researcher as opposed to the more kind of core market researcher. Um, so yeah, we, I feel like we've got a, a good representative group here for, uh, LA Qual. Um, yeah, a mixture of, um, large in-house and small in-house researchers and large research agencies and small research agencies as well. And Gerri had also conducted some interviews with academics. Um, cool. Just 
Speaker 1 (6:03 - 6:27): 
In, in terms of, um, just on the personas, in terms of the, um, in terms of the job titles, were they all the same? Uh, would or, and structurally, did they sit within marketing departments? Did they sit within research departments? I'm just, again, interested when it was a qual researcher that may be different to when it was a UX or product related job title. Um, 
Speaker 3 (6:27 - 6:51): 
Yeah, so I mean the, obviously like we categorize them as sort of CVP, um, they all had different names. You had sort of research director, you had head of research, um, when it was sort of the core research themselves. Yes. In, in in-house teams, they did tend to sit in research teams, dedicated research teams. And that was the same for product as well UX researchers. They were actually within a, a research team as opposed to the product team. 
Speaker 1 (6:52 - 6:57): 
Cool. Okay. So nobody from within marketing departments would you say? 
Speaker 3 (6:57 - 7:08): 
No, we didn't speak to anyone within marketing departments. Um, obviously market marketing was mentioned as stakeholders. Um, yeah, those researchers we, we spoke to, but not directly 
Speaker 1 (7:11 - 7:11): 
Continue. 
Speaker 3 (7:13 - 7:22): 
So Julia, do you want to kind of do a quick, basically we want to do a quick summary of some of the kind of core themes that have come out, um, in this core research. 
Speaker 2 (7:23 - 8:22): 
Yeah, so the, these five themes, we, we are calling them the, the five Ss. So speed, satisfaction, strength and scale and safety. Um, and what we did, we, we selected them because they're really aligned directly with these core challenges and opportunities that we identified going through the interviews and in the research. Um, and we believe these are sort of the key areas where we can provide the most value to, to researchers and organizations, um, making their work better and easier. So the first one, um, beings, beings helps get things done faster, um, and taking care of repetitive tasks so researchers can focus more on the important staff. Um, and speed obviously is, is an obvious one because many researchers are short on time and need to juggle multiple projects. Um, so the second one is all about making the research process more enjoyable for researchers, um, by handling tedious tasks. 
Speaker 2 (8:23 - 10:07): 
Um, but it also ultimately leads to better recommendations and outcomes, which leaves clients happy with these valuable insights. And also it's made, you know, they're satisfied because they've received good VA value for their investment in the research. Um, so strengthen is all again, you know, related to satisfaction is about making, helping researchers making make better decisions, um, and improve their relationships, strengthen their relationships with stakeholders by providing these clear and strong insights. Um, so you know that strengthen is really important 'cause researchers need to show the value of the work and, and get support from others, um, along the way, uh, scale. So, you know, it's all about letting researchers handle, handle bigger projects, um, with work that has larger large sets of data. And also it's about reaching more people across different regions. So, you know, um, global projects, um, with multiple languages, all of that stuff. Um, so we, we included scale because research is becoming more complex and, and, and beings can help manage this growth effectively. And finally, um, it's, it's essential to keep data private and, and meet, meet those ethical standards when using AI generally. Um, and, and it's really important because researchers are con are concerned about protecting sensitive information of their participants and all the data and, and they wanna make sure that AI is used responsibly. 
Speaker 3 (10:12 - 10:28): 
So we can basically kind of summarize all those key themes from the sentence above, you know, being significantly enhances the entire research process, um, elevating how researchers spend their time and use their expertise to make research more efficient, enjoyable, and impactful. Um, so that kind of, 
Speaker 1 (10:29 - 10:30): 
I know it. 
Speaker 3 (10:31 - 10:39): 
Cool, cool. So, um, this is language database for you to produce in your own time, um, any 
Speaker 1 (10:39 - 10:46): 
Of that <laugh>, but yes, we'll take a look. I'm super interested in that actually, really, really interested in that. Um, but yeah, we can come back to that. 
Speaker 3 (10:47 - 12:01): 
Cool. So just to kind of take a bit of a deeper dive then into some of those themes and, um, what we've learned around the core research questions. So obviously one of our big, uh, goals is to really understand the reasons why someone would buy, um, beings. Um, and we use the theory of job to be done as we've been, as we've kind of gone through before. Um, but really it's understanding what are the problems, struggles, um, and goals that, um, our audience have when it comes to, um, research and how do they then define, um, what making progress and the better kind of life around that particular job looks like. Um, so in order to kind of answer these questions, we've split, um, what we've learned into, you know, what are their core pains and also what are their kind of core, what are they really looking for in terms of how they define the making progress or, or kind of better reality that tool could help them get to, um, I dunno if we'll have time here through all of these, but, um, I, I want to kind of focus in on the ones that we feel we've learned a lot more around in addition to the, what we kind of mapped out more hypothesis mapping. 
Speaker 3 (12:02 - 12:41): 
Um, but I will go brief in brief, uh, run through all of them. So in terms of the pains, we've kind of got a, a category which is kind of just general qual research pains. So what are the kind of issues that these organizations face and researchers face when it comes to conducting qual research? Um, the main one being time constraints. Um, researchers and and organizations are really kind of feeling that pressure to deliver, uh, qual research insights quickly. Um, and when qual itself is quite a time consuming activity, it makes the, um, ability to kind of manage the, that those projects effectively difficult. Um, 
Speaker 1 (12:41 - 12:58): 
Sorry, quick one is, is your impression from the conversations that the time is the time it takes to conduct the interview or the time it takes to analyze the interview and code it and tag it and get the useful insights out of it? What would you say or what you got if you don't dunno, <laugh> 
Speaker 3 (12:59 - 13:14): 
It, it's a combination of all of it. It's definitely they recognize that the analysis takes a chunk of time, um, but equally sort of setting up the, the study scheduling the participants, that takes significantly longer than a research project. Mm. Um, so I 
Speaker 1 (13:14 - 13:46): 
Wonder if we could do again, probably as a separate little piece of work, but further down the line just to kind of just to understand through the process a bit better, where they feel their time is what takes the longest <laugh> and therefore what, what would have the biggest impact in terms of what we build next. Um, we're just thinking about what we're doing in Q4 in the products and so what we prioritize and actually obviously we'll build it around one of these jobs to be done that you come up with, but actually if there's something that directly impacts one of these in quite a, a big way, um, that would obviously make sense to prioritize that. 
Speaker 3 (13:46 - 14:11): 
Yeah, I think it's interesting because although they recognize that the analysis takes a big chunk of time, it's not necessarily what they want help speeding up, they just want more time to be able to spend on, on doing that. Where it's more of the kind of more manual, um, repetitive labor intensive kind of admin tasks, um, that they would actually prefer take less of their time than the analysis. Cool. Um, 
Speaker 1 (14:11 - 14:13): 
And so through this process, what was that for you? 
Speaker 3 (14:15 - 14:19): 
<laugh>? I dunno. Juliet did the, the main chunk of the analysis really didn't you? Juliet? So, 
Speaker 2 (14:20 - 14:20): 
Yeah. 
Speaker 3 (14:21 - 14:21): 
Um, 
Speaker 2 (14:21 - 14:36): 
But again, that is the funnest part of being a researcher going through and actually like pulling out those juicy insights rather than setting up trying to recruit and getting people canceling on you and having to find other people. That is never fun for anyone. 
Speaker 1 (14:36 - 14:40): 
So that recruitment process is just time consuming and painful and really dull <laugh>. 
Speaker 2 (14:40 - 14:43): 
Yeah. And it's not where our skill sets lie, not 
Speaker 1 (14:43 - 14:53): 
What you wanna be doing. Cool. And if you were to take a guess at the proportion of your time on a given project, I know it will vary wildly, but like 20%, 50% your overall 
Speaker 3 (14:55 - 15:08): 
Yeah, because it kind, it affects the timeline. So the timeline really shift according to, um, the, the the recruitment stage. So it's difficult to kind of proportionalize it, but according yeah, to the original plan. 
Speaker 1 (15:09 - 16:14): 
We did, um, some research with 1500 Google researchers and they fed back what we didn't actually get into the recruitment process 'cause we weren't focused on that at the time, but they sort of said in the planning phase, they spent on average five days per quarter, um, over the course of a year over, over a whole set of pro projects. And, um, in the analysis synthesis phase, they spent about 20 days per quarter. So they, we really were focusing the bulk of their time on that analysis piece, but we didn't actually look in detail at the recruitment and the sort of panel selection and, um, and the actual research time. Because the research time obviously varies depending on the nature of the research, but those sort of broad brush strokes we have a little bit of data on. And I just wanna fill out like some of these other gaps in terms of the, that recruitment particularly. 'cause I think that scheduling piece, you know, we can absolutely automate that with ai and so sounds like that could be quite a big priority, um, in terms of making a useful product that the researchers that will make an impact that will make the right sort of impact on the researchers process. Um, 
Speaker 3 (16:14 - 17:09): 
Cool. So that's kind of what we wanna do as a next step after we've done the surveys, we want to really clearly map out these challenges and opportunities along that research process Mm-Hmm. So it's very clear how we can talk about, um, yeah, how we offers value at each of those, those stages. Mm-Hmm. Um, cool. So, um, obviously another thing in terms of the the time burden is, is particularly, uh, in any, any team, both in-house and in agencies, researchers are managing multiple product projects simultaneously. Um, and that sort of leads it to extreme time pressure, um, and the importance of efficiency, um, to keep up with the kind of demands that are placed on them. Um, bias is a bit of an obvious one, but, um, just a kind of big challenge that they're face in terms of when they're actually planning out the research that they, and the designing the kind of study materials like the discussion guides, et cetera. 
Speaker 3 (17:10 - 17:54): 
Um, just wanna draw attention to this one actually, just, um, researchers are often using AI to kind of enhance their process already, but one of the core challenges they actually face is they don't have the time to actually train themselves up in how to do that effectively. So they might be relying on on chat GPT for example, but they haven't got the time, um, to learn how to best kind of leverage that through kind of prompt engineering, et cetera. Um, so this is kind of one of the core opportunities that we'll come onto is that they are looking for a, a AI tool that is specifically dedicated and tailored to that to research and so helps them, um, understand kind of how best to use it and give some suggestions of, you know, relevant prompts, et cetera. 
Speaker 1 (17:54 - 18:37): 
And that a hundred percent fits into the sort of first step of the kind of personal language model concept. Um, and how we then apply that to researchers. Again, our goal is to get something in, in the products late in Q4 so that I can go into the invest around in January, um, essentially pitching, look, we've got users using this personal language model, this is the most, or one of the most ful features. Um, and look at the impact it's gonna have and time saving and et cetera, et cetera. Um, so yeah, super interesting in that, just on bias though, quick little note, maybe for you Harshita, um, I think bias is one of those more exciting and interesting topics of conversation. Um, and so maybe that's a podcast thing. Um, yep. Rather than it, it could be one of the more podcasty things, right? 
Speaker 1 (18:37 - 19:08): 
'cause there's lots of people that can talk about bias in all sorts of fascinating different ways. Um, so maybe that's just something to take into that, that world. I think obviously this kind of AI and research and prompt engineering would feed into webinars really effectively. Um, and we are doing quite a lot of work in the background on the product side, just trying to work out what our, um, prompt engineering system and how the user customizes it for them, and then how it, it learns their, their methodologies and their preferences. Um, so again, that will feed into more of a webinar type approach, I think 
Speaker 3 (19:09 - 20:04): 
<inaudible> Yeah, absolutely. Thanks. So that, those are kind of general, more general, um, challenges with core research, but now I just wanna look at it across the challenges across the research process. So it's kind of just a brief look. Obviously we've spoken about the difficulty with recruitment. The main challenge there is that there's often a trade off between the speed at which you can recruit participants and the, um, confidence that you can have in that they have the right quality and relevant to the research question. Um, people do express sort of uncertainty in actually designing the research, choosing the right methods, um, and designing the right questions to ask. Um, a big core challenge as well is obviously aligning the research question, sorry, research plans and methods with the strategic objectives. So whether that's business objectives for in-house researchers or the kind, the kind of client strategic objectives, um, in an agency. 
Speaker 3 (20:04 - 20:38): 
Um, so there are nuances here that we will pull out, especially as we come out from the survey, um, for, for the agencies, you know, this aligning research with those strategic objectives is actually really core part of that competitive process. That's a really core part of their actual business model, um, is being able to make sure that their research plans are more pointed and, and kind of really aligned well with those, um, strategic objectives and going beyond maybe the brief that's been given to them to gather them. Kind of that further input, that further context into it. 
Speaker 1 (20:39 - 20:54): 
Is that different for quant versus qual? Or are these kind of, these are really, all of these responses are qual driven primarily, aren't they? I saw that your sample of quant was, was much lower, but presumably that sort of thing is more, I guess, prescient for qual researchers. 
Speaker 3 (20:55 - 21:50): 
Yeah, yeah. This is mainly coming from qual researchers. Yeah. Yeah. Um, so in terms of co actually conducting the research, um, qual research very much expressed like the importance of actually having a human in, in a research session. Um, and you know, that in itself causes kind of challenges in terms of, of time and also like maintaining kind of consistency in the data collection as well. Um, just that is kind of just a, a really, it's like a challenge that they do face understanding that they do need to put a human in there to understand and capture those, those kind of nuances. Um, also taking notes is, was a key challenge as well during the research. So often, um, researchers do not rely on kind of digital transcription softwares, but that kind of impacts then the experience of the research and the ability for them to detect those nuances and connect with the participants in a way that gets them to fully open up. 
Speaker 1 (21:51 - 22:27): 
I think for us, that applies specifically in our kind of messaging to enterprise. Um, enterprise seems to be much, much hotter on the participant experience 'cause they think of the participants as their customers, and therefore they want to ensure that their customer experience of that touch point with their brand is incredibly well handled. Um, and certainly with our, our sort of Google experience, that's one of their top top concerns. Um, it's probably less so for, for smaller marketing SMB style research agencies. Um, although still important, but I think perhaps that goes to live in our enterprise messaging partly. 
Speaker 3 (22:28 - 22:55): 
Yeah, and I think we've got a couple of insights around that actually, that, um, it's particularly the case for enterprise with, you know, high value customers, high lifetime value customers as opposed to enterprise with, you know, shorter, smaller one-off purchases. Yeah. Um, a thirdly kind of like financial services and banks and things like that, where that particularly is a, is a challenge. Um, cool. So coming onto the analysis, um, on synthesis part, obviously that is a big, so 
Speaker 1 (22:55 - 23:21): 
One, one really note on that, um, Lucy, I've got an opportunity with Schroders, um, and they're the one that generated one like that in one of those insights there, which was that, um, they wanna talk to 150 of their customers. Um, can you just take a, take a note, like, I'd love to talk to you about partnering to be able to deliver that piece of research for them essentially. Sorry, I off topic Anna, but just wanted grab it while we're here. 
Speaker 3 (23:22 - 23:23): 
It's on my list. 
Speaker 1 (23:23 - 23:24): 
Thank you. 
Speaker 3 (23:25 - 23:47): 
Yeah. And then, um, they also recognize that in the kind of analysis and this, um, part of research, there is a huge need for the kind of, um, trained expert in order to actually extract meaningful insights. So it's not just about kind of reporting on the finding, it's actually that creativity from the mind of the researcher to generate those insights. Um, and again, 
Speaker 1 (23:47 - 24:14): 
Sorry for keeping to interrupt the flow. Like the, again, I love about that is that goes to the idea of, um, for the, for the user, for the customer. Like, how do I get my next job in order to get my next job, my next promotion, my next, how do I make myself better and improve my meaningful life? I, I need to be able to improve my expertise. And I think the, some of the idea with the personal language model is that there are elements of it that can be shared or learned or, or provided to other people. Um, and I think that could play into that part of it. 
Speaker 3 (24:15 - 24:38): 
Yeah. And I think it's really, we'll come on to talking about the anxieties, but one of the kind of core blockers, um, for even relying on research is a fear from researchers that it takes away from their ability to develop their own skills. Um, so I think that's a really interesting opportunity to kind of position beings as different through the personal language model and the combination of actually developing the research at schools part rather than reducing them. 
Speaker 0 (24:39 - 24:44): 
It is that human plus tech, um, that we've theme that we've talked about before. 
Speaker 1 (24:45 - 24:51): 
Mm. Which we don't like AI plus HR or whatever it is, but we need to find some other way of being able to talk about it. <laugh>. 
Speaker 3 (24:53 - 26:22): 
Yeah. Um, in terms of like using AI currently for, um, synthesis part and the analysis, um, a core challenge is that is they can't fully rely on it. Um, it really does require human oversight. Um, and that's in terms of just preparing data, even transcriptions, you know, they, they do need to go through those manually to make sure that the, uh, it has been captured correctly. Um, but also, yeah, generally in terms of like the summaries that are generated as well, there's a huge need to kind of balance that AI and automation with hu human oversight to make sure that the output is accurate and, and relevant, um, collaboration as well. Um, collaboration is really important to core researchers because it is one of the ways that they aim to reduce bias, um, in the process. So, um, ensuring that there's those set up communication channels and regular updates, um, where they're sort of checking in with both stakeholders and, you know, collaborators, um, is, is a challenge as well. So that's an interesting one. Um, and then coming on to kind of the, the final stage, which is, you know, reporting on findings and influencing the action. Um, so communication comes in here as well because part of that is keeping those stakeholders engaged and updated throughout the research process, making sure that they're up to date and in the loop with what's been being learned throughout the process. Um, 
Speaker 2 (26:23 - 26:37): 
And I think just on top of that point, there's, there's added pressure for potentially more agency side of like having to constantly deliver and provide insights as they go, even if they're not fully ready. So just having that collaboration and communication 
Speaker 1 (26:37 - 26:52): 
Point this morning. Right. Exactly. Nice. Cool. Totally. I guess the, yeah, the, the time pressures that just always exist, but I guess they're amplified in the agency world, aren't they in the market research agency or in the marketing agency more generally. Cool. 
Speaker 3 (26:54 - 27:21): 
Um, and then actually presenting to stakeholders as well as one of the kind of more challenging parts actually, uh, distilling or the complex and huge amount of information that they've gained from qual into a clear and kind of actionable insights with stakeholders that are also very much that need to be appropriate for the different contexts and involvement levels of those different stakeholders, um, and the actions that they need to take as a result. Um, I just 
Speaker 1 (27:21 - 27:27): 
Briefly, you pretty much always use Miro or are you using slides and PowerPoint and other things as well? Like what's the 
Speaker 3 (27:28 - 27:37): 
Yeah, it really depends on the kind of, um, communication preferences of, of the clients that, yeah. Um, often it's people do prefer a PowerPoint deck Mm-Hmm. 
Speaker 1 (27:37 - 27:38): 
<affirmative> death by PowerPoint. 
Speaker 3 (27:41 - 28:37): 
Um, and then finally, you know, part of that, um, presentation as well, but over and above that is actually just convincing stakeholders of the value of the research. Um, and it kind of comes in at the start as well, but like really kind of being able to, with the idea of needing to align research objectives with business objectives or strategic objectives, it is a big challenge for, for researchers to demonstrate the value of their research. Um, and that's, say, that's one of the core, core challenges. So, um, what we've done then with the jobs and outcomes is keep them aligned around those themes that we went through at the start. Um, so speed, um, really the kind of outcome of speeding up the process is to free up the time, free up researchers time, um, to spend on more high, higher value tasks and ultimately accelerate the time to insight. 
Speaker 3 (28:38 - 29:42): 
Um, so spending less time on those, more kind of admin tasks, uh, like scheduling, participant management note taking, uh, your transcription or ultimately allowing those researchers to focus on those more. Um, yeah, like critical thinking and creative tasks to actually use their time more efficiently. Um, vasa preparation we've put in here as well. So actually the a key kind of communication theme that came out is, um, you know, that when they see AI helping them here, it's all about creating and automating first drafts, um, and acting like the junior researcher, um, who would kind of take away that kind of manual workload at Star, get them, um, off that blank page, um, to kind of speed up their abilities to actually get into the, um, more strategic thinking and set up the projects. Um, obviously in terms of speed, quick data processing. So it's felt well recognized that AI can process huge amounts of data much, much more quickly than, um, human camp. 
Speaker 3 (29:42 - 30:39): 
Um, ultimately enabling them then to kind of meet those tight qual, uh, time research timelines without compromising on the quality and depth that that qual can enable. Um, and then finally, you know, speed up time to insight, as we've already mentioned, just really quickly being able to get those reliable insights, um, are kind of author automating and speeding up that synthesis and, and summarization. Um, this satisfaction theme, I think is one of the more, um, the fresher insights that we, we kind of gained out of this process. Um, so when we're talking about satisfaction and what our target audience are looking for, they're really looking to, like Juliet said at the start, one, increase researchers enjoyment. The researcher themselves wants to enjoy the process more, but more important than that, they need to improve the value of the research. So the perceived value of the research and enhance the, you know, stakeholder satisfaction, um, with the research. 
Speaker 3 (30:40 - 31:35): 
So delegating those more mundane tasks, again, coming back to just allowing researchers to focus on the parts of the work that they enjoy the most and where they feel they can add the most value. Um, so, you know, like Juliet said, it's like where they, what they're actually trained to be doing is where they wanna be spending their time. Um, and then for the stakeholders, you know, ILI is about being able to kind of deliver those timely and more kind of impactful, um, outputs to increase that satisfaction, um, on the kind of strengthen point. Um, this one we've got quite a few with within, um, this strengthening, it's all about kind of just being able to enhance the actual output, so richer, more impactful outputs, um, and beings helps throughout the research process to get to this outcome. So we've mentioned aligning the plans with those, with the broader context, with the stakeholder context to make sure that it is really pointed. 
Speaker 3 (31:35 - 32:34): 
Um, and, um, you know, it's actually gonna contribute to those, um, decisions and those, uh, strategic objectives. Recruiting also, we've put in here, so we mentioned the challenge with the trade off often between, um, speed and, um, relevance and quality. Um, so the ability to kind of more quickly find, uh, the most, the high quality and relevant samples, um, ultimately it's just gonna increase the quality of the data and the insight generated. Um, in terms of strengthening the, the process and the outputs, the ability to sort of see experiences in more detail in, um, firsthand through the, the Beings platform, um, via those kind of multicam streams allows them to capture those kind of detailed nuances and the more unspoken insights that they wouldn't get, um, if they weren't to do it at <inaudible>. Um, is there 
Speaker 1 (32:34 - 32:55): 
Anything about context there, or not particularly out interest? So something we get from the YouTube team a lot is they're trying to understand context. So they're trying to understand how they're creators that are using primarily the YouTube app and other apps as part of the creation process. They wanna understand like where they are, what they're doing, like what's going on behind the scenes, rather than just purely, um, sort of nuance and and unspoken. 
Speaker 3 (32:57 - 33:10): 
Yeah, a hundred percent. I should put that in there. That did definitely come out. Um, and also, you know, generally, like there are researchers that do specific, um, context based studies, so it's sort of more the kind of eth eth type research. 
Speaker 1 (33:14 - 33:48): 
Was there much like, was the language quite researchy, <laugh>, if you see what I mean? Because like, I'm very aware that there's, um, there can be quite a lot of snobbery in that kind of research world where there's, where if you're speaking the right language and we, you are using all the right terms for the different types of research, um, obviously it, um, it's going to attract the right, like a different sort of researcher compared to somebody that's doing research more generally as part of other things that they might do, for example, in a marketing agency. Um, I don't know, what was the language like that people were using? 
Speaker 2 (33:50 - 34:14): 
I think from my side, um, you know, obviously they're using research terms, they, they're not gonna sort of, um, put in layman's terms or simplify, but generally speaking they're, they're talking about their pains and their struggles, which are pretty like standard Yeah. Struggles. So it was quite, you know, like everyday language, just conversational. Yeah. Cool. 
Speaker 3 (34:15 - 34:45): 
Yeah, and we'll also come on to when we're talking through the kind of blockers, but simplicity is, is really, really important. Mm-Hmm. <affirmative>. Um, and also we, there's an opportunity potentially as well for in these, in-house teams, often it's not just the dedicated researchers doing research. Um, there are sort of designers that will be conducting the most research due to the fact that the research team is so busy handling and at capacity. Um, it's sometimes important to have a more accessible tool that people who do research but aren't dedicated researchers can understand how to use. 
Speaker 1 (34:45 - 34:51): 
And that's what every product manager at Google tells us. They don't wanna wait for the research team even though they're supposed to 
Speaker 2 (34:51 - 34:51): 
<laugh>. 
Speaker 3 (34:54 - 35:38): 
Um, so we've mentioned, uh, seeing experiences firsthand, maintaining that human touch is also, uh, something that they really want to be doing in the research to enhance the strengths. So we've mentioned the importance of that to actually, um, make participants feel comfortable enough to evoke those deep, more deep insights in qual. Um, and then we've talked about actually making them better at what they do. So, um, so the language that came up here in terms of how they see AI being useful to strengthen their research is seeing it as a sort of spamming partner or a collaborator to actually kind of optimize their efforts, make sure they haven't missed anything, um, and also reduce some of the bias that they may have if they can, um, have a conversation, uh, with the AI about something. 
Speaker 1 (35:39 - 36:00): 
And just briefly on language around that, obviously in the tech space, it's often called kind of co-pilot or assistant or chat agent, isn't it? Um, was it dscout? Maybe we found that was chef and sous. Chef <laugh>. Yeah. Um, I think we sort of need to try and find our own own way 'cause that of just articulating that, but yeah. Love it. Thank you. Keep going. 
Speaker 3 (36:00 - 37:13): 
Yeah. Um, and then sort of influencing better decisions. So this is all around kind of being able to distill that large amount of data into those clear and actionable insights, um, which are specifically tailored to vastly the objectives, the strategic objectives, but also the context of the particular stakeholder that they're reporting to. Um, we've also got strengthening the stakeholder relationships, um, so building trust, creating opportunities for involvement, um, just yeah, enhancing and strengthening the kind of strength stakeholder. Um, buy-in, if you like, in into the research. Um, and then scale. Um, so really scale is about expanding that scope of qual, um, to make the insights qual research more reliable. So scaling up, being able to handle more qual data, um, in the short timelines that they're, they have. So to be able to broaden that scope without sacrificing on depth and quality by having to rely on something like quantum. Um, and then also, so scaling the research across markets, so being able to kind of synthesize multiple different, um, data sets in different regions to get a more kind of holistic and reliable view. 
Speaker 3 (37:15 - 37:53): 
Um, and then finally the final S is safety. So we mentioned this one, but it's really about ensuring that, uh, research data is kept safe, is kept private to sort of safeguard, uh, both participant trust, but also, you know, the, the company's reputation and the researchers reputation as well. And obviously avoiding fines and things like that as well. Um, so those are, are kind of reasons why, um, a researcher would, or the, the target audience would buy, um, beings. But we've also got some insights around why now. Um, which, Juliet, do you wanna go through those ones? Yeah. 
Speaker 2 (37:55 - 38:54): 
So, um, I mean, the first one's just basically that researchers have so much data and like we've said, they lack the time and resources to literally go through all of it and get, gain those insights. So, um, they're looking for ways to help make that process more efficient and that they can actually go through and use, go through those large volume sets and make use of the data that they've got. Um, so, um, with AI market trends, um, awareness is a big one. Um, and what we will come onto more with like the inertia or the things that are sort of blocking them for actually, um, using AI tools is discoverability and actually knowing what's out there. Um, so, um, when they become aware of these new AI tools or they're recommended or they've come across them, that's where they'll go in and, and, and, um, test them out and look to bring them into their organization, um, or for, for their own use. 
Speaker 3 (38:55 - 39:56): 
And I just wanna add on top of that as well. Um, actually it's quite interesting in some of the enterprise, um, businesses that we spoke to actually, um, researchers have looking for AI and, and integrating AI into their research process as some of that as built into their OKRs. Um, so they're actually extremely interested in, um, AI market trends, and they're actually more expert in AI than we'd expect. Um, so they are very much, we, we know, you know, they do evaluations of they comparing different, uh, chat GP team, different models, AI models to human researchers, um, and really they are following their kind of AI market trends and waiting for the kind of next generation of models to come out so they can test that again. Mm-Hmm. So it's really kind of, they are up to date with the ai, um, market, and that's, um, when they see a, a new development is actually when they'd start paying more attention, um, tools using a particular model which they deem more reliable and more useful. 
Speaker 1 (39:57 - 40:11): 
Cool. Yeah, I can see like from an enterprise perspective why it's built into OKRs. So the CEO can stand on stage at some point in the year and say, we've got 5,000 staff using AI to accelerate their blah, blah, blah. Cool. 
Speaker 2 (40:12 - 40:24): 
Yeah. Um, I guess that moves us nicely onto the next one. So it's about really just keeping up with those tech advances and those AI advances to stay competitive in the market as well. Um, nice. 
Speaker 3 (40:25 - 40:46): 
Yeah, just on that one as well, just outside of, um, AI research tools, there was actually, um, a big push maybe a couple years back around research repository tools. Yeah. And I was speaking to some of the, um, researchers, I sort of said, why did you, you know, start using a, a research repository? And the answer was just because everyone was doing it. 
Speaker 1 (40:47 - 40:47): 
Yeah. Mm-Hmm. 
Speaker 3 (40:48 - 40:49): 
That's what market was telling us to do. 
Speaker 1 (40:50 - 41:18): 
Yeah. It's fascinating, fascinating. What I found, um, with enterprises and repositories is they're not so sure, or they're not always so sure that it's, it's actually as safe as they think it is. Google's request is that it, it's software that runs on their systems, basically on their side of the firewall because they're not a hundred percent sure they can trust the dovetails or whoever, um, despite all the compliance that everyone says that they, they do and has the certifications for. 
Speaker 2 (41:20 - 41:43): 
Yep. Um, so, um, the research ops, so there's some, some, you know, agencies and in-house as well, um, actually have teams created that are solely focused on, um, making the research process better. Um, and so looking for these tools, actively seeking for, for ways to do that. Mm-Hmm. 
Speaker 1 (41:43 - 41:43): 
<affirmative>. 
Speaker 2 (41:44 - 42:24): 
Um, so, um, kind similar to the researchers with large data sets. Um, they also, you know, some have really specialized, um, niche projects as well, like working in different languages, um, working across the, across the globe. So they need to, um, find a tool that will help them handle those requirements. Um, yeah. And then, and then finally just generally, like we've spoken about just a desire and, and, um, a a need to improve their research capabilities and processes. 
Speaker 1 (42:25 - 42:26): 
Cool. 
Speaker 2 (42:28 - 43:39): 
So away from the pains and onto the good stuff. So what actually excites them, um, about using beings or, or, or AI tools generally? Um, what are they looking for that will help them feel confident that this is the right solution that will get them to where they wanna be, um, so that, you know, help them achieve their jobs to be done. Um, so the ability to handle these large data sets is, um, the first one. Um, and, uh, aware of time, I'm gonna like skim through more maybe the more obvious, obvious ones. Um, so specifically tailored to research. So this one's key because obviously there's, there's specific needs and different research methods, um, across companies and across projects. And so they need tools that offer this more like flexible way of working. Um, and that really align with their workflow. Um, so again, security and privacy, just making sure that it's all compliant and, and all data's protected is really important and, and they need to make sure that that's, um, you know, prioritized with the tool. 
Speaker 3 (43:40 - 44:20): 
Yeah. And it's a, it's a key phase in terms of, obviously as, you know, like onboarding and new research tool, it will go through rounds of information security reviews, um, and then often these sort of flexibility in the way that they use it, if it due to the results of those. Um, just a quick example of, uh, research that was using codes as a research repository, but also as an analysis tool. Um, they actually had to, it took six months to onboard it, but to take six months and not 12 months, they had to actually forego, um, uploading any direct recordings into the platform. They had to then in trans transcribe the recording separately and upload the transcripts into codes because it didn't pass that in terms of its security review. 
Speaker 1 (44:22 - 44:23): 
Nice. Thanks. 
Speaker 2 (44:25 - 45:07): 
Um, so user friendliness is key. Um, you know, like we said that researchers don't have a lot of time, um, to learn how to use these tools. So making sure it's easy to use is, is, is essential. And then that also integrates seamlessly into their workflows, um, without adding any more, um, sort of stress and pressure to them. Um, and then following on from that, the integration part, it, you know, this, this is really about making sure that they're not using multiple different tools and, and different platforms to get the same result. Um, yeah, 
Speaker 3 (45:07 - 45:08): 
Yeah. 
Speaker 1 (45:09 - 45:26): 
Yeah. I think that's a fascinating one. For our kind of product development, we have to choose a strategy and align with certain integrations and ignore, ignore certain other ones. Um, I think we've got quite a lot of, a lot more research work to do to understand the way to approach that. Um, cool. Thanks. 
Speaker 2 (45:27 - 45:34): 
Um, so cost is, is is another obvious one, just making sure it's value for money. Um, 
Speaker 3 (45:36 - 45:50): 
Yeah, I think it's important to recognize that, um, research teams often have a set budget for their research tools. Yeah. Um, and you know, if they were to need to expand that, then they really need to put together a ROI, uh, proposition the tool as well. Yeah. Cool. 
Speaker 2 (45:50 - 46:06): 
And what I did see in some, some, some of the interviews, you know, they're settling for those free research tools of their existing platforms already. Like the transcribing ones on Zoom and stuff, they're just sort of settling for that 'cause it's, they're ready for, um, even though they're not 
Speaker 1 (46:06 - 46:08): 
That crap, but it's included 
Speaker 2 (46:08 - 47:18): 
<laugh>. Exactly. <laugh>, um, customization of reports. So this goes in line with the flexibility and, and the, the ability to, you know, tailor reports that, um, you know, cater to the specific needs of, um, the stakeholders involved or you know, who, who's act what you are delivering and to who it's for, um, is really important. So being able to personalize and, and customize is, is, is really important. Um, global reach. So this is, is is really thinking about, uh, recruitment side of it as well. Um, and being able to, um, attract and recruit participants across the, across the globe. Um, one person, um, in an interview was talking about, you know, having the ability to recruit another countries and, and people sort of pretend they speak French when they don't speak French. And, and, you know, making sure that the tool has language verifications is one thing that came up and making sure that, you know, people aren't slipping through the lines of screeners and, and, um, yeah. High quality participants. 
Speaker 3 (47:19 - 47:27): 
Um, David, a quick question on this, like global reach part. Um, obviously translation of transcriptions also a big one. I 
Speaker 1 (47:29 - 48:39): 
A hundred percent. Like we, yeah. So we have like a particular interest in nuance and how language and nuance and what's lost in translation. Um, and so we don't, we have, uh, so for transcription, I think we support 27 languages, um, for, uh, for analysis. We are at the, the limits of chat GPT and Gemini currently. Um, but actually building nuance into like, project specific nuance. So as the researcher goes through and codes and tags, um, a set of transcripts or a set of notes, they can create these tags that include interpretations in a way. So like the researcher's understanding of nuance that can then be sort of rolled out across a set of, uh, a set of, um, transcripts, if that makes sense. And they do it for one, and then it gets rolled out across the rest. So if they find a particular person in a particular language, they said something, it was translated by the auto translation one way, but actually they, they know through their own experience or whatever, or they know through their own knowledge of that, of researching in that space that when someone, that the translation's wrong, but it should actually be interpreted differently because the nuance of it has a different meaning. 
Speaker 1 (48:39 - 48:48): 
Um, they can then tag that through the transcript and that can become context, um, for the rest of the, the research within that project, if that makes sense. 
Speaker 2 (48:48 - 49:13): 
Yeah, that's, that's really interesting. And that came out a few times, especially, um, one person was, it made me laugh actually. He was talking about how, you know, um, even in the UK people down south compared to people up north Yeah. Have completely different dialects and, you know, there's a lot of effing and swearing and some, you know, some people use that and it's not seen as a negative thing, but actually, like AI might pick it up and see it as a negative. 
Speaker 1 (49:13 - 50:25): 
Yeah, yeah. But I think there's, and there's, so there's the language bit, there's the dialect bit, but there's also just the people don't say what they mean or what they think. Right. And so actually in the given context, people might quite often respond in a certain way to certain types of questions, but if the researcher picks up on that tags that, and says, look, every time somebody's saying this in this context, it means, you know, in the healthcare context it means X, Y, z in the government policy context, it means, you know, A, B, C. And then again, that provides context into the, um, uh, or that become, that becomes part of the knowledge base, um, or for that researcher that, and that as it pertains to that project. So we've got this concept of there's the, the user knowledge base, there's the project knowledge base, and then there's this kind of overarching knowledge base and the personal language model kind of mediates between those different things. Um, is, is, yeah, how we're approaching that, building out that personal language model element. 'cause the idea there is that it becomes very sticky for the researcher 'cause they feel like they have personally trained, um, uh, all, all of their expertise into this system, and therefore they, hopefully they want to be able to use this system more than any of the others. 'cause they've, yeah, they gain from it every time they put effort into it. You see what I mean? 
Speaker 2 (50:26 - 51:10): 
Yeah. Um, so, um, finally, so, uh, you know, research capabilities, being able to support different research methods and functionalities, you know, research ha researchers do also have their own frameworks and ways of working. So being able to, um, integrate that and support that is, is really important for them. Um, and then of course, effective, um, coding, tagging and organization of, of, of data is for easy analysis. Um, and so the blockers, so these are thinking about those anxieties and inertia that are potentially stopping people, um, choosing and using, uh, AI tools. Um, 
Speaker 3 (51:11 - 51:16): 
Yeah. Should we focus on the, the kind of more surprising ones? So the ones with the stars on? Yeah, yeah, 
Speaker 2 (51:16 - 52:26): 
Definitely. So, um, I mean, we talked about this in the pools, but you know, the perceived complexity of it, they've, they've used the tools before. They find them really difficult and they've wasted their time. So they're just like more reluctant to try out other tools. Um, so the, also the idea that the tools need to be able to cater to both sort of extra expert and occasional users as well, um, is, is really important. And also potentially blocking, blocking them. Um, so the trust and accuracy concerns is, is probably one of the biggest ones. So, you know, the idea of, um, inaccurate outputs, hallucinations, um, limited context understanding, you know, they've either tried them before, they're just skeptical that they'll be able to understand their particular research and their, you know, um, more sensitive, especially more sensitive participants as well in some research projects. Um, they're, they're, yeah, they, they're skeptical about the accuracy and data quality and the reliability of the tools. 
Speaker 3 (52:27 - 52:28): 
Mm-Hmm. 
Speaker 1 (52:28 - 52:54): 
<affirmative> and <inaudible>, that's essentially what PLM addresses, right? Um, because it fundamentally is about them creating something that manages all of this. So if they send a transcript off the Gemini to get some sort of response back, they've already provided the, the context and the nuance and that, that set of understanding to be able to manage, um, manage those effectively and they can give obviously, feedback on it as soon as they, um, see something that comes back. Um, 
Speaker 3 (52:56 - 53:15): 
Yeah, that context window is a huge, um, yeah. Differentiator of the ability for ado to sort of be there throughout the whole project and have that context. Mm. It's definitely a huge differentiator. Um, is that, uh, yeah. 
Speaker 2 (53:15 - 53:46): 
Oh yeah. So the liability, um, of insights and recommendations. So this is, you know, the I idea of if these AI generated insights are incorrect or not totally accurate, um, who, who's responsible for it. Um, and you know, so that's, that's a key one. And this, um, especially for the agency side where they're dealing with like external stakeholders and external, they've got their client, um, it's a big worry for, for them of who's accountable if they're incorrect. 
Speaker 1 (53:47 - 53:49): 
Yeah, no, that's really fascinating. Um, 
Speaker 1 (53:51 - 54:27): 
And yeah, again, I would go back to our, our approach to the personal language model being how that is handled. Like, yes, you, you build out your PLM to get the best possible response back in the first place, but at the end of the day, a lot of what the AI is doing is making suggestions, um, not necessarily kind of hard and fast recommendations or insights. It's allowing you and it's accelerating you as the researcher to, to get to those insights quicker. Um, and hopefully removing some of the barriers in terms of time and admin, um, to be able to, um, give you more time to get to your results. Um, 
Speaker 3 (54:27 - 54:39): 
Just on that one as well, it's sort of, um, if they were to hand over to AI completely, they worry about then their ability to handle, you know, those follow up questions or any, any question from a stakeholder. Mm-Hmm. 
Speaker 1 (54:39 - 54:48): 
I think that, yeah, it just goes back to our kind of human plus tech. Mm. Is is what we're talking about here. We're not talking about AI replacing. Um, 
Speaker 2 (54:49 - 54:49): 
Yeah, 
Speaker 3 (54:49 - 54:53): 
Exactly. Cool. So the final two. 
Speaker 2 (54:53 - 55:38): 
Um, so yeah, difficulty to get buying. Um, so just like being able to justify the cost of, and, and the return of investment, um, to stakeholders, um, or leadership teams. Um, so therefore blocking them to be able to actually invest further into tools that will help support the research process. Mm-Hmm. Um, and the idea that using ai, um, researchers feel that it just will make them lazy or come, you know, be lazy in their own work and also, uh, reducing their own skills. Um, so, you know, we spoke about that earlier as well, so didn't wanna, don't wanna, um, go too much into it. 'cause I know we've only got two minutes left. Yeah. 
Speaker 1 (55:38 - 55:42): 
Yes. When I learned to read a map, I stopped remembering where things were. <laugh>. 
Speaker 2 (55:42 - 55:43): 
Yeah, <laugh>. 
Speaker 3 (55:46 - 57:05): 
Um, cool. So, so I'm out of time Dave. So I just wanna quickly, um, just outline the next steps. Um, so obviously we've got, uh, a wealth of insight. You know, this is just some of, um, what we will actually be kind of put putting into the survey. Um, but what we wanna do is create a survey kind of really helping us then calculate, I guess, the opportunity, um, to create value around each of those jobs to be done based on kind of where, who faces the, the biggest problems, um, and who kind of yeah. Has the biggest, um, need for a tool like beings. Um, and we'll do that by kind of recruiting from across the segments and sort of split the data and comparing then these opportunities. Um, so that's our, that's our next step I think. Just wanted to have a chat with you around sort of recruitment for the survey. Mm-Hmm. <affirmative>. So we normally recommend an absolute minimum of 50 participants in the quant survey per segment. Mm. Um, so in terms of the two segments, we can look at agencies as a whole as a segment or obviously we could split them further into agency, um, small agencies and large agencies. And the same for in-house. Um, it's obviously up to you, um, in regard regarding kind of the cost of them recruiting those 50 per segment. 
Speaker 4 (57:07 - 57:07): 
Um, 
Speaker 1 (57:07 - 58:21): 
Uh, yeah, no, cool. Absolutely fine. Like I, I guess Hashi and I think of these in terms of we've got two audiences, one that will go through a B2B sales process. So book a demo, talk to Dave, blah, blah, blah. And then the others, that's the product led growth side, which is the LinkedIn ads. They'll see something and they'll click it and they'll self self-serve. And we're hoping <laugh> that kind of the marketing agencies, market research agencies in their sort of various guises will fall into the product led growth side. Um, only really the very large market research agencies might go into the B2B sales process flow. Um, but really it would be kind of the, the different area segments with internal research teams that would primarily go into the B2B sales flow. Um, so I'm interested in, um, yeah, what size are those overall in the UK and then in the US and, and then in Europe. You see what I mean? And then of those, do we feel like we've got what, what, what become, you know, we need a statistically valid sample, right? So based on the, the overall size. And so I would go the, uh, my approach is to normally go to sales navigator, pick out the job titles, pick out the size, look at the quantities and the numbers, just to get a sense of, right, well, how many people are, they're actually doing this. Um, and sort of work back from there. Mm-Hmm. 
Speaker 4 (58:21 - 58:21): 
<affirmative>. 
Speaker 1 (58:22 - 58:28): 
But yeah, happy to discuss this a bit more offline over the rest of the day or tomorrow, just to get you up and running quickly on that, if that's helpful. 
Speaker 4 (58:29 - 58:29): 
Love. 
Speaker 1 (58:30 - 58:35): 
Cool. Thank you. That was amazing. Absolutely amazing. Love it. <laugh>. Hashi. Done anything from you? 
Speaker 4 (58:36 - 58:38): 
No. Really exciting. I was, I was, uh, 
Speaker 1 (58:39 - 58:39): 
Blown away. Was 
Speaker 4 (58:40 - 58:43): 
Fascinated. Yes. <laugh>. Yeah. Blown away. Blown away right away. <laugh>. 
Speaker 1 (58:43 - 58:51): 
Um, aji, have you got time to just stay on after or do you want do, yeah. Okay. We'll stay on in that case. Um, thank you so much. 
Speaker 4 (58:51 - 58:55): 
Okay, great. Thank you. Amazing. Everyone have a great day. Amazing. 
Speaker 1 (58:55 - 58:57): 
Bye. Great meet you Julia. Really great work. 
Speaker 4 (58:57 - 58:58): 
Nice meeting you. Bye 
Speaker 1 (58:58 - 58:59): 
Bye-Bye. 
Speaker 4 (59:01 - 59:03): 
Okay. It's awesome. Three rdo <laugh>. 
Speaker 1 (59:04 - 59:08): 
Yeah. Oh, we can't even remove them, can we? Because we're not the host. They're the host. 
Speaker 4 (59:08 - 59:09): 
No. 
Speaker 1 (59:10 - 59:14): 
<laugh> funny. Let's jump into another meeting. Yes. Um, okay, cool. Speak to you there. Bye. Yeah.

---
*Converted from original Word document.*
