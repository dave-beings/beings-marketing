  
**Speaker 0 (16:54 \- 7:37:35):** Products to improve market research and the, uh, qualitative research related experience for, for various kind of researcher, product manager, and then marketing and sales people as well. Uh, yeah. Uh, so yeah, we can start with the, you can explain, uh, a little bit about your job and then, uh, you can tell me, uh, your day-to-day responsibilities, and then we can start, uh, discussing more details. 

**Speaker 1 (7:54:25 \- 30:51:15):** Yeah, absolutely. So my name's Lois. I'm a research manager in the qualitative team at Yugo based in London. So within my team there's about 25 of us, um, five based in the us about 18 of us in the uk. Um, and then five more people based in India. In terms of my day-to-Day responsibilities, I solely work on qualitative research at the moment, and I manage projects end to end. So with our team, we don't specialize in kind of any specific kind of sector research. So the research that we do is across a variety of clients, really like digital media and technology, um, charities and the public sector, um, governments and regulators, um, banks, um, a whole host of clients. Um, and my role within that is managing the projects end to end, starting with research design and developing the proposals to win the work. And then setting up research, overseeing recruitment. I often delegate recruitment of projects and then right through to the field work itself. So interviews, focus groups and that kind of thing, moderating those and then analyzing the findings and writing up the reports for the clients. 

**Speaker 0 (31:14:55 \- 35:31:15):** Oh, very good. Thank you very much. I'm very interested to hear that you're basically covering almost everything \<laugh\> that, that I wanted to know. Uh, so are, are you leading a team of these 25 people? Uh, are they like reporting to you as well? Once they're going through their products? 

**Speaker 1 (35:45:15 \- 44:49:45):** A couple of the, um, more junior researchers report to me, so mm-Hmm. \<affirmative\>, I manage a couple of research executives. Um, so I would say I have a mid to senior level role within, um, my team. Um, I am in the position where I'm able to bring products into the business Mm-Hmm. \<affirmative\> if I see a use for it. So I'm often on the lookout for different products that might be helpful. And we have trialed a couple of AI products this year based on connections I've, um, made on LinkedIn. 

**Speaker 0 (45:09:35 \- 53:00:45):** Oh, very good. Thank you. And it's like, you are very, uh, interesting, uh, career trajectory. So o over the years, uh, what is your perspective about the evolving technologies? Like these days we are hearing more about ai, so, uh, do you see, uh, can, can you tell me a bit more about, uh, uh, your experience that how time to time a new, uh, technology or new trends are, uh, coming to market search and disrupting the field in a way? So, 

**Speaker 1 (53:26:35 \- 73:32:45):** Um, yeah, my, my perspective on AI is a little bit mixed. I definitely see it as a very useful tool for a researcher in kind of streamlining the analysis process and things like that. But I don't necessarily see it as a replacement for kind of good old fashioned human, um, analysis. I see it as something that can supplement that and something that whilst the tools are getting kind of more fine tuned and things like that, I think it's still something as well that requires human oversight. Um, yeah. But in terms of my exposure to different tools, um, I've predominantly used, um, analysis tools. So one is called Survey Mind, um, and another is called ybl. And we predominantly use ybl within the team recently for ybl. That's been part of us developing our AI products within the team where essentially when and the other side of the business, so one of the quantitative survey teams runs a survey. 

**Speaker 1 (73:48:25 \- 85:50:35):** We will write a particular kind of survey question with an open-ended response, and then we use all of these open-ended responses. We start kind of sell that to the client and we upload this to yabo and it splits it into kind of sentiment and themes, and then we sell access to the platform to the client along with some like analysis and reporting on the findings from the open-ended responses. So we sell that at the moment as a kind of quick version of qualitative research if they don't necessarily have the budgets for full blown qualitative research. So we do that and then we also use, we've started to use the Apple to analyze transcripts. 

**Speaker 0 (86:50:05 \- 89:49:35):** Okay. Yeah. Thanks. And, uh, so what was it just for the surveys or you're using Jabber for any other kind of application as well? 

**Speaker 1 (90:05:25 \- 94:18:15):** Yeah, so there's the surveys where we collect open-ended responses, and we put those into ybl to identify themes, but we also use ybl to analyze transcripts from focus groups and interviews as well. We've started to do that. 

**Speaker 0 (94:47:35 \- 101:47:25):** And, uh, uh, what kind of information it's, uh, throwing back at, uh, at you, uh, once you're feeding these open-ended question or, uh, no, sorry. Yeah, before that, uh, uh, when you say that the, you are using it all of also for transcription, is it only the audio transcription or you feed it to video, uh, recordings of your meetings and then it'll give you something and what kind of output it's giving you? 

**Speaker 1 (102:10:05 \- 129:05:15):** Yeah, so we can upload either a video or the, um, kind of MP free audio Mm-Hmm, \<affirmative\>. Um, and then from that it produces a kind of summary, um, within this platform on the, on the webpage. So it's not an app, it's a webpage, um, of different themes. So it produces, um, thematic analysis and also kind of summarizes the topics and content included within the transcript. Yeah. So that's for when we upload something that's Yeah. A recording of a focus group or an in-depth interview. Um, in terms of when we upload open-ended responses from a survey in Seattle that is its own kind of webpage within, um, I'm trying to think of the right word. Would it be, no, I couldn't do that actually. Basically, it has its own webpage, um, and you go by question and then it will show you the main themes and the sub themes and the prevalence of that in the data as well. You'll be able to click on the, the theme and then it will show comments that correlate with that. So it'll show a survey response that correlates with that. But then there's also a separate section called Chat with Jen, which allows you to ask the AI thing Jen, questions about the data. 

**Speaker 0 (129:36:55 \- 135:38:25):** Yeah, very interesting. And, uh, when you're talking to it, uh, and, uh, can you ask it to replace the information that it has already provided as well? Or, or chatting means? Uh, trying to know more what's not covered there. 

**Speaker 1 (136:12:15 \- 146:32:05):** You can build on questions. So say for example, if you ask the bot how prevalent a certain theme is within the data, um, it will give a response and then you can kind of say, what about X something else? And it kind of builds on it, and it doesn't give you information that isn't in the data. So, for example, it doesn't pull information from online to kind of contextualize the responses. If it's not in the data, it just doesn't say anything about it, if that makes sense. 

**Speaker 0 (146:41:15 \- 150:19:25):** Yeah, sure. Yeah. That's the, yeah, that's totally sensible because at the end of the day, as a researcher, you are probably interested in what you can can get out of your data and not, not like random suggestions coming from its pre-training information from the web. 

**Speaker 1 (150:26:45 \- 155:59:25):** Absolutely. Yeah. That element was important. And also using a tool that isn't using your data to learn. Sure. Because then you get into difficult scenarios regarding privacy. Privacy, um, especially 'cause as a business, I mean, you gov is huge and we have to jump through quite a lot of hoops in order to get a new product approved. 

**Speaker 0 (156:18:15 \- 171:13:15):** Right. And okay. Yeah, that, that, that was very interesting point about you are not comfortable with the, the AI tool getting training on your data. I totally understand and agree with that. Uh, but I'm also wondering that, uh, if we follow another loophole there, that if the AI tool is, uh, learning your data, learning from your data, uh, just to get optimized with your style and, uh, to get more familiar with the field of that product, but this data is strictly under your jurisdiction, like nobody else has access to the training, whatever training this model is getting from your data will be used only for your products and no one else has access to that training. Will you be okay then, uh, for that AI tool to get better by getting training from your data 

**Speaker 1 (171:33:25 \- 178:24:45):** In, in practice? Um, well, no, rather in principle, yes, but I guess for me it's how well the brand or business offering that product is able to give that rationale Yeah. For me to then escalate it when it comes to getting the product approved. Yeah. But on face value, that sounds fine. Mm-Hmm. \<affirmative\>, um, yeah. 

**Speaker 0 (178:41:55 \- 178:58:25):** Right. Yeah. Thank you. 

**Speaker 1 (179:03:25 \- 179:24:25):** You can see it as being beneficial. 

**Speaker 0 (179:51:15 \- 183:24:45):** Yeah. Thanks. Yeah. Okay. So yeah, if the statement is as clear as possible and you can also trust it, then you can let that model to get training to give you better service as long as no one else has access and your securities. Exactly. 

**Speaker 1 (183:36:25 \- 189:38:45):** And yeah, it would be helpful in the sense that I found that when we've used AI products before, if it is a kind of specialist subject mm, it, it is difficult sometimes to get the insights that you need. So the more fine tuned the product can become to a specific topic is only a good thing, um, in my view. 

**Speaker 0 (190:12:45 \- 198:32:45):** Right. Yeah, I totally agree. And, uh, about the fields, about the tools that you have used for different kind of fields, because you're working with like a plenty of fields, have you come across, uh, uh, some examples where you felt that the, uh, discover the tool that you're using or in general, other AI tools available in the market are giving you much better results for a particular field and very weird results for some other field? 

**Speaker 1 (199:01:25 \- 224:11:35):** Um, kind of, it's less to do with kind of sectors, but more to do with, um, the what you are feeding it, if that makes sense. So for example, with the tool that I mentioned, survey Mind, Mm-Hmm. \<affirmative\>, that was really effective at analyzing focus group transcripts, but Yabo doesn't do that very well, so you yabo analyzes transcripts where there's only two speakers. Oh. So an in-depth interview quite effectively. But the other tool that I was using was better for both. It could, it could handle analyzing, um, kind of recordings with multiple speakers or just two speakers. Fine. It was a bit more versatile in that sense. In terms of, um, kind of topic, I haven't observed huge differences, but sometimes when you are taking on more technical projects, so for example, I was working on a project where I was speaking to people that worked in marketing positions about the different platforms and applications and things they used. It didn't handle \<laugh\>, the kind of technical elements very well in terms of the official kind of names of brands and apps. Um, and also it can be a little bit difficult in the health space as well. 

**Speaker 0 (224:33:05 \- 229:46:15):** Right. And thanks. And, uh, like yeah, you raised a very interesting point that Yeah. Uh, like some of them are very suitable to one type of qualitative research. And, uh, what, uh, what was the name of the third one that you suggested? That it was good for a bit of both. 

**Speaker 1 (230:08:55 \- 235:40:25):** That was Survey Mind, but unfortunately, um, because we already had Yabo onboarded with my company. Okay. It was difficult to get the rationale to onboard Survey Mind. I used it at an old business earlier this year, but I couldn't get it onboarded once I moved to U gov. 

**Speaker 0 (235:49:15 \- 243:31:45):** Right. Uh, thanks. And, uh, what about the participants? Uh, how do you think, uh, the, uh, the AI technologies could help you potentially, uh, with the, uh, reaching or contacting to, uh, more suitable participants for that particular research? Depending on your, you know, topic or, or the field. Have you seen any example of the use case so far? 

**Speaker 1 (244:00:25 \- 244:46:45):** Um, what an AI tool supporting with recruitment? 

**Speaker 0 (244:55:15 \- 245:06:45):** Yeah, yeah. 

**Speaker 1 (245:19:45 \- 261:03:35):** Oh, I've not really heard about that. Um, or seen that in practice. I would be a little bit skeptical, um, in the sense of, is this tool just making people up? Um, are these actually real people? Because there's some methodologies that we have on the qualitative side, so online communities or text-based focus groups where we don't actually see the participant. So in that scenario, it would feel a little bit like we haven't done the due done, the due DI can't say the word. Yeah. Due diligence. Due diligence when it comes to recruiting. Um, but also I think we're in quite a unique position at you Govin, that we have this huge panel, so we largely recruit from that anyway. So I can't really see a use case for AI supporting with recruitment, 

**Speaker 0 (261:26:15 \- 262:55:15):** Uh, use panels. That means you have your own users and cus uh, people that you can talk to. 

**Speaker 1 (263:03:05 \- 271:00:35):** Yeah, there's millions of people on the, the U of panel. So we very rarely have to go anywhere else for recruitment. We occasionally use, um, recruitment agencies. Um, but that's, we only really do that when we're looking to recruit really specialist audiences. So for example, like senior lawyers Mm-Hmm. \<affirmative\>, um, or people in a very, very specific location for face-to-face research. Mm-Hmm. 

**Speaker 0 (271:01:15 \- 278:55:45):** \<affirmative\>. Right. Thanks. And, uh, let's say that once you have recorded the data and then you're analyzing or doing the interpretation of that, uh, so, uh, uh, how do you do that? Can you, uh, walk me through your, that process that after the data recording? Yeah. What are your next steps and for you see any application of AI in the following steps to reduce the admin task and quickly give you the insights? 

**Speaker 1 (279:20:45 \- 298:44:05):** Yeah, so we often use, um, an AI platform. So ybl in the early stages of analysis, we often have a brainstorm with the client after the field work. So that will allow, by using that and feeding it the transcripts, it allows us to have a kind of top line view of what the research is telling us to go in and discuss with the client before we then go in and do more manual research. So we would upload all of the transcripts, um, see what's coming out of it, but then separately look at the transcripts ourselves and kind of think about whether there's any gaps or if there's more detail and richness that can be brought out of the, the themes that are already being presented by the AI tools, essentially. So it's very much like a summarizing and a summarizing tool for us and a starting off point. Yeah. Um, except for in instances where I mentioned that we do the kind of quick version of qualitative research with the survey responses, and then we do rely on it quite heavily. 

**Speaker 0 (299:10:55 \- 307:23:35):** Right. So for all the sentiment analysis and transcription and thematic analysis, it's always \<inaudible\>. Yeah. Right. And, uh, w when you mentioned about, uh, communicating with your clients the result of your analysis, uh, what is the, the user format of those, uh, discussions? Uh, do you just send them a PPT or PDF file or you prepare report? How do you communicate, uh, what are the common formats, how you communicate your results with them? 

**Speaker 1 (307:37:45 \- 314:31:45):** Um, the PowerPoint sometimes, um, with our charity clients, it's a word report. The, the charity sector just appear to prefer a word report, um, often because they then go on to publish it. But, um, for the majority of our clients, we report findings and deep, like kind of present them to them in PowerPoint format. Mm-Hmm. 

**Speaker 0 (314:32:25 \- 324:09:45):** \<affirmative\>. And, uh, after these discussions, have you ever come across into a situation where they wanted you to, uh, revisit, uh, some kind of questions or, uh, ask you to do, uh, a little bit more follow up of that research or ask that, oh, well, well we couldn't get the answer of this thing. And, uh, what are your approach then? Will you go back to your, uh, analysis summary and take help, uh, from AI to see if there is any hidden pattern or you will go back and talk to the participant again? 

**Speaker 1 (324:32:15 \- 339:22:45):** Um, no, it's often trying to dig down into the data more. Um, in the first instance, I think it's unlikely we would revisit, um, speaking to a participant as normally the sample size itself has been fairly exhaustive anyway. So yeah, what I would often do is use YBL to search for specific words if a client's interested in a very specific topic and doesn't feel that we've teased it out or chat with Jen, so chat with the bot and ask them about the data as well. One good thing about that chat tool is that, um, when the bot kind of replies to you, you can click on the response and then it will show the relevant comments. So you have the direct quotes from the data. Yeah. So yeah, I do, I do sometimes use it as a tool when there's been follow ups from the client. 

**Speaker 0 (339:39:35 \- 347:08:25):** Right. Yeah. Thanks. And, uh, in terms of some fear of, uh, ai, uh, either in terms of potential bias when it's giving you the information or some ethical consideration or if there is anything else they can do, uh, would you like to tell me, uh, some of the, uh, things that we should be really concerned about when we are using ai? So you have a few things to watch for. 

**Speaker 1 (347:34:45 \- 349:29:25):** I think it's just a concern that the tool will make inferences that aren't necessarily there. 

**Speaker 0 (350:02:55 \- 351:13:55):** Oh, okay. Right. Yeah. Like making up information by itself. That's not in the data, you mean? 

**Speaker 1 (351:27:15 \- 358:29:15):** Yeah. So drawing from kind of broader context rather than from the data, I would say that's my, my main concern when it comes to, um, tools and then where the data's actually going and how it's stored and how long for, so one big selling point with the tool that we use at the moment, yabo, is that, um, the data isn't held for very long. 

**Speaker 0 (358:59:05 \- 360:58:15):** Would are they offering you a plan where they can ask you for more money and keep your data for few more years as long as you need? 

**Speaker 1 (361:41:35 \- 368:51:25):** Um, it's, it's, in terms of the plan we have for yabo, I'm not sure we can keep it for as long as we want, essentially. But it's more that the data that is stored on kind of, it's hard to explain. We're essentially in control of, um, how long the data is on the platform. 

**Speaker 0 (368:58:55 \- 369:04:45):** Right. 

**Speaker 1 (369:19:45 \- 371:28:45):** But I think it's more to do with what they're doing with the data on their side, if that makes sense in terms of storing it. 

**Speaker 0 (371:36:55 \- 381:19:55):** Right. Yeah, I understand. Thanks. And, uh, in terms of, uh, let's say there is no technical problem in idealistic scenario. Uh, and, uh, what are, what, what will be your wishlist from an AI tool to help you to reach those insights quicker? To reduce your admin work and help you to see some hidden patterns? Uh, yeah. Uh, what, what are your wishlist from an, uh, very idealistic AI tool? The things that are, that are not even available yet, or something that you really want to see? 

**Speaker 1 (381:33:25 \- 399:08:15):** Being able to summarize outputs from a variety of different methods. So as I said then with Yaba at the moment, it doesn't handle transcripts where there's multiple speakers very well. Mm-hmm \<affirmative\>. But it would be great even to be able to explore a transcript from an online community where there's lots of different responses to certain tasks and be able to analyze that way as well. Like at the moment, um, we're not able to do that. Um, I would also love to have the ability for it to kind of summarize a project, so not just maybe one transcript in one go, but summarizing all of the transcripts together, um, to identify any nuances between different interviews, for example, to be able to do that quite effectively would be good. Um, a kind of clean editing process as well. So I said that human oversight is quite important. 

**Speaker 1 (399:21:45 \- 421:51:45):** Often trans, when it transcribes the data and then analyzes it, there's obviously spelling mistakes sometimes because it's ai being able to edit those effectively and also edit the themes that come out as well. Sometimes it doesn't pick up sentiment very well either. It thinks that a certain word is kind of inherently negative when actually within the context, the way a human would understand it's not negative. Um, oh, and also because we, with ybl, I talked about two different things in terms of the summary, but also the um, the kind of access that we have when we upload the survey responses. We do give clients access so they can go through and look at the themes and then look at the quotes as well. So being able to have a scenario where clients can actually comment on something if they find it interesting and flagging that way would be good. So essentially being able to, um, like tag certain bits of text both on the, on the researcher side and also on the client side that are particularly interesting too. So just a very interactive experience on the researcher side. 

**Speaker 0 (422:12:55 \- 429:48:55):** Right. Yeah, thanks. That's very interesting. And like when you talk about tag, you are like in qualitative research method, you have your own, uh, you know, definitions of coding and tagging, but when you are talking about some tag to give access to clients to certain parts of your product, so they can look at the, you know, the overview and comment there. Mm-Hmm. \<affirmative\>. So you meant a little bit different tagging just to help them to look at the right piece of, uh, project. Right. 

**Speaker 1 (430:06:25 \- 432:11:15):** Yeah. But also if they see anything as well that they think is interesting that they wanna bring to the researchers mention being able to do that as well. 

**Speaker 0 (432:32:05 \- 432:49:05):** That's very interesting. 

**Speaker 1 (432:49:15 \- 441:49:05):** Often we're looking as we go along, so it would be good. So we use a platform called Recollective for, um, our online communities. And when we're doing analysis in the platform and replying to people each day, if we highlight a piece of text, we can then code it and then write a comment as to why we've coded it. Um, so it allows that kind of ongoing analysis as well. So that would be good, um, within a platform like, well, like an AI tool with the transcription. Yeah. 

**Speaker 0 (442:13:15 \- 451:58:45):** And, uh, if I zoom out a little bit from your search work and we look at your product management area, uh, so from that point of view, when you're managing a, some researchers and everybody's working on a different project, and then each project has multiple interviews, and those interview could be one-to-one or focus group or service. So with us all complexity, uh, where do you think AI could help you to be a better manager and uh, to efficiently talk to researcher and to get those insights quicker and in more cleaner way? 

**Speaker 1 (452:39:35 \- 466:48:35):** That's a good point. Even just a clean kind of user interface ability to have kind of lots of different folders manage missions that way. Probably being able to kind of tag people in the team as well to allow for communication. Say for example, if I did code something, I could, for example, tag someone in it or, um, or a scenario where people are, I'm trying to think of the right word. Say for example, if you went on a specific folder, you could quite clearly see the members of the team that were on that project. And also even if you were going to an individual transcript, you would know who moderated that interview. That would be quite effective as well, in terms of keeping track. 

**Speaker 0 (467:20:25 \- 474:56:05):** Oh, okay. Yeah. Right. Yeah, I understand now. Yeah. Thanks. Uh, and when you say that either will some possibility to also like add tags to product, like to connect different people to those products, so that in your view, uh, from management's perspective, you will see it like very clean, uh, correlation between project and interview and people and who did the last moderation on something. 

**Speaker 1 (475:03:15 \- 475:19:25):** Yeah, exactly that. 

**Speaker 0 (475:43:55 \- 479:43:45):** Okay. Very good. Yeah. Thanks. And, uh, is there anything else that you would like to add in this, uh, this Yeah, your, your whole work? If I didn't cover before that, where else AI could still help you? 

**Speaker 1 (482:17:05 \- 487:51:25):** I think not in recruitment in the sense of finding participants, but, and we, and we do have people that schedule in, but maybe in terms of identifying availability and being able to communicate to prospective participants might be helpful because 

**Speaker 0 (487:56:05 \- 490:04:45):** I like the better way for planning, uh, and finding their optimum time slots. And uh, is, is that what you're talking 

**Speaker 1 (490:10:35 \- 501:04:05):** Exactly that? Yeah, because we, we do run field work across different time zones and things like that. So that would be quite it. Then kind of removing the more manual labor from scheduling. Sometimes mistakes are made across time zones, for example. Mm-Hmm. \<affirmative\>, I'm working on a project at the moment where one of the moderators is based in on the US side, but the, the participants are in the uk. Um, so scheduling can sometimes, well, there's sometimes human error in scheduling. Yeah. So that might be helpful not in finding the people, but making sure that everything goes smoothly. 

**Speaker 0 (501:32:35 \- 513:36:15):** Yeah, yeah. Basically, yeah. We are, uh, I mean the tools that we are developing, we also basically want to make sure that, uh, where can you use ai, uh, to reduce the admin work, to reduce the repetitive task and to eventually empower the people so that, uh, as a researcher you can focus on what you're good at, you can focus on talking to people, you can focus on your product planning and trying to get, uh, more insights from the information that you receive at the end and not just, uh, you know, uh, admin related work. And when you say scheduling, that's like a sending emails, like manual scheduling, like appointments, yeah. Agreeing on a common time slot and Okay. Right. Thanks. 

**Speaker 1 (513:44:45 \- 516:47:25):** Yeah, I can see a word in which a good tool could kind of bring together documents that we save in several different places. \<laugh\>. 

**Speaker 0 (517:23:55 \- 519:43:25):** Yeah. Because as a manager you are also probably spending a lot of time when you have to find the right set of document for the right project. And the information is basically scattered, you mean? 

**Speaker 1 (520:01:55 \- 520:16:45):** Yeah, essentially. 

**Speaker 0 (520:55:35 \- 522:03:15):** And, uh, sorry, just last question. How do you guys manage it now? Uh, 

**Speaker 1 (522:18:25 \- 535:47:45):** Yeah. So at the moment we have an Excel master sheet, but even that isn't particularly user friendly because, because it's in a secure folder, because it has personally identifiable information, it can't be a shared document, which means that if someone else is in it, we just can't do anything. Um, correct. But then it's also not linked to our calendars. So the administrator that's scheduling people in can, has the details of the participants in one document, but then has to look in our calendars in a separate place. Mm-Hmm. And then there's time zones to consider. So there's just a lot of moving parts and it'd be quite nice for it to be in a more in one centralized place. 

**Speaker 0 (536:01:35 \- 544:21:25):** Yeah. And even if everything is in centralized place as a, as your job title, you will be, uh, interested in very different kind of information. Like as a researcher, you would be interested in more technical information and maybe one researcher if there is collaborating with another one on the same project, you want to basically have, uh, access to almost everything related to that product. But as a manager, maybe you don't want to get into too much detail and you want to like, uh, take a step back and look at the high level information. 

**Speaker 1 (544:35:55 \- 558:02:15):** Yeah. And, and at the moment as well, I'm a bit at the mercy of people updating these documents. So on a project I'm on at the moment, um, recruitment's going a bit slower than I'd like. We are wanting to come out field next Wednesday, but we're only about halfway through at the moment. Um, and it's quite a hard to recruit audience. Um, and yeah, I don't really know what's going on unless in terms of numbers and whether I need to speak to the client and understand whether we need to change the project milestones in terms of when we deliver the report. Unless my research administrator updates the master sheet. Mm-Hmm. \<affirmative\>, I don't have a lot of oversight in that sense. 

**Speaker 0 (558:13:45 \- 558:19:35):** Right. 

**Speaker 1 (558:21:55 \- 560:38:45):** Or at least the ideal amount that I would, when I'm managing a project, I have to like message them to ask separate things. Sure. 

**Speaker 0 (561:00:35 \- 573:22:45):** Yeah. Thanks. Okay. Sorry, just one last question. Like you already mentioned that when you're talking to your clients, you are taking your PowerPoint presentation or slides in general and, uh, with your other researcher, uh, you explore the output of a project, uh, probably together. And, uh, who else is the, you who, who, who, who's your other stakeholder in this whole, uh, post-analysis stage of a product? Uh, do you have to communicate, uh, uh, the summary or the finding of this project to somebody hiring the management chain at Hugo as well? And how do you communicate? Uh, I believe that the way of, uh, communicating could be very different depending on the person you're talking to, right? 

**Speaker 1 (573:57:15 \- 589:03:55):** Yeah. Um, not hugely. Um, as project manager, I sometimes have someone else within my team who quality assures whatever I get out. Um, so my job is to delegate the analysis and reporting. I do some of it myself, and I also manage the story that we're telling to the clients overall and how we're framing, um, the reporting of our findings in terms of, yeah, the report itself, um, I would have someone quality assuring it, but it's someone usually that I work quite closely with that's slightly senior to me. So it doesn't necessarily, I don't have to report it to them essentially, or debrief it, present it to them. They will just look over it. 

**Speaker 0 (589:14:15 \- 596:25:55):** Yep. Right. Thank you. And so just one note, I'm I'm sorry for, uh, the delay that we are running a little bit over. It's okay. Uh, so other than, uh, you know, survey Mind, Jabal and uh, uh, recollective, is there any other tool that you have explored, uh, for qualitative research that was providing some kind of, uh, AI enhancement? 

**Speaker 1 (597:29:15 \- 597:56:15):** No, it is just those, yeah. 

**Speaker 0 (598:09:45 \- 598:40:15):** Right. Okay. Oh, hang 

**Speaker 1 (598:40:15 \- 600:30:15):** On. Actually, not me personally, but elsewhere in the team, let me just check my email. 

**Speaker 0 (600:51:45 \- 600:59:35):** Thank you. 

**Speaker 1 (601:04:35 \- 603:09:35):** So I have demoed a, um, a tool called Discover ai, if you've heard of that. 

**Speaker 0 (603:30:45 \- 603:42:25):** No, not yet. 

**Speaker 1 (603:55:25 \- 622:37:55):** Um, so that tool, um, it, it is less something that we would access. It's more, say for example, at the beginning of a project if we wanted to understand on a more broader cultural sense something. So for example, I worked on a project near Christmas about gifting and people's relationship with gifting and how they approach it. So if we want like a jumping off point for a project before we start research design and do qualitative research ourselves, discover AI was a tool where they go through kind of, it's a bit more semiotics, but they go through things like social media posts, blog and things like that to kind of understand, um, the cultural landscape of a topic and then they produce a report for you. So essentially I would be the client for them. Um, but that's a tool that we looked at and also a Microsoft one. 

**Speaker 0 (622:48:55 \- 622:57:15):** Mm-Hmm. \<affirmative\>. 

**Speaker 1 (623:22:45 \- 624:22:25):** Oh yeah. We trialed Microsoft 3 6 5 co-pilot. 

**Speaker 0 (624:39:25 \- 625:21:35):** Yeah. And how was your experience? 

**Speaker 1 (625:42:35 \- 631:03:05):** Um, so I didn't personally try all this. My, um, my colleague did, um, let me see what she wrote in the email. Um, 

**Speaker 1 (636:08:35 \- 650:10:45):** Uh, actually it hasn't really summarized what she found from Microsoft Co-pilot. She didn't find it particularly helpful. Okay. Um, for our work. It didn't really seem to save a lot of time. Um, but yeah, some of the things would be interesting alongside the analysis is translation in sort of the languages that would be quite helpful. Right. Um, also the ability to write a kind of case study or a pen portrait kind of summarizing the person that you interviewed. Mm-Hmm. \<affirmative\>. Um, and for requests to be quite quick as well. So one of the criticisms of the platform that I was trying to onboard called Survey Mind was the, the request take about 20 minutes, which felt too long. 

**Speaker 0 (650:49:55 \- 652:16:15):** So request mean, whatever, you feed it into it and it'll take, uh, 20 minute to give you some input Yeah. 

**Speaker 1 (652:16:15 \- 652:48:15):** To produce the summary and the analysis. 

**Speaker 0 (652:57:25 \- 655:24:15):** Okay. Right. Okay. Uh, thank you very much Louis. I really enjoyed talking to you and I'm sorry for keeping you a bit longer. Uh, 

**Speaker 1 (655:32:05 \- 656:21:35):** It's okay. \<laugh\>, I do it all the time to other people, so 

**Speaker 0 (656:32:55 \- 657:20:55):** \<laugh\>, \<laugh\>, do, do you have any question for me? 

**Speaker 1 (657:32:35 \- 662:40:55):** Um, I don't think so. I'd be interested to, for you to keep in touch in terms of Yeah. Development of what you're doing at the moment in the way of products. Um, yeah. I also, I can't remember what the incentive was when I first got the message, um, but I just wondered how that would be delivered. 

**Speaker 0 (662:51:15 \- 666:19:35):** Oh. Uh, you'll get it soon through the email. Uh, I will order an Amazon, uh, voucher worth 50 pound and I will yeah, send it over to you. Uh, you will get it through your email, uh, very soon. 

**Speaker 1 (666:28:45 \- 668:12:55):** Okay. That sounds good to me. And yeah, let me know if you have any further questions, um, about anything that I mentioned today as well. 

**Speaker 0 (668:31:05 \- 671:38:15):** Thank you so much. Yeah, definitely. We will keep you in the loop and we are planning to launch our first product like MVP in September sometime. Yeah. Uh, thank you very much again. 

**Speaker 1 (671:38:25 \- 672:18:15):** Great. Sounds good. Um, enjoy the rest of your day. 

**Speaker 0 (672:21:55 \- 672:59:35):** Thanks, you too. Bye-Bye. Bye. Bye.