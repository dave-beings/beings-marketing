__*Meta Description: Why “we’ll know it when we see it” leads to flabby research, and how a bit of structure can sharpen both your insights and your AI outputs\.*__

<a id="_whrd6myn6i8f"></a>‘We’ll Know It When We See It’ Is Not a Strategy

I do love the opportunity to extend scope\. One of the best things about research, in general, is that sometimes we don’t know quite where it’s going to take us\. It’s literally the point of research to discover things we do not already know \(or perhaps don’t know are related\)\.   
  
That explorative option brings some of the best and freshest insights to any kind of project\. HOWEVER, and this a big however, you kind of need to know what you’re looking for\. And the title of this piece is going to sound contradictory, probably, but we need boundaries in research\.

We’ve all heard it, or said it ourselves\.

“Let’s just keep it open\.”  
“We’ll see what comes up\.”  
“We’ll know it when we see it\.”

It sounds flexible\. Curious, even\. But in practice, it’s often a way to avoid committing to a direction\. And when that happens, everything gets a bit… flabby\.

Objectives blur\. Discussion guides go vague\. People nod along without really knowing what they’re doing  or why\. You end up with hours of footage, pages of transcript, some half\-hearted takeaways, and a vague sense of “did we actually learn anything?”

It’s even worse when you try to run that kind of data through AI\. Because the thing is AI can’t fix woolly thinking\. It can only work with what you give it\. And if what you give it is fluff, you’re going to get fluff back\.

—\-

## <a id="_ummylvdpbvzl"></a>What You Need to Know

__The problem__

“We’ll know it when we see it” gives the illusion of openness, but usually signals a lack of direction\. Research starts without a clear objective, and ends up scattered, hard to analyse, harder to act on\.

__Why it matters__

If you don’t define what you’re trying to learn, the outputs drift\. You get long transcripts, vague takeaways, and decks full of quotes that don’t land\. It also makes AI tools less useful, because they need structure to produce meaningful summaries\.

__Where it goes wrong__

- Briefs stay broad to keep stakeholders happy
- Discussion guides don’t build towards anything
- Vague inputs go into AI tools and come back even vaguer
- The work feels active, but decisions don’t move forward
- Research gets repeated because the first round didn’t stick

__What helps__

- Start with intent, even if it’s loose\. Have a working theory
- Know what would count as useful, surprising, or new
- Shape your questions so they build somewhere
- Use AI tools to sharpen structure, not replace it
- Treat open\-endedness as a method, not a default

Read on to find out more

—

## <a id="_5f1bne7clxmp"></a>We Need A Starting Point

This doesn’t mean you have to lock everything down before you’ve even spoken to a single person\. It just means having a working theory\. A set of questions you want to answer\. A sense of what would count as useful, or interesting, or new\.

That starting point gives your research shape\. It lets you build discussion guides that don’t just meander\.

And if you’re using AI at any point in your process\. Whether that’s to cluster responses, summarise sessions, or map themes, that structure becomes even more important\. Because AI tools don’t know what’s relevant\. They don’t know what your stakeholders care about\. They can’t guess what’s politically sensitive, or commercially valuable\. They just reflect back what you feed them\.

So if your inputs are vague, inconsistent, or unclear… your outputs will be too\.

The answer isn’t to over\-script your guide or over\-control your participants\. It’s to start with intent\. Know the shape of what you’re aiming for so you’ll *actually* recognise it when you see it\.

## <a id="_kq03bn4x0n0e"></a>What Do You Get If You Don’t?

You get transcript soup\.

You get workshops where everyone squints at the Miro board hoping something jumps out\.

You get decks full of quotes that sound fine but don’t really go anywhere\.

You get findings that feel safe, shallow, or just plain obvious\.

You get NOTHING\.

Ok, that’s a bit extreme, but for information that will be about honing and fine tuning your products, marketing, or anything else, you need more clarity about what you’re hoping to achieve\.

This kind of research often *feels* like it went well\. Everyone was busy, lots of people talked, the client said thanks\. But six months later, nothing’s changed\. Or worse, another round of research gets commissioned to ask the same questions in a slightly different way, because no one’s quite sure what the last one was for\.

Even nearly ten years ago,[ Simon Dannatt, CEO at The Sound,](https://www.linkedin.com/pulse/vast-majority-market-research-complete-waste-time-money-simon-dannatt/) noted: “There is a vast amount of research done where the output never really gets used \[…\]\. Huge amounts of effort put into ‘tracking’ where what’s being tracked isn’t really tied to real business decisions” 

It’s a familiar trap, you collect data, feel busy, but end up no closer to real decisions\. Add the barrage of insight gathered, collated and analysed by AI and you have this times thousands\. 

## <a id="_odeywrc8brqt"></a>So What *Does* Good Look Like?

It doesn’t mean stripping all the openness out\. You can still create space for unexpected answers, but it has to be inside a framework that’s actually trying to find something\.

Starting research with something to prove or disprove, then allows you a fixed point to focus on\. Whenever things get off base, or start to get stuck in the weeds, that north star allows you pull things back\. 

Take this as a simple example:

You're running research on a new app feature, and your starting question is *“Does this help users feel more in control of their schedule?”*

That’s enough to anchor the conversation\. You might hear how people currently manage their time, what frustrates them, or whether they even see the feature as part of that problem\. You might find something totally unexpected\. But you’ve got a fixed point, something to return to when things drift or get murky\.

Without that, you’re left with vague reactions\. Comments that are hard to compare\. Feedback that sounds fine in isolation but doesn’t build to anything you can actually use\.

The good outcome from this is clarity\. Not just in the data, but in the decisions that follow\.

You’ll know whether to ship it, scrap it, or shape it differently, because you asked a question you actually wanted the answer to\.

If you’re nodding along but wondering *how* to actually do this, our step\-by\-step guide is a good place to start\. Click to read “How to Build AI\-Ready Discussion Guides That Actually Work”

