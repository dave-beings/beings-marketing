<a id="_9k7yrnkwse7u"></a>Why Are We Still Getting Research Recruitment So Wrong ? \(And How Do We Fix It?\)

*Standfirst: Better tools, better methods, but we’re still struggling with the people\. Recruitment in research is a known problem in the industry\. *

Participant recruitment isn’t just a headache anymore, but a full\-blown bottleneck to good insight\. In the UK alone,[ 1\.1 million people aged 65\+ are from minoritised ethnic backgrounds](https://www.mrs.org.uk/pdf/Impact%20report%20ISSUE%2044%20LR.pdf), yet most research samples still don’t reflect their experiences\. These are the blind spots we create when we rely on the same panels, the same quotas, and the same old methods\.

__For qualitative research, especially, the difference between “some people” and “the right people” has never felt more consequential\.__

### <a id="_tbdfhuow20r2"></a>What Do You Need to Know

__The problem?__

Recruitment is still where research falls down\. Same panels\. Same compromises\. Too often, the wrong people in the room\. 

__Why it matters?__

You can get everything else right: the questions, the structure, the analysis\. But if the participants aren't a good fit, the insight falters\.

__Where it breaks?__

- Overused respondents and "Compers" dominate screeners
- Timelines slip because briefs are too tight or pools too shallow
- Poor onboarding leads to no\-shows and flat data
- The participant experience often feels transactional, not human

__What could help?__

Smarter matching\. Clearer design\. Tools that support your judgement rather than replace it\.

Want the detail? Keep reading\.

You can have the right questions, the right moderator, the right deck, but if your participants aren’t on point, everything else wobbles\. Many researchers I have spoken to say that recruitment is the most time\-consuming part of their process\. It can have such a detrimental effect on entire studies, and can be one of the main causes of poor data or missed deadlines\. 

And yet, most recruitment processes still lean on the same panels, the same spreadsheets, and the same unreliable back\-and\-forth that’s been dragging timelines and skewing data for years\.

So: how do we fix this? Not with silver bullets or hype, but with smarter systems, and yes, eventually, AI that knows when to lead and when to step aside\.

Let’s look at where things are breaking… and where we could go from here\.

### <a id="_3fyitgorhm32"></a>__The Right People, Faster: Why We Can't Just "Panel" Our Way Out of This__

Panels are useful\. But when it comes to qualitative work, deep, messy, human insight, they’re often insufficient\.

You know this already:

- The same “professional” respondents show up, polished and slightly too perfect\.  

- [Niche or underserved groups are consistently underrepresented\.  
](https://www.theguardian.com/society/2025/may/07/concerning-lack-of-female-only-medical-trials-in-uk-say-health-experts)
- Timelines get squeezed because nobody fits the brief, or everyone’s already in five studies this month\. 

In fact, in my experience of speaking to people in the industry, it’s not unusual to see the same names pop up repeatedly across studies\. Some participants are clearly well\-versed in giving the answers they think researchers want\. They’re articulate, agreeable and often a little rehearsed\. While that might seem helpful on the surface, it usually leads to surface\-level insight from people who have learned how to play the game\. The real voices are the ones that challenge assumptions or say something that makes a stakeholder sit up, and they are much harder to reach\.

In B2C research, there’s also the rise of what people call colloquially “Compers”\. These people apply for every prize, every incentive, every freebie they can find, almost to the point of it [being a profession](https://www.aqr.org.uk/glossary/professional-respondent)\.

They’re often extremely responsive, fast to fill in screeners and keen to take part\. But their motivation isn’t always aligned with your research goals\. Incentives do drive participation, but they can also lower the bar, encouraging people to say whatever they think will get them in the room\. It’s a perfect example of demand characteristics at play: respondents shaping their behaviour to match what they believe the researcher wants, not what they truly think or feel\.

So we compromise\. We broaden the screener\. We let things slide\. And we end up with research that’s… fine\. But fine isn’t enough when your product roadmap or customer strategy hangs in the balance\.

What’s missing is *better matching*\. A smarter way to find people who not only qualify on paper but genuinely reflect the audience you need to hear from\.

This is where AI *could* help, not in some long distant future, but quietly, under the surface\. Tools are emerging that can analyse large datasets to surface better fits, flag inconsistencies in screeners, and reduce manual legwork without compromising ethics\.

Right now, they’re not quite there YET\. But the promise is clear: faster recruitment that doesn’t sacrifice relevance or rigour\. At Beings, we’re developing recruitment tools that don’t just match screeners—they adapt to project context, flag red flags, and learn over time to improve every brief\. It’s one of the biggest pain points that we hear from people, and it’s something that I believe we have the power to help support as we grow and develop\. 

### <a id="_xyzihfk90ukz"></a>__The Participant Experience Is Also the Data Quality Filter__

Too often, recruitment treats participants like interchangeable units\. The process is get them through the screener, onto the call, send them the incentive, done\.

But the truth is, your best data doesn’t just come from “qualified” people, but from *engaged* people\. And when that experience is poor, vague, transactional, confusing, dropouts increase, and so does risk\. 

Participants who know what they’ve signed up for\. Who feel respected, not processed\. Who show up curious, not confused\.

And that’s where the cracks form:

- Clunky onboarding\.  

- Vague instructions\.  

- Poor communication leading to dropouts or no\-shows\.  


In the UK, GDPR, MRS standards, and participant rights mean we have to get this right\.

The upside? This is one of the easiest places to apply smart automation\. AI\-driven assistants can handle reminders, explain processes clearly, manage scheduling across time zones, and even tailor nudges based on participant preferences\.

By creating a smoother, more human experience, not only do we have an opportunity for better data, better experiences to encourage more participants but ultimately you have the potential to protect your data quality from the ground up\.

### <a id="_irifmokz0dy3"></a>__Yes, AI Can Miss the Mark\. That’s the Point\.__

Let’s name the worry: what if AI gets it wrong?

What if it recruits someone completely off\-target? Or misses the nuance that a sharp human recruiter would catch?

Fair\. These are real concerns\. And anyone selling you an AI tool that “fixes” recruitment without friction hasn’t done the job themselves\.

But here’s the alternative: keep doing it the way it’s always been done\. Cross your fingers\. Hope the spreadsheet is up to date\. Hope your screener wasn’t too vague\. Hope the no\-show doesn’t throw the whole day off\.

AI supports your judgement, but doesn’t do the judgement for you\. It can make smart suggestions\. Flag potential issues\. Give you more time to focus on the people, not the process\.

The point isn’t to hand recruitment over to machines\. It’s to build systems that are smarter, faster, and fairer, and to do it with your ethics and instincts firmly in the loop\.

### <a id="_dufdgzl1bbgv"></a>__What Happens If We Don’t Fix This?__

The answer here is simple\. Researchers will keep wasting hours and budget chasing ghosts\. As an industry we will keep making do with “good enough” participants\. The worry is that we keep building strategies on shaky foundations\.

Or: we start experimenting\. Thoughtfully\. Carefully\. We pilot\. We test\. We question, and we keep improving\.

AI won’t fix research recruitment tomorrow\. But neither will doing nothing\.

So here’s the question:  


__What would it take for you to trust an AI to help recruit your next participant?__

I’d love to know your thoughts\!   


\(And if you're ready to go deeper — our companion blog explores smarter recruitment strategies in more detail\.\)

